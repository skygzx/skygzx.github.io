<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[number of splits 划分的条件]]></title>
    <url>%2F2019%2F04%2F08%2Fnumber%20of%20splits%20%E5%88%92%E5%88%86%E7%9A%84%E6%9D%A1%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1.分片（splits）相关概念 首先看一张图： 12345输入分片（Input Split）：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组。Hadoop 2.x默认的block大小是128MB，Hadoop 1.x默认的block大小是64MB，可以在hdfs-site.xml中设置dfs.block.size，注意单位是byte。分片大小范围可以在mapred-site.xml中设置，mapred.min.split.size mapred.max.split.size，minSplitSize大小默认为1B，maxSplitSize大小默认为Long.MAX_VALUE = 9223372036854775807 ==split：split是逻辑切片，在mapreduce中的map task开始之前，将文件按照指定的大小切割成若干个部分，每一部分称为一个split，默认是split的大小与block的大小相等，均为128MB。== 那么分片到底是多大呢？ 分片大小由这三个参数决定： 12345minSize=max&#123;minSplitSize,mapred.min.split.size&#125; maxSize=mapred.max.split.sizesplitSize=max&#123;minSize,min&#123;maxSize,blockSize&#125;&#125; 下面看一下源码： ​ 所以在我们没有设置分片的范围的时候，分片大小是由block块大小决定的，和它的大小一样。比如把一个258MB的文件上传到HDFS上，假设block块大小是128MB，那么它就会被分成三个block块，与之对应产生三个split，所以最终会产生三个map task。我又发现了另一个问题，第三个block块里存的文件大小只有2MB，而它的block块大小是128MB，那它实际占用Linux file system的多大空间？ 有大神已经验证这个答案了：http://blog.csdn.net/samhacker/article/details/23089157 1、往hdfs里面添加新文件前，hadoop在linux上面所占的空间为 464 MB： 2、往hdfs里面添加大小为2673375 byte(大概2.5 MB)的文件： 2673375 derby.jar 3、此时，hadoop在linux上面所占的空间为 467 MB——增加了一个实际文件大小(2.5 MB)的空间，而非一个block size(128 MB)： 4、使用hadoop dfs -stat查看文件信息： 这里就很清楚地反映出： 文件的实际大小(file size)是2673375 byte， 但它的block size是128 MB。 5、不过使用‘hadoop fsck’查看文件信息，看出了一些不一样的内容—— ‘1（avg.block size 2673375 B）’: ​ 值得注意的是，结果中有一个 ‘1（avg.block size 2673375 B）’的字样。这里的 ‘block size’并不是指平常说的文件块大小(Block Size)—— 后者是一个元数据的概念，相反它反映的是文件的实际大小(file size)。 123最后一个问题是： 如果hdfs占用Linux file system的磁盘空间按实际文件大小算，那么这个”块大小“有必要存在吗？其实块大小还是必要的，一个显而易见的作用就是当文件通过append操作不断增长的过程中，可以通过来block size决定何时split文件。 补充： 原文地址：http://blog.csdn.net/lylcore/article/details/9136555 12345678一个split的大小是由goalSize, minSize, blockSize这三个值决定的。computeSplitSize的逻辑是，先从goalSize和blockSize两个值中选出最小的那个（比如一般不设置map数，这时blockSize为当前文件的块size，而goalSize是文件大小除以用户设置的map数得到的，如果没设置的话，默认是1）。hadooop提供了一个设置map个数的参数mapred.map.tasks，我们可以通过这个参数来控制map的个数。但是通过这种方式设置map的个数，并不是每次都有效的。原因是mapred.map.tasks只是一个hadoop的参考数值，最终map的个数，还取决于其他的因素。 为了方便介绍，先来看几个名词：block_size : hdfs的文件块大小，默认为64M，可以通过参数dfs.block.size设置total_size : 输入文件整体的大小input_file_num : 输入文件的个数 （1）默认map个数​ 如果不进行任何设置，默认的map个数是和blcok_size相关的。​ default_num = total_size / block_size; （2）期望大小​ 可以通过参数mapred.map.tasks来设置程序员期望的map个数，但是这个个数只有在大于default_num的时候，才会生效。​ goal_num = mapred.map.tasks; （3）设置处理的文件大小​ 可以通过mapred.min.split.size 设置每个task处理的文件大小，但是这个大小只有在大于block_size的时候才会生效。​ split_size = max(mapred.min.split.size, block_size);​ split_num = total_size / split_size; （4）计算的map个数compute_map_num = min(split_num, max(default_num, goal_num)) 除了这些配置以外，mapreduce还要遵循一些原则。 mapreduce的每一个map处理的数据是不能跨越文件的，也就是说min_map_num &gt;= input_file_num。 所以，最终的map个数应该为： final_map_num = max(compute_map_num, input_file_num) 经过以上的分析，在设置map个数的时候，可以简单的总结为以下几点： （1）如果想增加map个数，则设置mapred.map.tasks 为一个较大的值。（2）如果想减小map个数，则设置mapred.min.split.size 为一个较大的值。 （3）如果输入中有很多小文件，依然想减少map个数，则需要将小文件merger为大文件，然后使用准则2。 ==注意:==1、若文件是压缩文件且压缩的格式并不支持文件分割（无法从文件任意地方读取文件），则该文件不管多大都是一个分片2、map job数最终是由分片数决定，程序员只能给一个期望map数。]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>splits</tag>
        <tag>map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop高级之HDFS && YARN HA部署]]></title>
    <url>%2F2019%2F04%2F08%2FHadoop%E9%AB%98%E7%BA%A7%E4%B9%8BHDFS%20%26%26%20YARN%20HA%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[HADOOP HA 搭建：1.SSH相互信任关系配置和host文件配置：注意事项： ~/.ssh的权限为700 1chmod -R 700 /.ssh ~/.ssh/authorized_keys的权限是600或者640 1chmod -R 600 /.ssh/authorized_keys 操作步骤：123依次在3台机器上分别执行：ssh-keygen 命令一直回车即可 进入ssh下，ll查看：将id——rsa.pub公钥文件追加到authorized认证文件中 将hadoop002和hadoop003机器上的id_rsa.pub文件传输到hadoop001机器中： 这个时候会发现 让我们输入密码，但是我们用的hadoop用户，并没有密码，所以需要用root用户： 查看hadoop001中ssh下的文件：将传过来的公钥文件追加到认证文件中： cat查看一下公钥文件中的信息： 这个时候我们需要配置一下hosts中的配置： 记住：每台机器都要测试对其他两台和自己的连接关系；因为第一次连接需要输入yes known_hosts是记录每台第一次连接进入的信息；也就是我们输入yes后会有记录在known_hosts里。​ 【坑】假设known_hosts文件里有一台的ssh发生了变更；不要把known_hosts删除【会造成整个分布式系统的瘫痪】；要进入known_hosts文件找到那一台机器所在行；删除那一行即可。 然后将配置到的公钥文件authorized传到其它两台机器的ssh下，这就完成了多台机器ssh互信配置2.JDK部署1.将解压包解压到mkdir /usr/java/ 【用root用户】 因为CDH shell脚本默认java安装目录是/usr/java/ 将jdk压缩包传入之后并解压： 2.权限变更: 解压 jdk之后会出现 权限变更的情况，需要修改权限： 3.Zookeeper部署及定位1.解压zk： 2. 建立软连接 3. 进入zoo.cfg配置hadoop01的zk ​ [hadoop@hadoop01 app]$ cd zookeeper​ [hadoop@hadoop01 zookeeper]$ cd conf​ [hadoop@hadoop01 conf]$ cp zoo_sample.cfg zoo.cfg​ [hadoop@hadoop01 conf]$ vi zoo.cfg 更改配置文件zoo.cfg 12345dataDir=/home/hadoop/app/zookeeper/data #此目录要提前创建好server.1=hadoop01:2888:3888server.2=hadoop02:2888:3888server.3=hadoop03:2888:3888 [hadoop@hadoop01 zookeeper]$ touch data/myid 【这里的data就是/home/hadoop/app/zookeeper/data】 ==注：配置中有默认的存储目录，在tmp下，但是这里会出现问题，Linux周期30天会清除tmp下的 文件，myid文件会被清除，所以我们需要指定存储目录== 【坑】myid的大小是两个字节【也就是只有一个数字；不要有空格】 [hadoop@hadoop01 zookeeper]$ echo 1 &gt; data/myid [hadoop@hadoop01 zookeeper]$ ll data/myid-rw-rw-r–. 1 hadoop hadoop 2 3月 31 13:38 data/myid 拷贝zoo.cfg到hadoop02和hadoop03 [hadoop@hadoop01 zookeeper]$ scp conf/zoo.cfg hadoop02:/home/hadoop/app/zookeeper/conf/ zoo.cfg 100% 1023 130.5KB/s 00:00 [hadoop@hadoop01 zookeeper]$ scp conf/zoo.cfg hadoop03:/home/hadoop/app/zookeeper/conf/ zoo.cfg 100% 1023 613.4KB/s 00:00 拷贝data目录到hadoop02和hadoop03 [hadoop@hadoop01 zookeeper]$ scp -r data hadoop03:/home/hadoop/app/zookeeper/ myid 100% 2 1.6KB/s 00:00 [hadoop@hadoop01 zookeeper]$ scp -r data hadoop02:/home/hadoop/app/zookeeper/ myid 100% 2 0.9KB/s 00:00 修改myid文件【一个&gt;号相当于覆盖】 [hadoop@hadoop02 zookeeper]$ echo 2 &gt; data/myid [hadoop@hadoop03 zookeeper]$ echo 3 &gt; data/myid 4、配置环境变量【~./bash_profile】完毕后加载一下source ~/.bash_profile 12345export JAVA_HOME=/usr/java/jdk1.8.0_45export ZOOKEEPER_HOME=/home/hadoop/app/zookeeperexport PATH=$JAVA_HOME/bin:$ZOOKEEPER_HOME/bin:$PATH 5、启动加查看状态 注意：如有出错以debug模式检查；shell脚本启动打开debug模式的方法在第一行加入（-x）即可如下： #!/usr/bin/env bash ==-x== 运行这个脚本即可看到运行debug模式来定位问题 5、部署Hadoop1.解压jar包；并建立软连接 2.配置环境变量【每台】 12345678910vim ~/.bash_profile添加如下export JAVA_HOME=/usr/java/jdk1.8.0_45export ZOOKEEPER_HOME=/home/hadoop/app/zookeeperexport HADOOP_HOME=/home/hadoop/app/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$ZOOKEEPER_HOME/bin:$PATHsource ~/.bash_profile 3.到配置文件夹下配置必须的配置文件 4.创建文件夹【每台】 1234mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/tmpmkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/namemkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/datamkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn 5.把需要配置的默认配置文件都删除 6.上传四个配置文件到节点【每个节点】$HADOOP_HOME/etc/hadoop ==【注意：大坑】下列文件最好在centos系统中建立；不要在windows上直接上传过去；会有坑 比如坑：Name or service not knownstname hadoop01；这是识别不了slaves里配置的服务== slaves 123hadoop01hadoop02hadoop03 core-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;!--Yarn 需要使用 fs.defaultFS 指定NameNode URI --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://wuwang&lt;/value&gt; &lt;/property&gt; &lt;!--==============================Trash机制======================================= --&gt; &lt;property&gt; &lt;!--多长时间创建CheckPoint NameNode截点上运行的CheckPointer 从Current文件夹创建CheckPoint;默认：0 由fs.trash.interval项指定 0指的是和fs.trash.interval值一样--&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认：0 不删除 --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop临时目录, hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这&gt;个路径中 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181&lt;/value&gt; &lt;/property&gt; &lt;!--指定ZooKeeper超时间隔，单位毫秒 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;!--hadoop.proxyuser.hadoop.hosts代表hadoop进程运行的用户；比如hadoop.proxyuser.jeek.hosts说明要以jeek的用户去启动hadoop的进程--&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!--压缩；可忽略！！！需要经过源码编译才可使用--&gt; &lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;!--HDFS超级用户 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt; &lt;/property&gt; &lt;!--开启web hdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name&lt;/value&gt; &lt;description&gt; namenode 存放name table(fsimage)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;$&#123;dfs.namenode.name.dir&#125;&lt;/value&gt; &lt;description&gt;namenode粗放 transaction file(edits)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/data&lt;/value&gt; &lt;description&gt;datanode存放block本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 块大小256M （默认128M） --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;/property&gt; &lt;!--======================================================================= --&gt; &lt;!--HDFS高可用配置 --&gt; &lt;!--指定hdfs的nameservice为wuwang,需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;wuwang&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置NameNode IDs 此版本最大只支持两个NameNode --&gt; &lt;name&gt;dfs.ha.namenodes.wuwang&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.rpc-address.[nameservice ID] rpc 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.wuwang.nn1&lt;/name&gt; &lt;value&gt;hadoop01:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.wuwang.nn2&lt;/name&gt; &lt;value&gt;hadoop02:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.http-address.[nameservice ID] http 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.wuwang.nn1&lt;/name&gt; &lt;value&gt;hadoop01:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.wuwang.nn2&lt;/name&gt; &lt;value&gt;hadoop02:50070&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode editlog同步 ============================================ --&gt; &lt;!--保证数据恢复 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8480&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8485&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置JournalNode服务器地址，QuorumJournalManager 用于存储editlog --&gt; &lt;!--格式：qjournal://&lt;host1:port1&gt;;&lt;host2:port2&gt;;&lt;host3:port3&gt;/&lt;journalId&gt; 端口同journalnode.rpc-address --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/wuwang&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--JournalNode存放数据地址 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn&lt;/value&gt; &lt;/property&gt; &lt;!--==================DataNode editlog同步 ============================================ --&gt; &lt;property&gt; &lt;!--DataNode,Client连接Namenode识别选择Active NameNode策略 --&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.wuwang&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode fencing：=============================================== --&gt; &lt;!--Failover后防止停掉的Namenode启动，造成两个服务 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少milliseconds 认为fencing失败 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!--==================NameNode auto failover base ZKFC and Zookeeper====================== --&gt; &lt;!--开启基于Zookeeper --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--动态许可datanode连接namenode列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 1234567891011121314151617181920212223242526272829303132&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;!-- 配置 MapReduce Applications --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- JobHistory Server ============================================================== --&gt; &lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop01:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop01:19888&lt;/value&gt; &lt;/property&gt;&lt;!-- 配置 Map段输出的压缩,snappy；可忽略！！！这一段必须经过源码编译的hadoop才可以--&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;!-- nodemanager 配置 ================================================= --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.localizer.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23344&lt;/value&gt; &lt;description&gt;Address where the localizer IPC is.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23999&lt;/value&gt; &lt;description&gt;NM Webapp address.&lt;/description&gt; &lt;/property&gt; &lt;!-- HA 配置 =============================================================== --&gt; &lt;!-- Resource Manager Configs --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群名称，确保HA选举时对应的集群 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarn-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!--这里RM主备结点需要单独指定,（可选） &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm2&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt; &lt;/property&gt; &lt;!-- ZKRMStateStore 配置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk.state-store.address&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- Client访问RM的RPC地址 (applications manager interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;hadoop01:23140&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;hadoop02:23140&lt;/value&gt; &lt;/property&gt; &lt;!-- AM访问RM的RPC地址(scheduler interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;hadoop01:23130&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;hadoop02:23130&lt;/value&gt; &lt;/property&gt; &lt;!-- RM admin interface --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt; &lt;value&gt;hadoop01:23141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt; &lt;value&gt;hadoop02:23141&lt;/value&gt; &lt;/property&gt; &lt;!--NM访问RM的RPC端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;hadoop01:23125&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;hadoop02:23125&lt;/value&gt; &lt;/property&gt; &lt;!-- RM web application 地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop01:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop02:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm1&lt;/name&gt; &lt;value&gt;hadoop01:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm2&lt;/name&gt; &lt;value&gt;hadoop02:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop01:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7.修改vim hadoop-env.sh 【三台】 1export JAVA_HOME=/usr/java/jdk1.8.0_45 8.第一次启动步骤： *（1）先启动JN【每台】* 这里有个问题：生产上如何增加JN的节点呢？ 详情：https://blog.csdn.net/lifuxiangcaohui/article/details/52329933 *（2）格式化Hadoop【hadoop01】并将data目录传入hadoop2* 123[hadoop@hadoop01 hadoop]$ hadoop namenode -format[hadoop@hadoop01 hadoop]$ scp -r data/ hadoop02:/home/hadoop/app/hadoop *（3）初始化ZKFC* 12345[hadoop@hadoop01 hadoop]$ hdfs zkfc -formatZK[hadoop@hadoop01 hadoop]$ start-dfs.sh如果出错重配置文件开始；首先关闭相关进程；清空data目录；删掉相关配置文件；并且ZK里的hadoop组件相关目录也要删除（hadoop ha）。 *（3）在hadoop01启动dfs集群* *（4）在hadoop01启动yarn集群* 1[hadoop@hadoop01 hadoop]$ start-yarn.sh （5）手动启动RM2 1[hadoop@hadoop02 hadoop]$ yarn-daemon.sh start resourcemanager *（6）启动日志管理* 1[hadoop@hadoop01 hadoop]$ mr-jobhistory-daemon.sh start historyserver （7）运行一个例子 1[hadoop@hadoop01 hadoop]$ hadoop jar ./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 5 10 出现错误排查：是这个版本不支持snappy压缩格式的问题 12345通过查看我们压缩格式不支持：如下查看解决问题需要编译好的支持本格式的压缩的hadoop组件；并且在相关配置文件配置压缩参数见注释&lt;!-- 配置 Map段输出的压缩,snappy；可忽略！！！这一段必须经过源码编译的hadoop才可以--&gt;&lt;!--压缩；可忽略！！！需要经过源码编译才可使用--&gt;为什么要使用这个压缩格式呢？可以减少map的磁盘io 12345678910[hadoop@hadoop01 hadoop]$ hadoop checknative19/03/31 17:38:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableNative library checking:hadoop: false zlib: false snappy: false lz4: false bzip2: false openssl: false 19/03/31 17:38:04 INFO util.ExitUtil: Exiting with status 1 去掉配置的压缩条件就可以正常运行本demo 6、停止集群并进行第二次启动1234567[hadoop@hadoop01 hadoop]$ stop-all.sh[hadoop@hadoop01 hadoop]$ zkServer.sh stop[hadoop@hadoop02 hadoop]$ zkServer.sh stop[hadoop@hadoop03 hadoop]$ zkServer.sh stop 启动集群 12345678910111213[hadoop@hadoop01 hadoop]$ zkServer.sh start[hadoop@hadoop02 hadoop]$ zkServer.sh start[hadoop@hadoop03 hadoop]$ zkServer.sh start[hadoop@hadoop01 hadoop]$ start-dfs.sh[hadoop@hadoop01 hadoop]$ start-yarn.sh[hadoop@hadoop02 hadoop]$ yarn-daemon.sh start resourcemanager[hadoop@hadoop01 hadoop]$ mr-jobhistory-daemon.sh start historyserver 7、集群监控123456789HDFS: http://hadoop01:50070/HDFS: http://hadoop02:50070/ResourceManger （Active ）：http://hadoop01:8088ResourceManger （Standby ）：http://hadoop02:8088/cluster/clusterJobHistory: http://hadoop01:19888/jobhistory 思考： 为什么map端用snappy压缩格式；而reduce用gzip或者bzip2的压缩格式呢？为什么每个reduce端压缩后的数据不要超过一个block的大小呢？ 1、首先我们根据博客中的压缩格式对比snappy的压缩时间最快；而map是输出数据落地磁盘故选择时间最快的输出压缩格式snappy；​ 2、reduce是结果落盘；故考虑占用磁盘空间的大小；选择高压缩比gzip或者bzip2；而考虑到会用reduce结果做二次运算；​ 则对于选用不支持分割gzip或者bzip2原因有两个：​ （1）是这两个压缩格式高​ （2）对于不可分割我们采用每个reduce端压缩后的数据不要超过一个block的大小的方法；则对于后续的map清洗也就不会出现分割问题。]]></content>
      <tags>
        <tag>HA</tag>
        <tag>Hadoop</tag>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据压缩]]></title>
    <url>%2F2019%2F04%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[1.压缩的好处和坏处：好处： 1.减少存储磁盘空间 2.降低IO(网络的IO和磁盘的IO) 3.加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度 坏处：由于使用数据时，需要先将数据解压，加重CPU负荷 2.压缩格式： 压缩格式 工具 算法 扩展名 是否支持分割 Hadoop编码/解码 DEFLATE N/A DEFLATE .deflate No org.apache.hadoop.io.compress.DefalutCodec gzip gzip DEFLATE .gz No org.apache.hadoop.io.compress.GzipCodec bzip2 bzip2 bzip2 .bz2 Yes org.apache.hadoop.io.compress.Bzip2Codec LZO Lzop LZO .lzo Yes(if index) com.hadoop.compression.lzo.LzoCodec LZ4 N/A LZ4 .lz4 No org.apache.hadoop.io.compress.Lz4Codec Snappy N/A Snappy .snappy No org.apache.hadoop.io.compress.SnappyCodec 压缩比: 可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&lt;LZ4&lt;LZO&lt;GZIP&lt;BZIP2 3.压缩格式各自优缺点a. gzip优点： 压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便 缺点： 不支持split b. lzo优点： 压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便 缺点： 压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式） c. snappy优点： 压缩速度快；支持hadoop native库 缺点： 不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令 应用场景： 当mapreduce作业的map输出的数据比较大的时候，作为map到reduce的中间数据的压缩格式；或者作为一个mapreduce作业的输出和另外一个mapreduce作业的输入。 d. bzip2优点： 支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便 缺点： 压缩/解压速度慢；不支持native 总结：​ 不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。 应用场景： 适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 应用场景： 一般在HDFS 、Hive、HBase中会使用； 当然一般较多的是结合Spark 来一起使用。]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>Snappy</tag>
        <tag>Bzip2</tag>
        <tag>LZ4</tag>
        <tag>LZO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop编译支持5种压缩格式]]></title>
    <url>%2F2019%2F04%2F07%2FHadoop%E7%BC%96%E8%AF%91%E6%94%AF%E6%8C%815%E7%A7%8D%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Hadoop-2.6.0-CDH-5.7.0版本编译： 前置环境：1. JDK为jdk-7u80-linux-x64.tar.gz ​ 2. Maven为apache-maven-3.3.9-bin.zip ​ 3. findbugs为findbugs-1.3.9.zip ​ 4. protoc为protobuf-2.5.0.tar.gz ​ 5. hadoop-2.6.0-cdh5.7.0-src.tar.gzprotoc为protobuf-2.5.0.tar.gz 组件名称 组件版本 百度网盘链接 vm vm10 链接：https://pan.baidu.com/s/1N5i8p8htXz9H_v__YNV1lA 提取码：yasn centos centos6.7 链接：https://pan.baidu.com/s/1Z_6AcQ_WnvKz1ga_VCSI9Q 提取码：a24x Hadoop Hadoop-2.6.0-cdh5.7.0-src.tar.gz 链接：https://pan.baidu.com/s/1uRMGIhLSL9QHT-Ee4F16jw 提取码：jb1d jdk jdk-7u80-linux-x64.tar.gz 链接：https://pan.baidu.com/s/1xSCQ8rjABVI-zDFQS5nCPA 提取码：lfze maven apache-maven-3.3.9-bin.tar.gz 链接：https://pan.baidu.com/s/1ddkdkLW7r7ahFZmgACGkVw 提取码：fdfz protobuf protobuf-2.5.0.tar.gz 链接：https://pan.baidu.com/s/1RSNZGd_ThwknMB3vDkEfhQ 提取码：hvc2 1)下载： ​ 下载hadoop-2.6.0-cdh5.7.0-src.tar.gz 1下载地址：http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0-src.tar.gz ​ 下载jdk-7u80-linux-x64.tar.gz 1下载地址：http://ghaffarian.net/downloads/Java/ 2）上传： ​ 上传hadoop-2.6.0-cdh5.7.0-src.tar.gz至/source下 ​ 上传jdk-7u80-linux-x64.tar.gz、apache-maven-3.3.9-bin.zip、findbugs-1.3.9.zip、protobuf-2.5.0.tar.gz至/software ​ 解压文件 3）查看环境要求： 123[root@hadoop001 ~]$ cd source/hadoop-2.6.0-cdh5.7.0[root@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cat BUILDING.txt 1234567891011121314151617181920212223242526272829Requirements:* Windows System* JDK 1.7+* Maven 3.0 or later* Findbugs 1.3.9 (if running findbugs)* ProtocolBuffer 2.5.0* CMake 2.6 or newer* Windows SDK or Visual Studio 2010 Professional* Unix command-line tools from GnuWin32 or Cygwin: sh, mkdir, rm, cp, tar, gzip* zlib headers (if building native code bindings for zlib)* Internet connection for first build (to fetch all Maven and Hadoop dependencies)If using Visual Studio, it must be Visual Studio 2010 Professional (not 2012).Do not use Visual Studio Express. It does not support compiling for 64-bit,which is problematic if running a 64-bit system. The Windows SDK is free todownload here:http://www.microsoft.com/en-us/download/details.aspx?id=8279 4）配置maven目录： 123[root@hadoop001 ~]$ cd /usr/local/apache-maven-3.3.9/conf[root@hadoop001 conf]$ vi settings.xml 配置本地仓库： 配置阿里源： 5）预编译安装： 12345[root@hadoop001 protobuf-2.5.0]# yum install -y gcc gcc-c++ make cmake[root@hadoop001 protobuf-2.5.0]# ./configure --prefix=/usr/local/protobuf[root@hadoop001 protobuf-2.5.0]# make &amp;&amp; make install 6）配置环境变量： 1[hadoop@hadoop001 ~]$ vi .bash_profile 12345678910111213PATH=$PATH:$HOME/binexport PATHexport JAVA_HOME=/usr/java/jdk1.7.0_80export MVN_HOME=/home/hadoop/app/apache-maven-3.3.9export FINDBUGS_HOME=/home/hadoop/app/findbugs-1.3.9export PROTOC_HOME=/usr/local/protobufexport PATH=$PROTOC_HOME/bin:$FINDBUGS_HOME/bin:$MVN_HOME/bin:$JAVA_HOME/bin:$PATH 7）yum源安装其他组件 123[root@hadoop001 ~]# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool[root@hadoop001 ~]# yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake 9）开始编译 1234567[root@hadoop001 ~]$ cd source/hadoop-2.6.0-cdh5.7.0[root@hadoop001 hadoop-2.6.0-cdh5.7.0]$ pwd/source/hadoop-2.6.0-cdh5.7.0[root@hadoop001 hadoop-2.6.0-cdh5.7.0]$ mvn clean package -Pdist,native -DskipTests -Dtar 10）开始编译 如果看到BUILD SUCCESS，且没有异常信息，说明hadoop已经编译成功 编译完成后可以看到hadoop的压缩包 通过日志可以知道编译好的压缩包所在位置:/source/hadoop-2.6.0-cdh5.7.0/hadoop-dish/target下 编译前： 编译后：]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>编译压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN HA架构图]]></title>
    <url>%2F2019%2F04%2F07%2FYARN%20HA%E6%9E%B6%E6%9E%84%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[1.YARN架构图： 2.架构图详解： 12345678910111213141516171819202122232425262728YARN HA hadoop001：zk rm(zkfc) nmhadoop002：zk rm(zkfc) nmhadoop003：zk nmZKFC: 线程 只作为RM进程的一个线程而非独立的进程存在RMStateStore: 存储在zk的/rmstore目录下。1.activeRM会向这个目录写APP信息2.当activeRM挂了，另外一个standby RM通过ZKFC选举成功为active，会从/rmstore读取相应的作业信息。重新构建作业的内存信息，启动内部的服务，开始接收NM的心跳，构建集群的资源信息，并且接收客户端的作业提交请求。RM:1.启动时候会向ZK的/rmstore目录写lock文件，写成功就为active，否则standby.rm节点zkfc会一直监控这个lock文件是否存在，假如不存在，就为active，否则为standby.2.接收client的请求，接收和监控NM的资源状况的汇报，负载资源的分配和调度。3.启动和监控APPMASTER on NM节点的container。applicationsmanager RMapplicationmaster NM container容器里 作业的主程序NM:节点资源的管理 启动容器运行task计算 上报资源]]></content>
      <tags>
        <tag>HA</tag>
        <tag>架构图</tag>
        <tag>YARN</tag>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS HA架构图]]></title>
    <url>%2F2019%2F04%2F07%2FHDFS%20HA%E6%9E%B6%E6%9E%84%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[1.HA进程: 假设有3台机器： 1234567891011121314151617hadoop001:ZK NN ZKFC JN DNhadoop002:ZK NN ZKFC JN DNhadoop003:ZK JN DNjounalNode数量布置的多少: 一般根据HDFS请求量 及数据量（一般部署2n+1个）ZK集群 ：部署2n+1 个，奇数， 选举 谁做active standby生产上：20台节点: 5台 20~100台节点: 7/9/11台 &gt;100台节点: 11台 但是: 不是说zk节点越多越好，如果部署的多，它选举active投票的时间就会长，会导致对外提供服务特别的慢。如果公司有几百台节点, 那么zk部署的机器就它一个进程，因为zk进行选举的时候，如果选举的快慢跟这台机器的繁忙程度有关系，跟进程数也有关系，若果机器过去繁忙，导致zk夯住了，那么如果这时候NN挂掉了，就会导致standby无法切换成active。 HDFS HA架构流程图：流程说明： 1234567891011121314151617181920HA是为了解决单点问题通过JN集群共享状态通过ZKFC选举active监控状态，自动备援。DN: 同时向NN1 NN2发送心跳和块报告。ACTIVE NN: 操作记录写到自己的editlog 同时写JN集群 接收DN的心跳和块报告STANDBY NN: 同时接收JN集群的日志，显示读取执行log操作(重演)， 使得自己的元数据和active nn节点保持一致。 接收DN的心跳和块报告JounalNode: 用于active standby nn节点的同步数据 一般部署2n+1ZKFC: 单独的进程 监控NN监控健康状态 向zk集群定期发送心跳，使得自己可以被选举； 当自己被zk选举为active的时候，zkfc进程通过RPC协议调用使NN节点的状态变为active， 对外提供实时服务，是无感知的。]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>HDFS</tag>
        <tag>HA</tag>
        <tag>架构图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 HDFS和Yarn HA 的了解]]></title>
    <url>%2F2019%2F04%2F07%2F%E5%85%B3%E4%BA%8E%20HDFS%E5%92%8CYarn%20HA%20%E7%9A%84%E4%BA%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1.企业中为什么要用集群： 1234567891011121314每一个角色都是一个进程：HDFS： NN：老大（接受读写流程请求）Master SNN：1h checkpoint secondary（每隔一小时都会备份NN中的editlog文件合并成新 的fsimage） DN：存储数据块和数据块的校验和YARN:RM 老大 masterNM （注：主从架构 master-slave 比如hdfs读写请求都是先NN节点；每一条请求都要先经过NN，如果单节点NN挂了，那么就不能提供对外服务，所以我们要用到集群的概念。 RM也是一样。) ==但是：hbase 读写请求不是经过老大master，这点需要注意，那什么经过master，就是建表语句，删表语句）== 2.企业生产中的配置： 12345678910111213如果只有一台节点，如果NN挂了，那么就无法对外提供访问了。企业中我们一般会配置两个NN节点，（实时的，任何时候只有一台active对外，另一台standby实时 备份，随时准备着从stanby切换成active状态，对外提供服务。）NN1：active hdfs://ip1:9000/ 代码 shell脚本NN2：standby hdfs://ip2:9000/ （假设NN1在11点挂了，就在那一霎那，NN2会瞬间切换成actice，对外提供访问。）查看hdfs可以这样查看： hdfs dfs -ls hdfs dfs -ls / hdfs dfs -ls hdfs://ip:9000/如果NN1挂掉了，我们切换到NN2，难道我们还要手动修改： hdfs://ip2:9000/吗?这个时候我们抛出一个概念：无感知的：（命名空间：nameservice1 CDH 生产上：dw） 3.命名空间： 1如上图所示：命名空间 RUOZEG6 不是一个进程，当我们输入命令：hdfs dfs -ls hdfs：//RUOZEG6/这个命令时，他会去找 core-size.xml 和 hdfs-site.xml这两个配置文件，这两个配置文件里配置了 hadoop001和hadoop002这两台机器挂在了 命名空间下面，它会去尝试连接第一台机器，如果第一台不是active，那么他会去连第二台机器。]]></content>
      <tags>
        <tag>HDFS</tag>
        <tag>HA</tag>
        <tag>YARN</tag>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN资源调优怎么调，依据是什么]]></title>
    <url>%2F2019%2F04%2F07%2Fyarn%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98%E6%80%8E%E4%B9%88%E8%B0%83%EF%BC%8C%E4%BE%9D%E6%8D%AE%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[1.yarn的资源调优怎么调，依据是什么 ①比如服务器256G物理内存，有DN,NM,RS，三个进程，yarn调优怎么做？ 123这个题目从两个角度回答：1.机器总内存，预留内存，各个进程内存的经验值2.余下就是yarn资源的总内存，然后参数就是公众号 里的参数。but，那些参数如何设置让资源利用最大化？ 以后再补！]]></content>
      <tags>
        <tag>YARN</tag>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 实战]]></title>
    <url>%2F2019%2F04%2F07%2FHive%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[需求：统计各个城市所属区域下最受欢迎的Top 3产品 需要用到窗口函数 （下面再说） 1234567891011大数据处理：离线、实时（不管是离线还是实时，都要进行以下的步骤：）input ：HDFS、mysql、Hbase...... 处理之前肯定有数据输入进来（数据可以存放的位置）处理（分布式） MapReduce/Hive/Spark/Flink 处理的方式有很多种output ：mysql、HDFS..... 处理之后肯定要输出到某个地方本次案例中，从hdfs和mysql拿数据，然后用hive进行处理，然后输入到mysql中去。本次需求：统计各个城市所属区域下最受欢迎的Top 3产品 我们可以查看一下电商网站，点击一个商品，（再浏览器里的开发者工具，查看一下列如：log。gif的日志，）会发现： 日志中会有：商品信息，比如城市ID，产品ID，用户信息，但是没有城市所属区域的名字和产品的名字。我么可以从日志中获取所需要的商品信息，但是城市的名称和产品的名称日志里是没有的。 一般固定的信息，不变的信息是存放在关系型数据库中的，比如：mysql中： 12345mysql下还会存放两张表格： 城市区域对应的表 产品信息表Hive中存放的是： 用户点击行为的日志表 MySQL里有两张表，city_info城市信息表、product_info产品信息表：city_info城市信息表：product_info产品信息表： 在user_click.txt中有5列，第一列是用户id，第二列是session id 不需要关心，第三列是访问的时间，第四列是城市的id，第五列是产品的id。 现在用hive创建一张 用户点击行为日志表 ： 12345678create table user_click(user_id int,session_id string,action_time string,city_id int,product_id int) partitioned by (day string)row format delimited fields terminated by &apos;,&apos;; 然后加载数据进去： 12load data local inpath &apos;/home/hadoop/data/topn/user_click.txt&apos; overwrite into table user_click partition(day=&apos;2016-05-05&apos;); 就是说现在有三张表：city_info城市信息表、product_info产品信息表（MySQL），user_click用户点击行为日志表（hive） 现在需要在hive中操作这三张表，所以需要把MySQL中的两张表想办法弄到hive上来。在Hive里面完成我们的业务逻辑统计操作，在hive中处理过后再把处理的结果输出到MySQL中去。在这里就需要一个工具：Sqoop。 Sqoop也是个顶级项目。网址：http://sqoop.apache.org/ 123Sqoop：关系型数据库 &lt;==&gt; Hadoop Sqoop是一个工具，可以在hadoop和关系型数据库之间高效的批量转移数据。就是把hadoop的数据传输到关系型数据库，或者把关系型数据库的数据传输到hadoop上。 Sqoop有两个版本 ：一和二 Sqoop: 1.4.7Sqoop2: 1.99.7 （用的很少，不好用）下载Sqoop软件：wget http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.7.0.tar.gz 解压：tar -zxvf sqoop-1.4.6-cdh5.7.0.tar.gz 解压之后把MySQL的驱动拷贝到lib目录下： cp mysql-connector-java.jar sqoop-1.4.6-cdh5.7.0/lib/ 配置：进入到/home/hadoop/app/sqoop-1.4.6-cdh5.7.0/conf 目录：vi sqoop-env.sh 编辑一下： 1234 加入这几行：export HADOOP_COMMON_HOME=/home/hadoop/software/compile/hadoop-2.6.0-cdh5.7.0export HADOOP_MAPRED_HOME=/home/hadoop/software/compile/hadoop-2.6.0-cdh5.7.0export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0 配置一下环境变量：[hadoop@10-9-140-90 ~]$ vi .bash_profile 加入这两行： 123export SQOOP_HOME=/home/hadoop/app/sqoop-1.4.6-cdh5.7.0export PATH=$SQOOP_HOME/bin:$PATHsource生效一下。 然后就可以启动sqoop了： 用 sqoop help 可以查看sqoop的用法： 12345导入导出的出发点是Hadoop导入(import)： MySQL ==&gt; Hadoop导出(export)： Hadoop ==&gt; MySQL可以 sqoop import --help 查看import的具体用法。 ==现在需要把MySQL上的城市区域信息和产品信息导入到hive中。== 1234567891011121314 MySQL ==&gt; Hive 需要先在hive中建立两张表： create table city_info(city_id int,city_name string,area string)row format delimited fields terminated by &apos;\t&apos;;create table product_info(product_id int,product_name string,extend_info string)row format delimited fields terminated by &apos;\t&apos;; ==然后用sqoop工具把数据导入进来：== 1234567891011121314通过：sqoop import --help 看一下如何导入的。sqoop import \--connect jdbc:mysql://localhost:3306/ruozedata \--username root --password 123456 \--delete-target-dir \--table city_info \--hive-import \--hive-table city_info \--hive-overwrite \--fields-terminated-by &apos;\t&apos; \--lines-terminated-by &apos;\n&apos; \--split-by city_id \-m 2 看上面的语句，总结下来就是，前面先是连接到数据库、用户名、密码、表名，后面是 导入到hive、导入到hive的表名、重写、字段分隔符、行分隔符。 加delete-target-dir 是因为跑MapReduce的时候，如果指定的目录已经存在会报错，所以加上这个，如果目录存在就删除。 加split-by 是因为 在Sqoop里面默认的mapper数是4，它处理会以主键id进行区分，比如如果有40条记录，那么就是按照主键id进行分配，每个mapper处理10条数据。但是我们的表是没有主键的，所以在这里要手动指定一下。 -m 2 指定map的数量为2 m不指定的话就是4 ==导入的时候报错了：==Exception in thread “main” java.lang.NoClassDefFoundError: org/json/JSONObject这是由于sqoop缺少java-json.jar包 导致的，下载这个jar包，然后放到/home/hadoop/app/sqoop-1.4.6-cdh5.7.0/lib/下即可。下载：weget http://www.java2s.com/Code/JarDownload/java-json/java-json.jar.zip下载后需要解压一下。 然后再运行import语句，发现还有错误：这是因为缺少hive下的hive-exec-1.1.0-cdh5.7.0.jar这个jar包导致的，需要把 /home/hadoop/app/hive-1.1.0-cdh5.7.0/lib/hive-exec-1.1.0-cdh5.7.0.jar 拷贝到 ~/app/sqoop-1.4.6-cdh5.7.0/lib/ 下。 12345678910111213141516然后再运行语句，运行成功了，但是去d6_test数据库查还是没有，仔细检查上面那个语句发现少了个 --hive-database d6_test \ ，然后发现数据导入到default数据库里面了。sqoop import \--connect jdbc:mysql://localhost:3306/ruozedata \--username root --password 123456 \--delete-target-dir \--table city_info \--hive-import \--hive-database d6_test \--hive-table city_info \--hive-overwrite \--fields-terminated-by &apos;\t&apos; \--lines-terminated-by &apos;\n&apos; \--split-by city_id \-m 2 然后就可以看到了： 到现在 前期的准备工作已经做完，‘ Hive中已经有三张表： city_info product_info user_click 下面要做的就是统计分析： ==先做一张 商品基础信息 表== 12345678create table tmp_product_click_basic_infoasselect u.product_id, u.city_id, c.city_name, c.areafrom(select product_id, city_id from user_click where day=&apos;2016-05-05&apos; ) ujoin(select city_id, city_name,area from city_info) con u.city_id = c.city_id ; ==再创建一张 各区域下各商品的访问次数 的表== 12345678create table tmp_area_product_click_count as select product_id, area, count(1) click_count from tmp_product_click_basic_info group by product_id, area ; ==再创建一张 获取完整的商品信息的各区域的访问次数 的表：== 1234567create table tmp_area_product_click_count_full_infoasselect a.product_id, b.product_name, a.area, a.click_countfrom tmp_area_product_click_count a join product_info bon a.product_id = b.product_id ==最后需要一个窗口函数，根据区域进行分组，然后过滤：== 这个就是所要的结果： create table area_product_click_count_top3 asselect t.*,’2016-05-05’ as dayfrom (selectproduct_id, product_name,area, click_count,==row_number() over(partition by area order by click_count desc) rank ==（窗口函数，通过区域分区，再通过点击量排名）from tmp_area_product_click_count_full_info) t where t.rank &lt;=3;（在获取最后的结果时，要将day带上，我们是通过分区建表的，最后当然要将day带上，这样才知道统计的是哪一天的。） ==最后把统计结果输出到MySQL 用 sqoop export。== 12345678910在MySQL中创建一张一样的表：create table area_product_click_count_top3 (product_id int(11),product_name varchar(255),area varchar(255),click_count int(11),rank int(11),day varchar(255))ENGINE=InnoDB DEFAULT CHARSET=utf8; ==然后用 sqoop export 把hive的这张表的数据导入进来。== 123456789101112用下面语句：sqoop export \--connect jdbc:mysql://localhost:3306/ruozedata \--username root --password 123456 \--table area_product_click_count_top3 \--columns product_id,product_name,area,click_count,rank,day \--export-dir /user/hive/warehouse/d6_test.db/area_product_click_count_top3 \--input-fields-terminated-by &apos;\001&apos; \-m 2运行成功后，去mysql里看一下： 再重新运行一下上面的sqoop export 语句，再看： ==发现重复了。这时需要加上两行 去重：（下面蓝色标注）==sqoop export \–connect jdbc:mysql://localhost:3306/ruozedata \–username root –password 123456 \–table area_product_click_count_top3 \–columns product_id,product_name,area,click_count,rank,day \–export-dir /user/hive/warehouse/d6_test.db/area_product_click_count_top3 \ –update-key product_id \–update-mode updateonly \ –input-fields-terminated-by ‘\001’ \-m 2 另外：上面的过程如果每天需要处理，不可能每次都要像上面一样处理，需要把上面的整个过程放在shell脚本里面，每天凌晨去执行处理昨天的数据，这个就是离线处理。离线处理的时间一般都比较久，比如几小时、十几个小时等。 另外，上面很多group by 这可能会导致数据倾斜，那么数据倾斜应该怎么去解决？？？（面试必问）]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 进阶]]></title>
    <url>%2F2019%2F04%2F07%2FHive%20%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[** 一.分区表:(静态分区和动态分区) PARTITION 分区表：** 分区表： 话务记录、日志记录 rdbms 记录表是要分表的，因为生产上数据量是很大的，这样可以提高性能，可以当作是分表，将每一天的记录分成一张表：call_record_20190808​ call_record_20190809​ call_record_20190810​ 比如：大数据 分区表​ /user/hive/warehouse/emp/d=20190808/…..​ /user/hive/warehouse/emp/d=20190809/…..​​ 当你要查询某一天的记录时只需要：select …. from table where d=’20190808’ 就行了，记住要加上where条件。where后面带上分区条件，它会去相应的分区中查找而不需要在整张表中查询，提高了性能。 大数据经常遇到的瓶颈问题：IO 有几个方面：磁盘（disk） IO 第二个：网络（network）IO 以后在优化的过程中必然要考虑的两点。 下面是分区的练习： 在/home/hadoop/data/目录下有个order.txt订单文件：（有订单编号和时间两个字段) 创建一张分区表order_partition： create table order_partition(order_no string,event_time string)PARTITIONED BY(event_month string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’; 把数据加载到表里：load data local inpath ‘/home/hadoop/data/order.txt’ overwrite into table order_partitionPARTITION (event_month=’2014-05’); 加载之后查询一下： 从上面看到有三列，前面两列是真正的列，是字段名，最后一列并不是真正的列，它只是分区的一个标识，是伪列。desc可以看一下： 然后我们用另一种方式，然后在order_partition下面手动创建一个分区event_month=2014-06 然后把order.txt文件丢进去：然后再去hive里查一下这张表：发现并没有2014-06的分区；为什么？？？是因为2014-06这个分区是手动去创建的，并不会在mysql的元数据里。你去查的话是查不到的。 可以去mysql的partitions表里看一下：根据官网写好这个语句加个分区：ALTER TABLE order_partition ADD IF NOT EXISTS PARTITION (event_month=’2014-06’) ; 然后再查一下就有分区了：那么在hive里如何查看表有哪些分区？？ show partitions 表名; ：（这些都是从mysql的元数据里查出来的） 上面是创建一级分区，怎样创建多级分区？？create table order_mulit_partition(order_no string,event_time string)PARTITIONED BY(event_month string, step string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’; 然后加载数据到表里：load data local inpath ‘/home/hadoop/data/order.txt’ overwrite into table order_mulit_partitionPARTITION (event_month=’2014-05’, step=’1’); （上面是通过网页查看）什么时候会用到多级分区？？比如 数据量很大，按照 天 进行分区，然后还是很大，再按照小时进行分区。如果你去查的话，按照小时去查会更快。 在生产上面，数据量大的话会有好几层分区。 在写查询语句的时候，一定把条件中分区写到最底层，不然数据量很大的话，可能会被刷屏：select * from order_mulit_partition where event_month=’2014-05’ and step=’1’;上面有一级分区、多级分区，这些都是静态分区。 还有动态分区。 （小技巧：获取一张表的创建语句： show create table 表名） 现在先创建一张静态分区表emp_static_partition： CREATE TABLE emp_static_partition( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)partitioned by(deptno int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’; 然后向表里插入数据（这个要跑MapReduce的）： 插入deptno=10的数据到deptno=10分区里： insert into table emp_static_partition PARTITION (deptno=10)select empno,ename,job,mgr,hiredate,sal,comm from empwhere deptno=10; 然后查一下数据：select * from emp_static_partition where deptno=10;插入deptno=20的数据到deptno=20分区里： insert into table emp_static_partition PARTITION (deptno=20)select empno,ename,job,mgr,hiredate,sal,comm from empwhere deptno=20; 然后deptno=30、40、50……. 加入有1万个部门呢？是不是要去insert 1万次？ 然后就有了动态分区： 动态分区：按照部门编号写到指定的分区中去 先创建一张动态分区表：（创建和静态分区创建是一样的） CREATE TABLE emp_dynamic_partition( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double)partitioned by(deptno int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’; 然后向动态分区表里插入数据：insert into table emp_dynamic_partition PARTITION (deptno)select empno,ename,job,mgr,hiredate,sal,comm,deptno from emp; （注意：第一个deptno是分区，后面不要加条件，只是一个key，不要value；第二个deptno，前面是插入的字段，而deptno是根据它把数据分到相应的分区里去，根据后面deptno这个字段分到前面deptno这个分区里）一定要将分区加到select 字段的最后一个。（这个时候报错了，因为遵循严格模式，按照提示，把它修改成非严格模式即可）set hive.exec.dynamic.partition.mode=nonstrict insert完之后： 如果是多级分区（要有deptno、step字段）： insert into table emp_dynamic_partition PARTITION (deptno，step)select empno,ename,job,mgr,hiredate,sal,comm,deptno，step from emp; ** 二、用hiveserver2和beeline 访问hive：**上面hive都是hive回车，在里面输入命令，进行操作。除了上面这种方式，还有什么方式呢？之前使用是第2种方式，还有1和3，就是beeline和hiveserver2 这两个。hiveserver2和beeline是配合使用的。（后面spark课程中还有thriftserver+beeline 其实是一模一样） 启了一个服务之后，就可以用客户端连到这个服务上面去，就可以执行sql了。HiveServer1已经淘汰了。HiveServer2 支持多并发和授权。 一个服务+客户端。先把服务启起来，然后用客户端连进去。 现在把hiveserver2启起来。可以后端启起来： 比如：nohup命令 ： nohup /home/hadoop/app/hive-1.1.0-cdh5.7.0/bin/hiveserver2 &amp; 也可以前端启起来：（后端启的话，窗口可以关掉，但是前端启的话不能关掉窗口） 然后另外启动一个窗口，启动beeline： 用法：（参照官网） beeline -u jdbc:hive2://10-9-140-90:10000/d6_test -n hadoop这样就连进来了。在这个窗口执行sql成功后，在刚才那个前端窗口会出现一个OK。如果失败，那个窗口会出现失败以及失败的原因）也可以再打开几个窗口，执行beeline去访问（多并发访问）。以上hiveserver2和beeline只是访问hive的一种方式。可以用也可以不用，看个人习惯。三、复杂数据类型（需要掌握 ：如何存？如何取？） 官网：之前学的都是primitive_type基本数据类型。还有其他数据类型：array_type、map_type、struct_type等。 array_type： 现在有个文件：然后创建一张表： create table hive_array(name string,work_locations array)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’ COLLECTION ITEMS TERMINATED BY ‘,’; （集合的分隔符） 然后把数据加载进来： load data local inpath ‘/home/hadoop/data/hive_array.txt’overwrite into table hive_array;==那么如何取值呢==select name,work_locations[0] from hive_array; （取数组的第一个值，数组名[索引]）select name,==size(work_locations)== from hive_array; （size(数组名) 取数组的有多少成员 查看每个人的工作地点有多少） select * from hive_array where ==array_contains==(work_locations,’tianjin’); （取工作地点在天津的成员记录 用函数array_contains(数组名,’成员’)） map_type：map : key-value 有个文件：现在创建一张表： create table hive_map(id int,name string,members map&lt;string,string&gt;,age int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ （字段之间的分隔符）COLLECTION ITEMS TERMINATED BY ‘#’ （集合之间的分隔符，这里是#）MAP KEYS TERMINATED BY ‘:’; （key和value之间的分隔符，这里是逗号） 然后把数据加载进来： load data local inpath ‘/home/hadoop/data/hive_map.txt’overwrite into table hive_map; 那么如何取数据呢？？？ select id,name,age,members[‘father’] from hive_map; struct_type：结构体类型（可以存放各种格式的）：有个文件：（前面IP 后面用户信息（比如：姓名、年龄、职业、爱好等来表示一个用户的信息）） 现在创建一张表： create table hive_struct(ip string,userinfo structname:string,age:int)ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘#’COLLECTION ITEMS TERMINATED BY ‘:’; 然后加载数据进去; load data local inpath ‘/home/hadoop/data/hive_struct.txt’overwrite into table hive_struct; 查看一下：]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive DML学习]]></title>
    <url>%2F2019%2F04%2F07%2FHive%20DML%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1.经验： 在关系型数据库中，使用insert，update的情况是很多的，但是在大数据中，比如hive中，这种使用情况是很少的，基本上都用用load，把一个文件和一批文件load进hive表里，其实就是把这些文件load到hdfs中去。 2.LOAD： LOAD DATA [LOCAL] INPATH ‘filepath’ [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 …)] LOCAL：表示的是本地，就是Linux上。如果不带LOCAL，相当于这个数据在hdfs上。 ‘filepath’ ： 表示指向你数据所在的一个路径。 OVERWRITE： 表示将之前的数据覆盖。 INTO TABLE：表示追加。 PARTITION ：表示分区。首先我们先创建一张表： create table dept(deptno int,dname string,location string) row format delimited fields terminated by ‘\t’; 然后从本地LOAD数据到dept中：LOAD DATA LOCAL INPATH ‘/home/hadoop/data/dept.txt’ OVERWRITE INTO TABLE dept;（覆盖）LOAD DATA LOCAL INPATH ‘/home/hadoop/data/dept.txt’ INTO TABLE dept;（追加）从上图就可以看出OVERWRITE和 INTO TABLE的区别。下面我们试一下从hdfs上加载数据到表里： 首先现在hdfs上创建一个目录，将文件放到目录下： 然后加载数据到dept表里：LOAD DATA INPATH ‘/hive/dept/dept.txt’ OVERWRITE INTO TABLE dept;（没有LOCAL,表示从hdfs加载数据）这个时候到hdfs上看，发现文件没有了，（它被移到dept这张表默认的hdfs的路径下了，这里是：hdfs://10-9-140-90:9000/user/hive/warehouse/d6_hive.db/dept）下面我们将hive上查询出的结果写到本地（或者hdfs）文件系统上：INSERT OVERWRITE LOCAL DIRECTORY ‘/home/hadoop/data/emptmp’row format delimited fields terminated by ‘,’SELECT empno,ename FROM emp;写到hdfs上（把LOCAL去掉即可）：INSERT OVERWRITE DIRECTORY ‘/emptmp’row format delimited fields terminated by ‘,’SELECT empno,ename FROM emp;INSERT语法方面是支持的，但是生产中我们很少很少用，会产生很多小文件。 hive -e 和hive -f的使用： hive -e “select * from d6_test.emp” 或者 hive -e “use d6_test; select * from emp”，一般生产中写到脚本中，通过执行脚本，执行。这种不进入hive就可以查询表的情况适合？适合写脚本，比如写一个脚本，在里面写hive语句：hive -f 的使用：将sql写到文件中， 从上面可以看出，可以把一大堆sql写到一个sql文件里面，然后用hive -f去执行这个sql文件。然后如果每天需要执行一次，crontab -e ，添加一个计划即可。 hive里的清屏：!clear 常用sql语法：where = &gt; &gt;= &lt; &lt;=limitbetween and [](not) in 聚合函数：max min sum count avg 多进一出每个部门的平均工资1) 拿到每个部分的信息2) 在1)的基础之上求平均工资select deptno,avg(sal) from emp group by deptno; （出现select中的字段要么出现在group by中，要么出现在聚合函数中） select deptno,avg(sal) avg_sal from emp group by deptno having avg_sal &gt;=2000; case when then 常用于报表中： select ename, sal,casewhen sal &gt; 1 and sal &lt;=1000 then ‘lower’when sal &gt; 1000 and sal &lt;=2000 then ‘middle’when sal &gt; 2000 and sal &lt;=3000 then ‘high’else ‘highest’ endfrom emp;函数 build-in（内置的函数：hive本身自带的函数） （UDFs外置的函数：自定义的函数）看官网，在这里面可以找到相应的函数以及使用说明： 在hive里面使用： show functions; 可以看到hive所有内置的函数： 用 desc function 函数名称; 可以查看这个函数的相关说明： (官网)时间相关的函数：当前时间：current_date当前具体时间：current_timestamp 时间戳：unix_timestamp()（经常用）nix_timestamp()时间戳可以传入参数进行转换。比如select unix_timestamp(‘2019-03-09 13:41:15.841’, ‘yyyy-MM-dd hh:mm:ss’) from dual;求每个月月底： 其它函数：round() 四舍五入 ceil(x) 取不小于x的最小整数 floor(x) 取不大于x的最大整数 substr() 取子字符串 concat() 连接多个字符串 concat_ws() 连接两个字符串（有分隔符） split() 根据分隔符去拆分字符串 ==下面我们用Hive运算一个wc的案例==create table hive_wc(sentence string);load data local inpath ‘/home/hadoop/data/hive_wc.txt’ into table hive_wc; 分开之后每一行变成了数组的形式。现在已经把它们分隔开了，但是我们如果想要的结果是下面这样子：helloworldhellohelloworldwelcomehello==这属于行转列、列转行。(一个非常经典的面试题目)==需要借助一个函数explode;select explode(split(sentence,”\t”)) from hive_wc;再进行分组，count，就可以计算出wordcount了：select word,count(1) from (select explode(split(sentence,”\t”)) as word from hive_wc)t group by word;select word, count(1) as countfrom(select explode(split(sentence, “\t”)) as word from hive_wc ) tgroup by wordorder by count desc;]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive的DDL学习]]></title>
    <url>%2F2019%2F04%2F07%2FHive%E7%9A%84DDL%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1.小知识点： ① Ｈive数据一共分为两部分，一种是以数据形式存储在hdfs上，另一种是以元数据的形式存储在数据库上或者是关系型数据库上（元数据相关的配置在hive-size.xml中） ② 在启动Ｈive时，一定要先将hdfs和yarn先启动起来。③当启动 Hive时，会发现有一些错误，这些错误有的很短，我们应该去哪里查看错误的详细情况： 官方文档都是以.template结尾，需要用的时候，cp一份将.template去掉，再进行修改，hive的日志配置在这里配置：hive-log4j.properties.template，进入文件，会有下面两行信息： hive.log.dir=${java.io.tmpdir}/${user.name} hive.log.file=hive.log第一个代表日志存放的位置：/tmp目录下（这里的tmp指的是根目录下的tmp）用户的名称（这里指的是hadoop） ④ 一个非常经典的错误：An exception was thrown while adding/validating class(es) : Specified key was too long; max key length is 767 bytes⑤ 如何使用Hive；首先打开官网，我们要学会查看官网，http://hive.apache.org/ 官网上是最权威的，不要百度，谁知道你找的是哪个版本的。 这几个是常用的DDL语法； 在Hive中，DB/TABLE/PARTITION（数据库、表、分区） 都是目录或者文件夹 在Hive里组成方式要么是文件夹，要么是文件。 ⑦DDL学习： Create Database： CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] （对数据库做一个描述） [LOCATION hdfs_path] （指定一个存放数据库的路径（因为数据库组织方式是文件夹）） [WITH DBPROPERTIES (property_name=property_value, …)]; （加上DB的一些属性） 必选 （二选一）【可选】 必选 【可选】 【可选】 【可选】 比如：CREATE DATABASE hive; 备注：[LOCATION hdfs_path] 如果不指定路径，那么就会使用默认的路径。 这里的信息是：hdfs://10-9-140-90:9000/user/hive/warehouse/test.db ⑴hdfs://10-9-140-90:9000 ：这个是 HDFS目录，可以在core-site.xml文件里查到（有些hadoop是8020端口）：/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/core-site.xml⑵/user/hive/warehouse/：默认的hive存储在HDFS上的目录 从hive官网的Hive里面进入，然后找到下图的Hive Configuration Properties：Hive所有配置信息在这里都可以找到。打开后搜一下找到下面这个:从这个可以看到hive数据仓库的元数据信息的默认路径是：/user/hive/warehouse下面讲一下在hive中，修改hive参数的两种形式：1） set hive.metastore.warehouse.dir;​ set key 取值​ set key=value 设置值​ 这种设置是局部的，只对当前窗口有效，是单session的。 2）配置hive-site.xml在这里面配置出你要修改的参数，这里的修改是全局的。上面两个各有优缺点，你设置了第二种方式，可能会影响到他人的使用。用第一种也最好在你用完之后，把参数再设置回去。③hdfs://10-9-140-90:9000/user/hive/warehouse/test.db中test.db是数据的名称，固定的格式 数据库.db 后面都要加个db。创建database时指定路径，创建在哪里。比如hive 元数据： hive元数据是存放在mysql里面的。在hive-site.xml里面配置。 登上mysql数据库查看：select * from dbs \G; 查看一下dbs这张表：（ \G表示格式化一下） 这里面就是元数据信息。 row format/ file format：行分隔符和文件分隔符​ 两大分隔符：行与行 字段与字段之间的分隔符​ 列分隔符（默认是）：\001 行与行之间的分隔符默认是 换行符（我们只需要管列就好）​ file格式：行式 列式 数据类型：常用的基本用这些就够了 数值类型： int bigint float double DECIMAL字符串：string （包括date类型也用string来表示，这样会方便一些） 小知识点补充;你用数据库的时候，不知道用的是哪个数据库，看不到相关信息，可以这样设置：把hive.cli.print.current.db这个参数修改成true就可以了。这个是在当前窗口生效，如果想在全局生效，需要修改hive-site.xml文件： 加入这几行： hive表的创建（一定要掌握的）： 下面创建一张这个txt存放的这张表： （可以参照官网，不过掌握下面这个常用的基本差不多了）create table emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;最后一句是列与列之间的分隔符是’\t’，行与行的分隔符默认是回车，这里不用写了。 （如果出现下图这种常见的错误：（这个可能跟字符集有关，比如utf-8 lating gbk32什么的）需要保证hive的字符集与mysql的元数据数据库的字符集保持一致。alter database ruoze_d6 character set latin1;use ruoze_d6;alter table PARTITIONS convert to character set latin1;alter table PARTITION_KEYS convert to character set latin1;） DML语句： LOAD加载数据：官网有详细解释LOAD DATA [LOCAL] INPATH ‘filepath’ [OVERWRITE] INTO TABLE tablename ;LOCAL: 从本地(linux)加载数据 ，如果没有写，就是从hdfs上加载。 LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE emp ; 如何设置列名： 在hive-site.xml里面设置下面两个参数即可。看需要去设置。也可以不设置，直接在当前session 进行set。 创建表结构，不含数据：CREATE TABLE emp2 LIKE emp;创建emp一样的表，并copy数据到新表里面：create table emp3 as select * from emp;重命名表：ALTER TABLE emp3 RENAME TO new_emp3;推荐查看方式： desc formatted emp;内部表/外部表：（面试经常遇到） 从上图可以看出，有个Table Type：MANAGED_TABLE MANAGED_TABLE：内部表 （hive里面默认是内部表）比如： create table emp_managed as select * from emp; （这种创建的都是默认的内部表） 去mysql里面查一下tbls这张元数据表， select * from tbls \G; 可以看到有条记录： 外部表： 现在创建一张外部表： create EXTERNAL table emp_external(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’location ‘/d6_hive/external’; mysql里面tbls表中是可以查到emp_external这个表的。 但是刚创建的表是没有数据的，现在用以下命令把本地的数据上传上去。 然后把表给删掉：然后再去mysql里面查： select * from tbls \G; 就没有emp_external这张表了。 然后再去hdfs上查看，发现数据没有删除： 所以： 删除内部表：数据+元数据 删除删除外部表：数据不删，元数据删 1）内部表和外部的区别以及使用场景其它项目组也用防止误删表 2）梳理元数据信息表中的DBS和TBLS中的字段信息 desc哪里来的底层拼出来的SQL查询出来的]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive的部署]]></title>
    <url>%2F2019%2F04%2F07%2FHive%E7%9A%84%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1.hive（数据仓库:data warehouse） 123456 构建再Hadoop之上的数据仓库 数据：HDFS 执行：MR（2.0过时）Spark Tez 运行：YARN Hive 是一个使用SQL来操作分布式存储系统上面的大数据集的读写和管理操作的一个客户端，Hive它不是一个集群。用JDBC去连接Server的话，不应该是走查询统计分析，而是去拿到统计结果，只拿结果，不做计算。 2.有人说Hive不难，就是写SQL实现（这是错误的说法） 1架构层面，语法层面，底层执行层面，考虑优化 3.install hive 12wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gz上传完以后将文件解压到 app文件夹下 4.配置环境变量 1234vi ~/.bash_profileexport HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0export PATH=$HIVE_HOME/bin:$PATH配置完成后不要忘记：source ~/.bash_profile，其他开启的窗口也要执行 5.hive的存储 12345678910 由于hive是构建再Hadoop之上的数据仓库，那么他的存储数据的位置也在hdfs中。 但是它的元数据（metadata）信息存储的位置是 mysql中。 元数据：描述数据的数据。 1、MR编程不便性 2、传统的RDBMS人员的需求 HDFS上面的文件就是普通的文件，它并没有schema的概念 schema：RDBMS中的表结构 people.txt &lt;== id name age address sql ===&gt; 搞定海量数据的统计分析===&gt; 产生Hive 6.安装步骤 1234567891011121314151617181920212223242526 1）下载 2）解压到~/app 3）bin添加到环境变量 4）拷贝mysql的驱动到lib下 5）hive-site.xml配置mysql相关信息（hive-site.xml在conf下） &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/ruoze_d6?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;（这里注意 密码要修改成自己的） &lt;/property&gt; 7.Hive vs RDBMS 12都可以使用SQL时效性：Hive底层是使用MR的，主要是做离线任务的，比较慢。 8.单点问题： 12Hive和mysql的链接中，mysql属于单点，如果mysql挂了，那么元数据也就没了，生产中我们一般需要让运维人员帮我们搭一个主备Hive属于客户端，虽然它没有集群概念，但是在生产中我们也要配备多个，如下图所示，在生产中 比如有三台机器都装有Hive，在这之上会有一个调度系统，调度系统定时的会将你的作业提交到执行机上（Executor），这时如果一个Hive坏了没有关系，可以用别的机器提交。]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN生产上调度器]]></title>
    <url>%2F2019%2F04%2F07%2FYARN%E7%94%9F%E4%BA%A7%E4%B8%8A%E8%B0%83%E5%BA%A6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1.生产上 job去申请调度资源时： 1234规则： FIFO 先进先出 Capacity 计算 Fair 公平 生产 123456789图解： FiFO:表示 先进先出调度器 假设总共由40G的内存容器，job1在0点时进入运行作业，那么40G的内存全部运行job1，当job2在1点请求运行的时候，它需要等job1作业全部运行完成释放资源以后，再运行job2作业。 Capacity：表示 计算调度器 假设总共有40G的内存容器，它会分成两部分，5G的内村容器留给小的作业运算，它会一直运算小作业，不释放，剩下的35G运行其他作业。 Fair：表示 公平调度器（生产上都是用这一种） 假设一共有42G的内存容器，0点的时候job1任务先申请运算，然后会将40G的容器全部用作job1的运算中，当1带点时，job2请求运算作业，这时会分给job2部分容器，让它同时运行，&gt; 注意：这时会有延迟的，必须要等到job1有资源释放出来之后，才会给job2运算。 CDH 动态资源池 放置规则 12345&lt;queuePlacementPolicy&gt; &lt;rule name=&quot;specified&quot; /&gt; &lt;rule name=&quot;primaryGroup&quot; create=&quot;false&quot; /&gt; &lt;rule name=&quot;default&quot; queue=&quot;ABC&quot;/&gt;&lt;/queuePlacementPolicy&gt; JOB -queue ABC 主组 jepson bigdata root 生产上怎么配置：参考官网：http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot;?&gt;&lt;allocations&gt; &lt;queue name=&quot;ABC&quot;&gt; &lt;minResources&gt;10000 mb,10vcores&lt;/minResources&gt; &lt;maxResources&gt;60000 mb,30vcores&lt;/maxResources&gt; &lt;weight&gt;2.0&lt;/weight&gt; &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt; &lt;/queue&gt; &lt;queue name=&quot;XYZ&quot;&gt; &lt;minResources&gt;20000 mb,0vcores&lt;/minResources&gt; &lt;maxResources&gt;80000 mb,0vcores&lt;/maxResources&gt; &lt;weight&gt;3.0&lt;/weight&gt; &lt;schedulingPolicy&gt;fifo&lt;/schedulingPolicy&gt; &lt;/queue&gt;&lt;queueMaxResourcesDefault&gt;40000 mb,20vcores&lt;/queueMaxResourcesDefault&gt;&lt;queuePlacementPolicy&gt; &lt;rule name=&quot;specified&quot; /&gt; &lt;rule name=&quot;primaryGroup&quot; create=&quot;false&quot; /&gt; &lt;rule name=&quot;default&quot; queue=&quot;ABC&quot;/&gt; &lt;/queuePlacementPolicy&gt;&lt;/allocations&gt; https://netjs.blogspot.com/2018/04/fair-scheduler-in-yarn-hadoop.html]]></content>
      <tags>
        <tag>YARN</tag>
        <tag>大数据开发</tag>
        <tag>调度器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN生产上的优化]]></title>
    <url>%2F2019%2F04%2F07%2FYARN%E7%94%9F%E4%BA%A7%E4%B8%8A%E7%9A%84%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1.yarn生产上的资源管理（至关重要） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100 假设一台机器：48物理内存 8个core--&gt;16个vcore Linux本身要占用内存+空留：20%=9.6个G（空留是根据实际机器的内存来决定的，如果内存大，可以考虑空留15%） 剩余：80%=38.4G=38G DN进程：生产上4G 如何修改DN的内存配置信息：进入hadoop-env.sh脚本， HADOOP_NAMENODE_OPTS=-Xmx1024m HADOOP_DATANODE_OPTS=-Xmx1024m （在前面追加，如下图所示） NM进程：生产上4G 如何修改NM的内存配置信息：进入yarn-env.sh脚本， export YARN_RESOURCEMANAGER_HEAPSIZE=1024 export YARN_NODEMANAGER_HEAPSIZE=1024 DN和NM部署在同一台机器上是为了：数据本地化 NN RM 经常性部署同一台 说白了 集群节点少 (正常情况下，DataNode进程和NodeManager进程部署在同一台机器： 数据本地化比如：在NM上运行task任务，task任务需要数据，数据是放在当前机器的hdfs上面的，发现当前有DataNode进程，节点上有需要的数据，那就只需要从当前节点拿数据就行了。但是如果发现当前机器上没有数据，要去另外的机器上拿数据，那就会通过网络，这时候会网络消耗，计算就会变慢了。） 资源内存： 38G-4-4=30G（这30G是做什么？就是运行container容器） yarn.nodemanager.resource.memory-mb 30*1024MB（这是总的大小） 默认配置 yarn.scheduler.minimum-allocation-mb 1024 （最小可分配容器的大小） yarn.scheduler.maximum-allocation-mb 8192 （最大可分配容器的大小） （分配的容器的大小就是根据上面的参数来进行分配的） 30G 30G/1G=30个container 30G 30/8=3个container ...6G (还留有6个G的内存被浪费掉) 30个~3个（这样是不合理的） 生产一： yarn.nodemanager.resource.memory-mb 30*1024MB yarn.scheduler.minimum-allocation-mb 2G yarn.scheduler.maximum-allocation-mb 30G 15个~1个 （这个是大作业的情况。要看每个公司的作业情况，如果作业特别大的情况，就设成和总的一样大） （容器最小2G，就是这台机器最多可以运行15个容器，容器最大30G，就是这台机器最多可以运行1个容器。那这台机器可以运行1-15个容器。当你去请求资源的时候，它先给你分配一个最小的值，比如说2G，然后你如果不够，给你加比如说1G，直到加到最大比如说30G。（这个是有参数可以去配置的，在hadoop官网没有，在cdh官网是可以找到的） 生产二： yarn.nodemanager.resource.memory-mb 32G（可以将预留空间再缩小一点，这样留给container容器的空间会大一点）（这个由30G调整到32G，就需要从预留的20%里面拿出2G出来。这样32/8=4个容器，可以整除，不会浪费了） yarn.scheduler.minimum-allocation-mb 2G yarn.scheduler.maximum-allocation-mb 8G 16个~4个 生产三: 如果物理机的内存是256G:（我们预留的空间可以按15%来算） yarn.nodemanager.resource.memory-mb 168G yarn.scheduler.minimum-allocation-mb 4G yarn.scheduler.maximum-allocation-mb 24G 16个~4个 再生产中很容易遇到这样一种情况：container p memory oom ：（意思是你的最大connertion内存不够用） 遇到这种情况，我们首先先把这任务kill掉，然受修改参数，将分配的内存调大一些。 生产默认 不做修改 yarn.nodemanager.pmem-check-enabled true （表示如果 container tast任务超过内存 就会 kill 掉进程， 检查容器内存，一个是物理内存，一个是虚拟内存，虚拟内存是物理内存的 2倍，哪个超了都会被kill 掉） yarn.nodemanager.vmem-check-enabled true yarn.nodemanager.vmem-pmem-ratio 2.1 物理内存 1m 虚拟内存 2.1m新版本参数yarn.nodemanager.resource.pcores-vcores-multiplier 1 （老的是2，现在是1，需要手动把它修改成2）yarn.nodemanager.resource.memory-mb yarn.scheduler.minimum-allocation-mb yarn.scheduler.maximum-allocation-mb 这些参数的需要看官网里的默认配置yarn-default.xml。比如：yarn.nodemanager.resource.memory-mb如果设置成-1：Amount of physical memory, in MB, that can be allocated for containers. If set to -1and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the defaultis 8192MB.yarn.nodemanager.resource.detect-hardware-capabilities：Enable auto-detection of nodecapabilities such as memory and CPU.（默认是false） CPU: yarn.nodemanager.resource.cpu-vcores 12（一般vcore是16个，但是我们还要分给其他进程。） yarn.scheduler.minimum-allocation-vcores 1 yarn.scheduler.maximum-allocation-vcores 4 container: 2G 3c container: memory 16c~4c vcores 12c~3c &gt; 生产调优的重点：&gt; 在生产中，运行task时所需要的资源需要由内存和vcore共同来决定，它俩时相互影响的，如果vcore&gt; 最小的只有3个crontainer，那么memory也只能分到3个crontainer，那么3*8=24G，还余6个G&gt; 的内存就会被浪费掉，所以yarn生产调优 需要计算好内存和vcore 的之间的数据。如果vore分给&gt; 它16个，那么vcore就可以分成16个~4个crontainer容易，最大利用了资源。 详细可以参考 博客 ：http://blog.itpub.net/30089851/viewspace-2127851/]]></content>
      <tags>
        <tag>YARN</tag>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce优化----Shuffle过程剖析及性能优化]]></title>
    <url>%2F2019%2F04%2F07%2F%5B%E8%BD%AC%E8%BD%BD%5DMapReduce%E4%BC%98%E5%8C%96----Shuffle%E8%BF%87%E7%A8%8B%E5%89%96%E6%9E%90%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1. Map端 当Map 开始产生输出时，它并不是简单的把数据写到磁盘，因为频繁的磁盘操作会导致性能严重下降。它的处理过程更复杂，数据首先是写到内存中的一个缓冲区，并做了一些预排序，以提升效率。 每个Map 任务都有一个用来写入输出数据的循环内存缓冲区。这个缓冲区默认大小是100MB，可以通过io.sort.mb 属性来设置具体大小。当缓冲区中的数据量达到一个特定阀值(io.sort.mb * io.sort.spill.percent，其中io.sort.spill.percent 默认是0.80)时，系统将会启动一个后台线程把缓冲区中的内容spill 到磁盘。在spill 过程中，Map 的输出将会继续写入到缓冲区，但如果缓冲区已满，Map 就会被阻塞直到spill 完成。spill 线程在把缓冲区的数据写到磁盘前，会对它进行一个二次快速排序，首先根据数据所属的partition 排序，然后每个partition 中再按Key 排序。输出包括一个索引文件和数据文件。 如果设定了Combiner，将在排序输出的基础上运行。Combiner 就是一个Mini Reducer，它在执行Map 任务的节点本身运行，先对Map 的输出做一次简单Reduce，使得Map 的输出更紧凑，更少的数据会被写入磁盘和传送到Reducer。 spill 文件保存在由mapred.local.dir指定的目录中，Map 任务结束后删除。 每当内存中的数据达到spill 阀值的时候，都会产生一个新的spill 文件，所以在Map任务写完它的最后一个输出记录时，可能会有多个spill 文件。在Map 任务完成前，所有的spill 文件将会被归并排序为一个索引文件和数据文件。这是一个多路归并过程，最大归并路数由io.sort.factor 控制(默认是10)。如果设定了Combiner，并且spill文件的数量至少是3（由min.num.spills.for.combine 属性控制），那么Combiner 将在输出文件被写入磁盘前运行以压缩数据。 对写入到磁盘的数据进行压缩，通常是一个很好的方法，因为这样做使得数据写入磁盘的速度更快，节省磁盘空间，并减少需要传送到Reducer 的数据量。默认输出是不被压缩的， 但可以很简单的设置mapred.compress.map.output 为true 启用该功能。压缩所使用的库由mapred.map.output.compression.codec 来设定。 当spill 文件归并完毕后，Map 将删除所有的临时spill 文件，并告知TaskTracker 任务已完成。Reducers 通过HTTP来获取对应的数据。用来传输partitions 数据的工作线程数由tasktracker.http.threads 控制，这个设定是针对每一个TaskTracker 的，并不是单个Map，默认值为40，在运行大作业的大集群上可以增大以提升数据传输速率。 2. Reduce端2.1 copy阶段 Map 的输出文件放置在运行Map 任务的TaskTracker 的本地磁盘上（注意：Map 输出总是写到本地磁盘，但Reduce 输出不是，一般是写到HDFS），它是运行Reduce 任务的TaskTracker 所需要的输入数据。Reduce 任务的输入数据分布在集群内的多个Map 任务的输出中，Map 任务可能会在不同的时间内完成，只要完成的Map 任务数达到占总Map任务数一定比例（mapred.reduce.slowstart.completed.maps 默认0.05），Reduce 任务就开始拷贝它的输出。 ​ Reduce 任务拥有多个拷贝线程， 可以并行的获取Map 输出。可以通过设定mapred.reduce.parallel.copies 来改变线程数，默认是5。 如果Map 输出足够小，它们会被拷贝到Reduce TaskTracker 的内存中（缓冲区的大小 由mapred.job.shuffle.input.buffer.percent 控制，指定了用于此目的的堆内存的百分比）；如果缓冲区空间不足，会被拷贝到磁盘上。当内存中的缓冲区用量达到一定比例阀值（由mapred.job.shuffle.merge.percent 控制），或者达到了Map 输出的阀值大小（由mapred.inmem.merge.threshold 控制），缓冲区中的数据将会被归并然后spill到磁盘。 拷贝来的数据叠加在磁盘上，有一个后台线程会将它们归并为更大的排序文件，这样做节省了后期归并的时间。对于经过压缩的Map 输出，系统会自动把它们解压到内存方便对其执行归并。 2.2 sort阶段 当所有的Map 输出都被拷贝后，Reduce 任务进入排序阶段（更恰当的说应该是归并阶段，因为排序在Map 端就已经完成），这个阶段会对所有的Map 输出进行归并排序，这个工作会重复多次才能完成。 假设这里有50 个Map 输出（可能有保存在内存中的），并且归并因子是10（由io.sort.factor 控制，就像Map 端的merge 一样），那最终需要5 次归并。每次归并会把10个文件归并为一个，最终生成5 个中间文件。 注：每趟合并的文件数实际上比示例中展示的更微妙。目标是合并最小数量的文件以便满足最后一趟的合并系数。因此如果是40个文件，我们不会在四趟中，每趟合并10个文件从而得到4个文件。相反，第一趟只合并4个文件，随后三趟合并所有十个文件。在最后一趟中，4个已合并的文件和余下的6个（未合并的）文件合计10个文件。这并没有改变合并的次数，它只是一个优化措施，尽量减少写到磁盘的数据量，因为最后一趟总是直接合并到reduce。 2.3 reduce阶段 在Reduce 阶段，Reduce 函数会作用在排序输出的每一个key 上。这个阶段的输出被直接写到输出文件系统，一般是HDFS。在HDFS 中，因为TaskTracker 节点也运行着一个DataNode 进程，所以第一个块备份会直接写到本地磁盘。 3. 配置调优该配置调优方案主要是对以上Shuffle整个过程中涉及到的配置项按流程顺序一一呈现并给以调优建议。 1. Map端 1) io.sort.mb 用于map输出排序的内存缓冲区大小 类型：Int 默认：100mb 备注：如果能估算map输出大小，就可以合理设置该值来尽可能减少溢出写的次数，这对调优很有帮助。 2)io.sort.spill.percent map输出排序时的spill阀值（即使用比例达到该值时，将缓冲区中的内容spill 到磁盘） 类型：float 默认：0.80 3)io.sort.factor 归并因子（归并时的最多合并的流数），map、reduce阶段都要用到 类型：Int 默认：10 备注：将此值增加到100是很常见的。 4)min.num.spills.for.combine 运行combiner所需的最少溢出写文件数（如果已指定combiner） 类型：Int 默认：3 5)mapred.compress.map.output map输出是否压缩 类型：Boolean 默认：false 备注：如果map输出的数据量非常大，那么在写入磁盘时压缩数据往往是个很好的主意，因为这样会让写磁盘的速度更快，节约磁盘空间，并且减少传给reducer的数据量。 6)mapred.map.output.compression.codec 用于map输出的压缩编解码器 类型：Classname 默认：org.apache.Hadoop.io.compress.DefaultCodec 备注：推荐使用LZO压缩。Intel内部测试表明，相比未压缩，使用LZO压缩的 TeraSort作业，运行时间减少60%，且明显快于Zlib压缩。 7) tasktracker.http.threads 每个tasktracker的工作线程数，用于将map输出到reducer。 （注：这是集群范围的设置，不能由单个作业设置） 类型：Int 默认：40 备注：tasktracker开http服务的线程数。用于reduce拉取map输出数据，大集群可以将其设为40~50。 2. reduce端 1)mapred.reduce.slowstart.completed.maps 调用reduce之前，map必须完成的最少比例 类型：float 默认：0.05 2)mapred.reduce.parallel.copies reducer在copy阶段同时从mapper上拉取的文件数 类型：int 默认：5 3)mapred.job.shuffle.input.buffer.percent 在shuffle的复制阶段，分配给map输出的缓冲区占堆空间的百分比 类型：float 默认：0.70 4)mapred.job.shuffle.merge.percent map输出缓冲区（由mapred.job.shuffle.input.buffer.percent定义）使用比例阀值，当达到此阀值，缓冲区中的数据将会被归并然后spill 到磁盘。 类型：float 默认：0.66 5)mapred.inmem.merge.threshold map输出缓冲区中文件数 类型：int 默认：1000 备注：0或小于0的数意味着没有阀值限制，溢出写将有mapred.job.shuffle.merge.percent单独控制。 6)mapred.job.reduce.input.buffer.percent 在reduce过程中，在内存中保存map输出的空间占整个堆空间的比例。 类型：float 默认：0.0 备注：reduce阶段开始时，内存中的map输出大小不能大于该值。默认情况下，在reduce任务开始之前，所有的map输出都合并到磁盘上，以便为reducer提供尽可能多的内存。然而，如果reducer需要的内存较少，则可以增加此值来最小化访问磁盘的次数，以提高reduce性能。 3.性能调优补充 相对于大批量的小文件，hadoop更合适处理少量的大文件。一个原因是FileInputFormat生成的InputSplit是一个文件或该文件的一部分。如果文件很小，并且文件数量很多，那么每次map任务只处理很少的输入数据，每次map操作都会造成额外的开销。 转自：http://blog.itpub.net/30089851/viewspace-2122878/]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>MapReduce优化</tag>
        <tag>Shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-2.7.2+zookeeper-3.4.6完全分布式环境搭建(HDFS、YARN HA)]]></title>
    <url>%2F2019%2F04%2F07%2F%5B%E8%BD%AC%E8%BD%BD%5DHadoop-2.7.2%2Bzookeeper-3.4.6%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA(HDFS%E3%80%81YARN%20HA)%2F</url>
    <content type="text"><![CDATA[Hadoop-2.7.2+Zookeeper-3.4.6完全分布式环境搭建 一.版本 组件名 版本 说明 JRE java version “1.7.0_67” Java™ SE Runtime Environment (build 1.7.0_67-b01) Java HotSpot™ 64-Bit Server VM (build 24.65-b04, mixed mode) Hadoop hadoop-2.7.2.tar.gz 主程序包 Zookeeper zookeeper-3.4.6.tar.gz 热切,Yarn 存储数据使用的协调服务 二.主机规划 IP Host 及安装软件 部署模块 进程 172.16.101.55 sht-sgmhadoopnn-01 hadoop NameNode ResourceManager NameNode DFSZKFailoverController ResourceManager 172.16.101.56 sht-sgmhadoopnn-02 hadoop NameNode ResourceManager NameNode DFSZKFailoverController ResourceManager 172.16.101.58 sht-sgmhadoopdn-01 hadoop、zookeeper DataNode NodeManager Zookeeper DataNode NodeManager JournalNode QuorumPeerMain 172.16.101.59 sht-sgmhadoopdn-02 Hadoop、zookeeper DataNode NodeManager Zookeeper DataNode NodeManager JournalNode QuorumPeerMain 172.16.101.60 sht-sgmhadoopdn-03 Hadoop、zookeeper DataNode NodeManager Zookeeper DataNode NodeManager JournalNode QuorumPeerMain 三.目录规划 名称 路径 $HADOOP_HOME /hadoop/hadoop-2.7.2 Data $ HADOOP_HOME/data Log $ HADOOP_HOME/logs 四.常用脚本及命令 1.启动集群 start-dfs.sh start-yarn.sh 2.关闭集群 stop-yarn.sh stop-dfs.sh 3.监控集群 hdfs dfsadmin -report 4.单个进程启动/关闭 hadoop-daemon.sh start|stop namenode|datanode| journalnode yarn-daemon.sh start |stop resourcemanager|nodemanager http://blog.chinaunix.net/uid-25723371-id-4943894.html 五.环境准备 *1 .**设置ip地址(5台*) 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-01 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=”eth0” BOOTPROTO=”static” DNS1=”172.16.101.63” DNS2=”172.16.101.64” GATEWAY=”172.16.101.1” HWADDR=”00:50:56:82:50:1E” IPADDR=”172.16.101.55” NETMASK=”255.255.255.0” NM_CONTROLLED=”yes” ONBOOT=”yes” TYPE=”Ethernet” UUID=”257c075f-6c6a-47ef-a025-e625367cbd9c” 执行命令: service network restart 验证:ifconfig *2 .**关闭防火墙(5台*) 执行命:service iptables stop 验证:service iptables status *3.**关闭防火墙的自动运行(5台*) 执行命令:chkconfig iptables off 验证:chkconfig –list | grep iptables *4 设置主机名**(5台)*** 执行命令 (1)hostname sht-sgmhadoopnn-01 (2)vi /etc/sysconfig/network 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-01 ~]# vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=sht-sgmhadoopnn-01.telenav.cn GATEWAY=172.16.101.1 *5 ip**与hostname绑定(5台*) 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-01 ~]# vi /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.16.101.55 sht-sgmhadoopnn-01.telenav.cn sht-sgmhadoopnn-01 172.16.101.56 sht-sgmhadoopnn-02.telenav.cn sht-sgmhadoopnn-02 172.16.101.58 sht-sgmhadoopdn-01.telenav.cn sht-sgmhadoopdn-01 172.16.101.59 sht-sgmhadoopdn-02.telenav.cn sht-sgmhadoopdn-02 172.16.101.60 sht-sgmhadoopdn-03.telenav.cn sht-sgmhadoopdn-03 验证:ping sht-sgmhadoopnn-01 *6. 设置**5台machines,SSH互相通信*http://blog.itpub.net/30089851/viewspace-1992210/ *7 .**安装JDK(5台*) 点击(此处)折叠或打开 (1)执行命令 [root@sht-sgmhadoopnn-01 ~]# cd /usr/java [root@sht-sgmhadoopnn-01 java]# cp /tmp/jdk-7u67-linux-x64.gz ./ [root@sht-sgmhadoopnn-01 java]# tar -xzvf jdk-7u67-linux-x64.gz (2)vi /etc/profile 增加内容如下: export JAVA_HOME=/usr/java/jdk1.7.0_67 export HADOOP_HOME=/hadoop/hadoop-2.7.2 export ZOOKEEPER_HOME=/hadoop/zookeeper export PATH=.:$HADOOP_HOME/bin:$JAVA_HOME/bin:$ZOOKEEPER_HOME/bin:$PATH #先把HADOOP_HOME, ZOOKEEPER_HOME配置了 #本次实验机器已经配置好了jdk1.7.0_67-cloudera (3)执行 source /etc/profile (4)验证:java –version *8.**创建文件夹(5台*) mkdir /hadoop *六**.安装Zookeeper*** *sht-sgmhadoopdn-01/02/03* *1.**下载解压*zookeeper-3.4.6.tar.gz 点击(此处)折叠或打开 [root@sht-sgmhadoopdn-01 tmp]# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz [root@sht-sgmhadoopdn-02 tmp]# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz [root@sht-sgmhadoopdn-03 tmp]# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz [root@sht-sgmhadoopdn-01 tmp]# tar -xvf zookeeper-3.4.6.tar.gz [root@sht-sgmhadoopdn-02 tmp]# tar -xvf zookeeper-3.4.6.tar.gz [root@sht-sgmhadoopdn-03 tmp]# tar -xvf zookeeper-3.4.6.tar.gz [root@sht-sgmhadoopdn-01 tmp]# mv zookeeper-3.4.6 /hadoop/zookeeper [root@sht-sgmhadoopdn-02 tmp]# mv zookeeper-3.4.6 /hadoop/zookeeper [root@sht-sgmhadoopdn-03 tmp]# mv zookeeper-3.4.6 /hadoop/zookeeper *2.**修改配置*** *点击(此处)折叠或打开* *[root@sht-sgmhadoopdn-01 tmp]# cd /hadoop/zookeeper/conf* *[root@sht-sgmhadoopdn-01 conf]# cp zoo_sample.cfg zoo.cfg* *[root@sht-sgmhadoopdn-01 conf]# vi zoo.cfg* *修改dataDir* *dataDir=/hadoop/zookeeper/data* *添加下面三行* *server.1=sht-sgmhadoopdn-01:2888:3888* *server.2=sht-sgmhadoopdn-02:2888:3888* *server.3=sht-sgmhadoopdn-03:2888:3888* *[root@sht-sgmhadoopdn-01 conf]# cd ../* *[root@sht-sgmhadoopdn-01 zookeeper]# mkdir data* *[root@sht-sgmhadoopdn-01 zookeeper]# touch data/myid* *[root@sht-sgmhadoopdn-01 zookeeper]# echo 1 &gt; data/myid* *[root@sht-sgmhadoopdn-01 zookeeper]# more data/myid* *1* *## sht-sgmhadoopdn-02/03,也修改配置,就如下不同* *[root@sht-sgmhadoopdn-02 zookeeper]# echo 2 &gt; data/myid* *[root@sht-sgmhadoopdn-03 zookeeper]# echo 3 &gt; data/myid* *七**.安装Hadoop(HDFS HA+YARN HA)*** *#step3~7,**用**SecureCRT ssh 到 linux**的环境中,假如**copy 内容从**window 到 linux 中,中文乱码,请参照修改*http://www.cnblogs.com/qi09/archive/2013/02/05/2892922.html *1.**下载解压*hadoop-2.7.2.tar.gz 点击(此处)折叠或打开 [root@sht-sgmhadoopdn-01 tmp]# cd /hadoop/zookeeper/conf [root@sht-sgmhadoopdn-01 conf]# cp zoo_sample.cfg zoo.cfg [root@sht-sgmhadoopdn-01 conf]# vi zoo.cfg 修改dataDir dataDir=/hadoop/zookeeper/data 添加下面三行 server.1=sht-sgmhadoopdn-01:2888:3888 server.2=sht-sgmhadoopdn-02:2888:3888 server.3=sht-sgmhadoopdn-03:2888:3888 [root@sht-sgmhadoopdn-01 conf]# cd ../ [root@sht-sgmhadoopdn-01 zookeeper]# mkdir data [root@sht-sgmhadoopdn-01 zookeeper]# touch data/myid [root@sht-sgmhadoopdn-01 zookeeper]# echo 1 &gt; data/myid [root@sht-sgmhadoopdn-01 zookeeper]# more data/myid 1 ## sht-sgmhadoopdn-02/03,也修改配置,就如下不同 [root@sht-sgmhadoopdn-02 zookeeper]# echo 2 &gt; data/myid [root@sht-sgmhadoopdn-03 zookeeper]# echo 3 &gt; data/myid *2.**修改*$HADOOP_HOME/etc/hadoop/hadoop-env.sh export JAVA_HOME=”/usr/java/jdk1.7.0_67-cloudera” *3.**修改*$HADOOP_HOME/etc/hadoop/core-site.xml 点击(此处)折叠或打开 &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; ​ ​ ​ fs.defaultFS ​ hdfs://mycluster ​ ​ ​ ​ dfs.permissions.superusergroup ​ root ​ ​ ​ ​ ​ fs.trash.checkpoint.interval ​ 0 ​ ​ ​ ​ fs.trash.interval ​ 1440 ​ *4.**修改$HADOOP_HOME/etc/hadoop/hdfs-site.xml*** 点击(此处)折叠或打开 &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; ​ ​ ​ dfs.webhdfs.enabled ​ true ​ ​ ​ dfs.namenode.name.dir ​ /hadoop/hadoop-2.7.2/data/dfs/name ​ namenode 存放name table(fsimage)本地目录（需要修改） ​ ​ ​ dfs.namenode.edits.dir ​ ${dfs.namenode.name.dir} ​ namenode粗放 transaction file(edits)本地目录（需要修改） ​ ​ ​ dfs.datanode.data.dir ​ /hadoop/hadoop-2.7.2/data/dfs/data ​ datanode存放block本地目录（需要修改） ​ ​ ​ dfs.replication ​ 3 ​ ​ ​ ​ dfs.blocksize ​ 268435456 ​ ​ ​ ​ ​ ​ dfs.nameservices ​ mycluster ​ ​ ​ ​ dfs.ha.namenodes.mycluster ​ nn1,nn2 ​ ​ ​ ​ dfs.namenode.rpc-address.mycluster.nn1 ​ sht-sgmhadoopnn-01:8020 ​ ​ ​ dfs.namenode.rpc-address.mycluster.nn2 ​ sht-sgmhadoopnn-02:8020 ​ ​ ​ ​ dfs.namenode.http-address.mycluster.nn1 ​ sht-sgmhadoopnn-01:50070 ​ ​ ​ dfs.namenode.http-address.mycluster.nn2 ​ sht-sgmhadoopnn-02:50070 ​ ​ ​ ​ ​ dfs.journalnode.http-address ​ 0.0.0.0:8480 ​ ​ ​ dfs.journalnode.rpc-address ​ 0.0.0.0:8485 ​ ​ ​ ​ ​ dfs.namenode.shared.edits.dir ​ qjournal://sht-sgmhadoopdn-01:8485;sht-sgmhadoopdn-02:8485;sht-sgmhadoopdn-03:8485/mycluster ​ ​ ​ ​ dfs.journalnode.edits.dir ​ /hadoop/hadoop-2.7.2/data/dfs/jn ​ ​ ​ ​ ​ dfs.client.failover.proxy.provider.mycluster ​ org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider ​ ​ ​ ​ ​ dfs.ha.fencing.methods ​ sshfence ​ ​ ​ dfs.ha.fencing.ssh.private-key-files ​ /root/.ssh/id_rsa ​ ​ ​ ​ dfs.ha.fencing.ssh.connect-timeout ​ 30000 ​ ​ ​ ​ ​ dfs.ha.automatic-failover.enabled ​ true ​ ​ ​ ha.zookeeper.quorum ​ sht-sgmhadoopdn-01:2181,sht-sgmhadoopdn-02:2181,sht-sgmhadoopdn-03:2181 ​ ​ ​ ​ ha.zookeeper.session-timeout.ms ​ 2000 ​ *5.**修改$HADOOP_HOME/etc/hadoop/yarn-env.sh*** #Yarn Daemon Options #export YARN_RESOURCEMANAGER_OPTS #export YARN_NODEMANAGER_OPTS #export YARN_PROXYSERVER_OPTS #export HADOOP_JOB_HISTORYSERVER_OPTS #Yarn Logs export YARN_LOG_DIR=”/hadoop/hadoop-2.7.2/logs” *6.**修改$HADOOP_HOEM/etc/hadoop/mapred-site.xml*** *点击(此处)折叠或打开* *[root@sht-sgmhadoopnn-01 hadoop]# cp mapred-site.xml.template mapred-site.xml* *[root@sht-sgmhadoopnn-01 hadoop]# vi mapred-site.xml* ** ​ ** ​ ** ​ *mapreduce.framework.name* ​ *yarn* ​ ** ​ ** ​ ** ​ ** ​ *mapreduce.jobhistory.address* ​ *sht-sgmhadoopnn-01:10020* ​ ** ​ ** ​ ** ​ *mapreduce.jobhistory.webapp.address* ​ *sht-sgmhadoopnn-01:19888* ​ ** ** *7.**修改$HADOOP_HOME/etc/hadoop/yarn-site.xml*** *点击(此处)折叠或打开* ** ​ ** ​ ** ​ *yarn.nodemanager.aux-services* ​ *mapreduce_shuffle* ​ ** ​ ** ​ *yarn.nodemanager.aux-services.mapreduce.shuffle.class* ​ *org.apache.hadoop.mapred.ShuffleHandler* ​ ** ​ ** ​ *Address where the localizer IPC is.* ​ *yarn.nodemanager.localizer.address* ​ *0.0.0.0:23344* ​ ** ​ ** ​ *NM Webapp address.* ​ *yarn.nodemanager.webapp.address* ​ *0.0.0.0:23999* ​ ** ​ ** ​ ** ​ ** ​ *yarn.resourcemanager.connect.retry-interval.ms* ​ *2000* ​ ** ​ ** ​ *yarn.resourcemanager.ha.enabled* ​ *true* ​ ** ​ ** ​ *yarn.resourcemanager.ha.automatic-failover.enabled* ​ *true* ​ ** ​ ** ​ ** ​ *yarn.resourcemanager.ha.automatic-failover.embedded* ​ *true* ​ ** ​ ** ​ ** ​ *yarn.resourcemanager.cluster-id* ​ *yarn-cluster* ​ ** ​ ** ​ *yarn.resourcemanager.ha.rm-ids* ​ *rm1,rm2* ​ ** ​ *&lt;!–这里RM主备结点需要单独指定,（可选）* ​ ** ​ *yarn.resourcemanager.ha.id* ​ *rm2* ** *–&gt;* ​ ** ​ *yarn.resourcemanager.scheduler.class* ​ *org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler* ​ ** ​ ** ​ *yarn.resourcemanager.recovery.enabled* ​ *true* ​ ** ​ ** ​ *yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms* ​ *5000* ​ ** ​ ** ​ ** ​ *yarn.resourcemanager.store.class* ​ *org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore* ​ ** ​ ** ​ *yarn.resourcemanager.zk-address* ​ *sht-sgmhadoopdn-01:2181,sht-sgmhadoopdn-02:2181,sht-sgmhadoopdn-03:2181* ​ ** ​ ** ​ *yarn.resourcemanager.zk.state-store.address* ​ *sht-sgmhadoopdn-01:2181,sht-sgmhadoopdn-02:2181,sht-sgmhadoopdn-03:2181* ​ ** ​ ** ​ ** ​ *yarn.resourcemanager.address.rm1* ​ *sht-sgmhadoopnn-01:23140* ​ ** ​ ** ​ *yarn.resourcemanager.address.rm2* ​ *sht-sgmhadoopnn-02:23140* ​ ** ​ ** ​ ** ​ *yarn.resourcemanager.scheduler.address.rm1* ​ *sht-sgmhadoopnn-01:23130* ​ ** ​ ** ​ *yarn.resourcemanager.scheduler.address.rm2* ​ *sht-sgmhadoopnn-02:23130* ​ ** ​ ** ​ ** ​ *yarn.resourcemanager.admin.address.rm1* ​ *sht-sgmhadoopnn-01:23141* ​ ** ​ ** ​ *yarn.resourcemanager.admin.address.rm2* ​ *sht-sgmhadoopnn-02:23141* ​ ** ​ ** ​ ** ​ *yarn.resourcemanager.resource-tracker.address.rm1* ​ *sht-sgmhadoopnn-01:23125* ​ ** ​ ** ​ *yarn.resourcemanager.resource-tracker.address.rm2* ​ *sht-sgmhadoopnn-02:23125* ​ ** ​ ** ​ ** ​ *yarn.resourcemanager.webapp.address.rm1* ​ *sht-sgmhadoopnn-01:8088* ​ ** ​ ** ​ *yarn.resourcemanager.webapp.address.rm2* ​ *sht-sgmhadoopnn-02:8088* ​ ** ​ ** ​ *yarn.resourcemanager.webapp.https.address.rm1* ​ *sht-sgmhadoopnn-01:23189* ​ ** ​ ** ​ *yarn.resourcemanager.webapp.https.address.rm2* ​ *sht-sgmhadoopnn-02:23189* ​ ** ** *8.**修改*slaves [root@sht-sgmhadoopnn-01 hadoop]# vi slaves sht-sgmhadoopdn-01 sht-sgmhadoopdn-02 sht-sgmhadoopdn-03 *9.**分发文件夹*** [root@sht-sgmhadoopnn-01 hadoop]# scp -r hadoop-2.7.2 root@sht-sgmhadoopnn-02:/hadoop [root@sht-sgmhadoopnn-01 hadoop]# scp -r hadoop-2.7.2 root@sht-sgmhadoopdn-01:/hadoop [root@sht-sgmhadoopnn-01 hadoop]# scp -r hadoop-2.7.2 root@sht-sgmhadoopdn-02:/hadoop [root@sht-sgmhadoopnn-01 hadoop]# scp -r hadoop-2.7.2 root@sht-sgmhadoopdn-03:/hadoop *八**.*启动集群 另外一种启动方式:http://www.micmiu.com/bigdata/hadoop/hadoop2-cluster-ha-setup/ *1.**启动*zookeeper 点击(此处)折叠或打开 command: ./zkServer.sh start|stop|status [root@sht-sgmhadoopdn-01 bin]# ./zkServer.sh start JMX enabled by default Using config: /hadoop/zookeeper/bin/../conf/zoo.cfg Starting zookeeper … STARTED [root@sht-sgmhadoopdn-01 bin]# jps 2073 QuorumPeerMain 2106 Jps [root@sht-sgmhadoopdn-02 bin]# ./zkServer.sh start JMX enabled by default Using config: /hadoop/zookeeper/bin/../conf/zoo.cfg Starting zookeeper … STARTED [root@sht-sgmhadoopdn-02 bin]# jps 2073 QuorumPeerMain 2106 Jps [root@sht-sgmhadoopdn-03 bin]# ./zkServer.sh start JMX enabled by default Using config: /hadoop/zookeeper/bin/../conf/zoo.cfg Starting zookeeper … STARTED [root@sht-sgmhadoopdn-03 bin]# jps 2073 QuorumPeerMain 2106 Jps *2.**启动*hadoop(HDFS+YARN) *a.**格式化前,先在**journalnode 节点机器上先启动**JournalNode*进程 点击(此处)折叠或打开 [root@sht-sgmhadoopdn-01 ~]# cd /hadoop/hadoop-2.7.2/sbin [root@sht-sgmhadoopdn-01 sbin]# hadoop-daemon.sh start journalnode starting journalnode, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-journalnode-sht-sgmhadoopdn-03.telenav.cn.out [root@sht-sgmhadoopdn-03 sbin]# jps 16722 JournalNode 16775 Jps 15519 QuorumPeerMain [root@sht-sgmhadoopdn-02 ~]# cd /hadoop/hadoop-2.7.2/sbin [root@sht-sgmhadoopdn-02 sbin]# hadoop-daemon.sh start journalnode starting journalnode, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-journalnode-sht-sgmhadoopdn-03.telenav.cn.out [root@sht-sgmhadoopdn-03 sbin]# jps 16722 JournalNode 16775 Jps 15519 QuorumPeerMain [root@sht-sgmhadoopdn-03 ~]# cd /hadoop/hadoop-2.7.2/sbin [root@sht-sgmhadoopdn-03 sbin]# hadoop-daemon.sh start journalnode starting journalnode, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-journalnode-sht-sgmhadoopdn-03.telenav.cn.out [root@sht-sgmhadoopdn-03 sbin]# jps 16722 JournalNode 16775 Jps 15519 QuorumPeerMain *b.NameNode**格式化*** 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-01 bin]# hadoop namenode -format 16/02/25 14:05:04 INFO namenode.NameNode: STARTUP_MSG: /**** STARTUP_MSG: Starting NameNode STARTUP_MSG: host = sht-sgmhadoopnn-01.telenav.cn/172.16.101.55 STARTUP_MSG: args = [-format] STARTUP_MSG: version = 2.7.2 STARTUP_MSG: classpath = …………….. ……………… 16/02/25 14:05:07 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033 16/02/25 14:05:07 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0 16/02/25 14:05:07 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension = 30000 16/02/25 14:05:07 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10 16/02/25 14:05:07 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10 16/02/25 14:05:07 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25 16/02/25 14:05:07 INFO namenode.FSNamesystem: Retry cache on namenode is enabled 16/02/25 14:05:07 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis 16/02/25 14:05:07 INFO util.GSet: Computing capacity for map NameNodeRetryCache 16/02/25 14:05:07 INFO util.GSet: VM type = 64-bit 16/02/25 14:05:07 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB 16/02/25 14:05:07 INFO util.GSet: capacity = 2^15 = 32768 entries 16/02/25 14:05:08 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1182930464-172.16.101.55-1456380308394 16/02/25 14:05:08 INFO common.Storage: Storage directory /hadoop/hadoop-2.7.2/data/dfs/name has been successfully formatted. 16/02/25 14:05:08 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0 16/02/25 14:05:08 INFO util.ExitUtil: Exiting with status 0 16/02/25 14:05:08 INFO namenode.NameNode: SHUTDOWN_MSG: /**** SHUTDOWN_MSG: Shutting down NameNode at sht-sgmhadoopnn-01.telenav.cn/172.16.101.55 ***/* *c.**同步NameNode元数据*** 点击(此处)折叠或打开 同步sht-sgmhadoopnn-01 元数据到sht-sgmhadoopnn-02 主要是：dfs.namenode.name.dir，dfs.namenode.edits.dir还应该确保共享存储目录下(dfs.namenode.shared.edits.dir ) 包含NameNode 所有的元数据。 [root@sht-sgmhadoopnn-01 hadoop-2.7.2]# pwd /hadoop/hadoop-2.7.2 [root@sht-sgmhadoopnn-01 hadoop-2.7.2]# scp -r data/ root@sht-sgmhadoopnn-02:/hadoop/hadoop-2.7.2 seen_txid 100% 2 0.0KB/s 00:00 fsimage_0000000000000000000 100% 351 0.3KB/s 00:00 fsimage_0000000000000000000.md5 100% 62 0.1KB/s 00:00 VERSION 100% 205 0.2KB/s 00:00 *d.**初始化*ZFCK 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-01 bin]# hdfs zkfc -formatZK …………….. …………….. 16/02/25 14:14:41 INFO zookeeper.ZooKeeper: Client environment:user.home=/root 16/02/25 14:14:41 INFO zookeeper.ZooKeeper: Client environment:user.dir=/hadoop/hadoop-2.7.2/bin 16/02/25 14:14:41 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=sht-sgmhadoopdn-01:2181,sht-sgmhadoopdn-02:2181,sht-sgmhadoopdn-03:2181 sessionTimeout=2000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@5f4298a5 16/02/25 14:14:41 INFO zookeeper.ClientCnxn: Opening socket connection to server sht-sgmhadoopdn-01.telenav.cn/172.16.101.58:2181. Will not attempt to authenticate using SASL (unknown error) 16/02/25 14:14:41 INFO zookeeper.ClientCnxn: Socket connection established to sht-sgmhadoopdn-01.telenav.cn/172.16.101.58:2181, initiating session 16/02/25 14:14:42 INFO zookeeper.ClientCnxn: Session establishment complete on server sht-sgmhadoopdn-01.telenav.cn/172.16.101.58:2181, sessionid = 0x15316c965750000, negotiated timeout = 4000 16/02/25 14:14:42 INFO ha.ActiveStandbyElector: Session connected. 16/02/25 14:14:42 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/mycluster in ZK. 16/02/25 14:14:42 INFO zookeeper.ClientCnxn: EventThread shut down 16/02/25 14:14:42 INFO zookeeper.ZooKeeper: Session: 0x15316c965750000 closed *e.**启动**HDFS 系统* 集群启动,在sht-sgmhadoopnn-01执行start-dfs.sh 集群关闭,在sht-sgmhadoopnn-01执行stop-dfs.sh #####集群启动############ 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-01 sbin]# start-dfs.sh 16/02/25 14:21:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable Starting namenodes on [sht-sgmhadoopnn-01 sht-sgmhadoopnn-02] sht-sgmhadoopnn-01: starting namenode, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-namenode-sht-sgmhadoopnn-01.telenav.cn.out sht-sgmhadoopnn-02: starting namenode, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-namenode-sht-sgmhadoopnn-02.telenav.cn.out sht-sgmhadoopdn-01: starting datanode, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-datanode-sht-sgmhadoopdn-01.telenav.cn.out sht-sgmhadoopdn-02: starting datanode, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-datanode-sht-sgmhadoopdn-02.telenav.cn.out sht-sgmhadoopdn-03: starting datanode, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-datanode-sht-sgmhadoopdn-03.telenav.cn.out Starting journal nodes [sht-sgmhadoopdn-01 sht-sgmhadoopdn-02 sht-sgmhadoopdn-03] sht-sgmhadoopdn-01: journalnode running as process 6348. Stop it first. sht-sgmhadoopdn-03: journalnode running as process 16722. Stop it first. sht-sgmhadoopdn-02: journalnode running as process 7197. Stop it first. 16/02/25 14:21:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable Starting ZK Failover Controllers on NN hosts [sht-sgmhadoopnn-01 sht-sgmhadoopnn-02] sht-sgmhadoopnn-01: starting zkfc, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-zkfc-sht-sgmhadoopnn-01.telenav.cn.out sht-sgmhadoopnn-02: starting zkfc, logging to /hadoop/hadoop-2.7.2/logs/hadoop-root-zkfc-sht-sgmhadoopnn-02.telenav.cn.out You have mail in /var/spool/mail/root ####单进程启动########### *NameNode(sht-sgmhadoopnn-01, sht-sgmhadoopnn-02):* hadoop-daemon.sh start namenode *DataNode(sht-sgmhadoopdn-01, sht-sgmhadoopdn-02, sht-sgmhadoopdn-03):* hadoop-daemon.sh start datanode *JournamNode(sht-sgmhadoopdn-01, sht-sgmhadoopdn-02, sht-sgmhadoopdn-03):* hadoop-daemon.sh start journalnode *ZKFC(sht-sgmhadoopnn-01, sht-sgmhadoopnn-02):* hadoop-daemon.sh start zkfc *f.**验证*namenode,datanode,zkfc *1) 进程* 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-01 sbin]# jps 12712 Jps 12593 DFSZKFailoverController 12278 NameNode [root@sht-sgmhadoopnn-02 ~]# jps 29714 NameNode 29849 DFSZKFailoverController 30229 Jps [root@sht-sgmhadoopdn-01 ~]# jps 6348 JournalNode 8775 Jps 559 QuorumPeerMain 8509 DataNode [root@sht-sgmhadoopdn-02 ~]# jps 9430 Jps 9160 DataNode 7197 JournalNode 2073 QuorumPeerMain [root@sht-sgmhadoopdn-03 ~]# jps 16722 JournalNode 17369 Jps 15519 QuorumPeerMain 17214 DataNode *2) 页面* *sht-sgmhadoopnn-01:* http://172.16.101.55:50070/ *sht-sgmhadoopnn-02:* http://172.16.101.56:50070/ *g.**启动YARN运算框架*** #####集群启动############ *1) sht-sgmhadoopnn-01**启动Yarn*，命令所在目录：$HADOOP_HOME/sbin 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-01 sbin]# start-yarn.sh starting yarn daemons starting resourcemanager, logging to /hadoop/hadoop-2.7.2/logs/yarn-root-resourcemanager-sht-sgmhadoopnn-01.telenav.cn.out sht-sgmhadoopdn-03: starting nodemanager, logging to /hadoop/hadoop-2.7.2/logs/yarn-root-nodemanager-sht-sgmhadoopdn-03.telenav.cn.out sht-sgmhadoopdn-02: starting nodemanager, logging to /hadoop/hadoop-2.7.2/logs/yarn-root-nodemanager-sht-sgmhadoopdn-02.telenav.cn.out sht-sgmhadoopdn-01: starting nodemanager, logging to /hadoop/hadoop-2.7.2/logs/yarn-root-nodemanager-sht-sgmhadoopdn-01.telenav.cn.out *2) sht-sgmhadoopnn-02**备机启动RM*** 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-02 sbin]# yarn-daemon.sh start resourcemanager starting resourcemanager, logging to /hadoop/hadoop-2.7.2/logs/yarn-root-resourcemanager-sht-sgmhadoopnn-02.telenav.cn.out ####单进程启动########### *1) ResourceManager(sht-sgmhadoopnn-01, sht-sgmhadoopnn-02)* yarn-daemon.sh start resourcemanager *2) NodeManager(sht-sgmhadoopdn-01, sht-sgmhadoopdn-02, sht-sgmhadoopdn-03)* yarn-daemon.sh start nodemanager ######关闭############# [root@sht-sgmhadoopnn-01 sbin]# stop-yarn.sh #包含namenode的resourcemanager进程，datanode的nodemanager进程 [root@sht-sgmhadoopnn-02 sbin]# yarn-daemon.sh stop resourcemanager *h.**验证*resourcemanager,nodemanager *1) 进程* 点击(此处)折叠或打开 [root@sht-sgmhadoopnn-01 sbin]# jps 13611 Jps 12593 DFSZKFailoverController 12278 NameNode 13384 ResourceManager [root@sht-sgmhadoopnn-02 sbin]# jps 32265 ResourceManager 32304 Jps 29714 NameNode 29849 DFSZKFailoverController [root@sht-sgmhadoopdn-01 ~]# jps 6348 JournalNode 559 QuorumPeerMain 8509 DataNode 10286 NodeManager 10423 Jps [root@sht-sgmhadoopdn-02 ~]# jps 9160 DataNode 10909 NodeManager 11937 Jps 7197 JournalNode 2073 QuorumPeerMain [root@sht-sgmhadoopdn-03 ~]# jps 18031 Jps 16722 JournalNode 17710 NodeManager 15519 QuorumPeerMain 17214 DataNode 2)页面 ResourceManger（Active）：http://172.16.101.55:8088 ResourceManger（Standby）：http://172.16.101.56:8088/cluster/cluster 九.监控集群 [root@sht-sgmhadoopnn-01 ~]# hdfs dfsadmin -report 十.附件及参考 #http://archive-primary.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.5.2.tar.gz #http://archive-primary.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.5.2.tar.gz hadoop :http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz zookeeper :http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz 参考: Hadoop-2.3.0-cdh5.0.1完全分布式环境搭建(NameNode,ResourceManager HA): http://blog.itpub.net/30089851/viewspace-1987620/ 如何解决这类问题：The string “–” is not permitted within comments: http://blog.csdn.net/free4294/article/details/38681095 SecureCRT连接linux终端中文显示乱码解决办法: http://www.cnblogs.com/qi09/archive/2013/02/05/2892922.html 参照:http://blog.itpub.net/30089851/viewspace-1987620/ 转自博客：http://blog.itpub.net/30089851/viewspace-1994585/]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN的Memory和CPU调优配置详解]]></title>
    <url>%2F2019%2F04%2F07%2F%5B%E8%BD%AC%E8%BD%BD%5DYARN%E7%9A%84Memory%E5%92%8CCPU%E8%B0%83%E4%BC%98%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Hadoop YARN同时支持内存和CPU两种资源的调度，本文介绍如何配置YARN对内存和CPU的使用。 YARN作为一个资源调度器，应该考虑到集群里面每一台机子的计算资源，然后根据application申请的资源进行分配Container。Container是YARN里面资源分配的基本单位，具有一定的内存以及CPU资源。 在YARN集群中，平衡内存、CPU、磁盘的资源的很重要的，根据经验，每两个container使用一块磁盘以及一个CPU核的时候可以使集群的资源得到一个比较好的利用。 内存配置关于内存相关的配置可以参考hortonwork公司的文档Determine HDP Memory Configuration Settings来配置你的集群。 YARN以及MAPREDUCE所有可用的内存资源应该要除去系统运行需要的以及其他的hadoop的一些程序，总共保留的内存=系统内存+HBASE内存。 可以参考下面的表格确定应该保留的内存： 每台机子内存 系统需要的内存 HBase需要的内存 4GB 1GB 1GB 8GB 2GB 1GB 16GB 2GB 2GB 24GB 4GB 4GB 48GB 6GB 8GB 64GB 8GB 8GB 72GB 8GB 8GB 96GB 12GB 16GB 128GB 24GB 24GB 255GB 32GB 32GB 512GB 64GB 64GB 计算每台机子最多可以拥有多少个container，可以使用下面的公式: containers = min (2CORES, 1.8DISKS, (Total available RAM) / MIN_CONTAINER_SIZE) 说明： CORES为机器CPU核数 DISKS为机器上挂载的磁盘个数 Total available RAM为机器总内存 MIN_CONTAINER_SIZE是指container最小的容量大小，这需要根据具体情况去设置，可以参考下面的表格： 每台机子可用的RAM container最小值 小于4GB 256MB 4GB到8GB之间 512MB 8GB到24GB之间 1024MB 大于24GB 2048MB 每个container的平均使用内存大小计算方式为： RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers)) 通过上面的计算，YARN以及MAPREDUCE可以这样配置： 配置文件 配置设置 默认值 计算值 yarn-site.xml yarn.nodemanager.resource.memory-mb 8192 MB = containers * RAM-per-container yarn-site.xml yarn.scheduler.minimum-allocation-mb 1024MB = RAM-per-container yarn-site.xml yarn.scheduler.maximum-allocation-mb 8192 MB = containers * RAM-per-container yarn-site.xml (check) yarn.app.mapreduce.am.resource.mb 1536 MB = 2 * RAM-per-container yarn-site.xml (check) yarn.app.mapreduce.am.command-opts -Xmx1024m = 0.8 2 RAM-per-container mapred-site.xml mapreduce.map.memory.mb 1024 MB = RAM-per-container mapred-site.xml mapreduce.reduce.memory.mb 1024 MB = 2 * RAM-per-container mapred-site.xml mapreduce.map.java.opts = 0.8 * RAM-per-container mapred-site.xml mapreduce.reduce.java.opts = 0.8 2 RAM-per-container 举个例子：对于128G内存、32核CPU的机器，挂载了7个磁盘，根据上面的说明，系统保留内存为24G，不适应HBase情况下，系统剩余可用内存为104G，计算containers值如下： containers = min (232, 1.8 7 , (128-24)/2) = min (64, 12.6 , 51) = 13 计算RAM-per-container值如下： RAM-per-container = max (2, (124-24)/13) = max (2, 8) = 8 你也可以使用脚本yarn-utils.py来计算上面的值： 点击(此处)折叠或打开 #!/usr/bin/env python import optparse from pprint import pprint import logging import sys import math import ast ‘’’ Reserved for OS + DN + NM, Map: Memory =&gt; Reservation ‘’’ reservedStack = { 4:1, 8:2, 16:2, 24:4, 48:6, 64:8, 72:8, 96:12, ​ 128:24, 256:32, 512:64} ‘’’ Reserved for HBase. Map: Memory =&gt; Reservation ‘’’ reservedHBase = {4:1, 8:1, 16:2, 24:4, 48:8, 64:8, 72:8, 96:16, ​ 128:24, 256:32, 512:64} GB = 1024 def getMinContainerSize(memory): if (memory &lt;= 4): ​ return 256 elif (memory &lt;= 8): ​ return 512 elif (memory &lt;= 24): ​ return 1024 else: ​ return 2048 pass def getReservedStackMemory(memory): if (reservedStack.has_key(memory)): ​ return reservedStack[memory] if (memory &lt;= 4): ​ ret = 1 elif (memory &gt;= 512): ​ ret = 64 else: ​ ret = 1 return ret def getReservedHBaseMem(memory): if (reservedHBase.has_key(memory)): ​ return reservedHBase[memory] if (memory &lt;= 4): ​ ret = 1 elif (memory &gt;= 512): ​ ret = 64 else: ​ ret = 2 return ret ​ def main(): log = logging.getLogger(name) out_hdlr = logging.StreamHandler(sys.stdout) out_hdlr.setFormatter(logging.Formatter(‘ %(message)s’)) out_hdlr.setLevel(logging.INFO) log.addHandler(out_hdlr) log.setLevel(logging.INFO) parser = optparse.OptionParser() memory = 0 cores = 0 disks = 0 hbaseEnabled = True parser.add_option(‘-c’, ‘–cores’, default = 16, ​ help = ‘Number of cores on each host’) parser.add_option(‘-m’, ‘–memory’, default = 64, ​ help = ‘Amount of Memory on each host in GB’) parser.add_option(‘-d’, ‘–disks’, default = 4, ​ help = ‘Number of disks on each host’) parser.add_option(‘-k’, ‘–hbase’, default = “True”, ​ help = ‘True if HBase is installed, False is not’) (options, args) = parser.parse_args() cores = int (options.cores) memory = int (options.memory) disks = int (options.disks) hbaseEnabled = ast.literal_eval(options.hbase) log.info(“Using cores=” + str(cores) + “ memory=” + str(memory) + “GB” + ​ “ disks=” + str(disks) + “ hbase=” + str(hbaseEnabled)) minContainerSize = getMinContainerSize(memory) reservedStackMemory = getReservedStackMemory(memory) reservedHBaseMemory = 0 if (hbaseEnabled): ​ reservedHBaseMemory = getReservedHBaseMem(memory) reservedMem = reservedStackMemory + reservedHBaseMemory usableMem = memory - reservedMem memory -= (reservedMem) if (memory &lt; 2): ​ memory = 2 ​ reservedMem = max(0, memory - reservedMem) ​ memory *= GB containers = int (min(2 * cores, ​ min(math.ceil(1.8 * float(disks)), ​ memory/minContainerSize))) if (containers &lt;= 2): ​ containers = 3 log.info(“Profile: cores=” + str(cores) + “ memory=” + str(memory) + “MB” ​ + “ reserved=” + str(reservedMem) + “GB” + “ usableMem=” ​ + str(usableMem) + “GB” + “ disks=” + str(disks)) ​ container_ram = abs(memory/containers) if (container_ram &gt; GB): ​ container_ram = int(math.floor(container_ram / 512)) * 512 log.info(“Num Container=” + str(containers)) log.info(“Container Ram=” + str(container_ram) + “MB”) log.info(“Used Ram=” + str(int (containers*container_ram/float(GB))) + “GB”) log.info(“Unused Ram=” + str(reservedMem) + “GB”) log.info(“yarn.scheduler.minimum-allocation-mb=” + str(container_ram)) log.info(“yarn.scheduler.maximum-allocation-mb=” + str(containers*container_ram)) log.info(“yarn.nodemanager.resource.memory-mb=” + str(containers*container_ram)) map_memory = container_ram reduce_memory = 2*container_ram if (container_ram &lt;= 2048) else container_ram am_memory = max(map_memory, reduce_memory) log.info(“mapreduce.map.memory.mb=” + str(map_memory)) log.info(“mapreduce.map.java.opts=-Xmx” + str(int(0.8 * map_memory)) +”m”) log.info(“mapreduce.reduce.memory.mb=” + str(reduce_memory)) log.info(“mapreduce.reduce.java.opts=-Xmx” + str(int(0.8 * reduce_memory)) + “m”) log.info(“yarn.app.mapreduce.am.resource.mb=” + str(am_memory)) log.info(“yarn.app.mapreduce.am.command-opts=-Xmx” + str(int(0.8*am_memory)) + “m”) log.info(“mapreduce.task.io.sort.mb=” + str(int(0.4 * map_memory))) pass if name == ‘main‘: try: ​ main() except(KeyboardInterrupt, EOFError): ​ print(“\nAborting … Keyboard Interrupt.”) ​ sys.exit(1) 执行下面命令： 1python yarn-utils.py -c 32 -m 128 -d 7 -k False 返回结果如下： 点击(此处)折叠或打开 Using cores=32 memory=128GB disks=7 hbase=False Profile: cores=32 memory=106496MB reserved=24GB usableMem=104GB disks=7 Num Container=13 Container Ram=8192MB Used Ram=104GB Unused Ram=24GB yarn.scheduler.minimum-allocation-mb=8192 yarn.scheduler.maximum-allocation-mb=106496 yarn.nodemanager.resource.memory-mb=106496 mapreduce.map.memory.mb=8192 mapreduce.map.java.opts=-Xmx6553m mapreduce.reduce.memory.mb=8192 mapreduce.reduce.java.opts=-Xmx6553m yarn.app.mapreduce.am.resource.mb=8192 yarn.app.mapreduce.am.command-opts=-Xmx6553m mapreduce.task.io.sort.mb=3276 这样的话，每个container内存为8G，似乎有点多，我更愿意根据集群使用情况任务将其调整为2G内存，则集群中下面的参数配置值如下： 配置文件 配置设置 计算值 yarn-site.xml yarn.nodemanager.resource.memory-mb = 52 * 2 =104 G yarn-site.xml yarn.scheduler.minimum-allocation-mb = 2G yarn-site.xml yarn.scheduler.maximum-allocation-mb = 52 * 2 = 104G yarn-site.xml (check) yarn.app.mapreduce.am.resource.mb = 2 * 2=4G yarn-site.xml (check) yarn.app.mapreduce.am.command-opts = 0.8 2 2=3.2G mapred-site.xml mapreduce.map.memory.mb = 2G mapred-site.xml mapreduce.reduce.memory.mb = 2 * 2=4G mapred-site.xml mapreduce.map.java.opts = 0.8 * 2=1.6G mapred-site.xml mapreduce.reduce.java.opts = 0.8 2 2=3.2G 对应的xml配置为： 点击(此处)折叠或打开 ​ yarn.nodemanager.resource.memory-mb ​ 106496 ​ yarn.scheduler.minimum-allocation-mb ​ 2048 ​ yarn.scheduler.maximum-allocation-mb ​ 106496 ​ yarn.app.mapreduce.am.resource.mb ​ 4096 ​ yarn.app.mapreduce.am.command-opts ​ -Xmx3276m 另外，还有一下几个参数： yarn.nodemanager.vmem-pmem-ratio：任务每使用1MB物理内存，最多可使用虚拟内存量，默认是2.1。 yarn.nodemanager.pmem-check-enabled：是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 yarn.nodemanager.vmem-pmem-ratio：是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 第一个参数的意思是当一个map任务总共分配的物理内存为2G的时候，该任务的container最多内分配的堆内存为1.6G，可以分配的虚拟内存上限为2*2.1=4.2G。另外，照这样算下去，每个节点上YARN可以启动的Map数为104/2=52个。 CPU配置YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟CPU个数。 在YARN中，CPU相关配置参数如下： yarn.nodemanager.resource.cpu-vcores：表示该节点上YARN可使用的虚拟CPU个数，默认是8，注意，目前推荐将该值设值为与物理CPU核数数目相同。如果你的节点CPU核数不够8个，则需要调减小这个值，而YARN不会智能的探测节点的物理CPU总数。 yarn.scheduler.minimum-allocation-vcores：单个任务可申请的最小虚拟CPU个数，默认是1，如果一个任务申请的CPU个数少于该数，则该对应的值改为这个数。 yarn.scheduler.maximum-allocation-vcores：单个任务可申请的最多虚拟CPU个数，默认是32。 对于一个CPU核数较多的集群来说，上面的默认配置显然是不合适的，在我的测试集群中，4个节点每个机器CPU核数为31，留一个给操作系统，可以配置为： 点击(此处)折叠或打开 ​ yarn.nodemanager.resource.cpu-vcores ​ 31 ​ yarn.scheduler.maximum-allocation-vcores ​ 124 转自博客:http://blog.javachen.com/2015/06/05/yarn-memory-and-cpu-configuration.html?utm_source=tuicool&amp;utm_medium=referral]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>Memory和Cpu调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop生态系统官网、下载地址、文档]]></title>
    <url>%2F2019%2F04%2F07%2FHadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E5%AE%98%E7%BD%91%E3%80%81%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80%E3%80%81%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[Apache版本： Hadoop官网：http://hadoop.apache.org/Hadoop下载：http://mirror.bit.edu.cn/apache/hadoop/common/ Hadoop历史版本下载：http://archive.apache.org/dist/hadoop/core/Hadoop文档：http://hadoop.apache.org/docs/ Hive官网：http://hive.apache.org/Hive下载：http://mirror.bit.edu.cn/apache/hive/ Hive历史版本下载：http://archive.apache.org/dist/hive/Hive文档：https://cwiki.apache.org/confluence/display/Hive HBase官网：http://hbase.apache.org/HBase下载：http://mirrors.sonic.net/apache/hbase/ HBase历史版本下载：http://archive.apache.org/dist/hbase/HBase文档：http://hbase.apache.org/book.htmlHBase中文文档：http://abloz.com/hbase/book.html Spark官网：http://spark.apache.org/Spark下载：http://spark.apache.org/downloads.htmlSpark文档：http://spark.apache.org/docs/latest/ Zookeeper官网：http://zookeeper.apache.org/Zookeeper下载：http://zookeeper.apache.org/releases.html#download Flume官网：http://flume.apache.org/Flume下载：http://flume.apache.org/download.htmlFlume文档：http://flume.apache.org/documentation.html Mahout官网：http://mahout.apache.org/Mahout下载：http://mahout.apache.org/general/downloads.html Tez官网：http://tez.apache.org/ cdh5版本：下载地址：http://archive.cloudera.com/cdh5/cdh/5/ 文档地址：http://archive.cloudera.com/cdh5/cdh/5/]]></content>
      <tags>
        <tag>大数据开发</tag>
        <tag>顶级项目官网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS命令操作]]></title>
    <url>%2F2019%2F04%2F07%2FHDFS%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[命令基本格式: 1hadoop fs -cmd &lt; args &gt; 1.ls 1hadoop fs -ls / 列出hdfs文件系统根目录下的目录和文件 1hadoop fs -ls -R / 列出hdfs文件系统所有的目录和文件 2.put 1hadoop fs -put &lt; local file &gt; &lt; hdfs file &gt; hdfs file的父目录一定要存在，否则命令不会执行 1hadoop fs -put &lt; local file or dir &gt;...&lt; hdfs dir &gt; hdfs dir 一定要存在，否则命令不会执行 1hadoop fs -put - &lt; hdsf file&gt; 从键盘读取输入到hdfs file中，按Ctrl+D结束输入，hdfs file不能存在，否则命令不会执行 2.1.moveFromLocal 1hadoop fs -moveFromLocal &lt; local src &gt; ... &lt; hdfs dst &gt; 与put相类似，命令执行后源文件 local src 被删除，也可以从从键盘读取输入到hdfs file中 2.2.copyFromLocal 1hadoop fs -copyFromLocal &lt; local src &gt; ... &lt; hdfs dst &gt; 与put相类似，也可以从从键盘读取输入到hdfs file中 3.get 1hadoop fs -get &lt; hdfs file &gt; &lt; local file or dir&gt; local file不能和 hdfs file名字不能相同，否则会提示文件已存在，没有重名的文件会复制到本地 1hadoop fs -get &lt; hdfs file or dir &gt; ... &lt; local dir &gt; 拷贝多个文件或目录到本地时，本地要为文件夹路径注意：如果用户不是root， local 路径要为用户文件夹下的路径，否则会出现权限问题， 3.1.moveToLocal 3.2.copyToLocal 1hadoop fs -copyToLocal &lt; local src &gt; ... &lt; hdfs dst &gt; 与get相类似 4.rm 1234hadoop fs -rm &lt; hdfs file &gt; ...hadoop fs -rm -r &lt; hdfs dir&gt;...每次可以删除多个文件或目录 5.mkdir 1hadoop fs -mkdir &lt; hdfs path&gt; 只能一级一级的建目录，父目录不存在的话使用这个命令会报错 1hadoop fs -mkdir -p &lt; hdfs path&gt; 所创建的目录如果父目录不存在就创建该父目录 6.getmerge 1hadoop fs -getmerge &lt; hdfs dir &gt; &lt; local file &gt; 将hdfs指定目录下所有文件排序后合并到local指定的文件中，文件不存在时会自动创建，文件存在时会覆盖里面的内容 1hadoop fs -getmerge -nl &lt; hdfs dir &gt; &lt; local file &gt; 加上nl后，合并到local file中的hdfs文件之间会空出一行 7.cp 1hadoop fs -cp &lt; hdfs file &gt; &lt; hdfs file &gt; 目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件还存在 1hadoop fs -cp &lt; hdfs file or dir &gt;... &lt; hdfs dir &gt; 目标文件夹要存在，否则命令不能执行 8.mv 1hadoop fs -mv &lt; hdfs file &gt; &lt; hdfs file &gt; 目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件不存在 1hadoop fs -mv &lt; hdfs file or dir &gt;... &lt; hdfs dir &gt; 源路径有多个时，目标路径必须为目录，且必须存在。注意：跨文件系统的移动（local到hdfs或者反过来）都是不允许的 9.count 1hadoop fs -count &lt; hdfs path &gt; 统计hdfs对应路径下的目录个数，文件个数，文件总计大小 显示为目录个数，文件个数，文件总计大小，输入路径 10.du 1hadoop fs -du &lt; hdsf path&gt; 显示hdfs对应路径下每个文件夹和文件的大小 1hadoop fs -du -s &lt; hdsf path&gt; 显示hdfs对应路径下所有文件和的大小 1hadoop fs -du - h &lt; hdsf path&gt; 显示hdfs对应路径下每个文件夹和文件的大小,文件的大小用方便阅读的形式表示，例如用64M代替67108864 11.text 1hadoop fs -text &lt; hdsf file&gt; 将文本文件或某些格式的非文本文件通过文本格式输出 12.setrep 1hadoop fs -setrep -R 3 &lt; hdfs path &gt; 改变一个文件在hdfs中的副本个数，上述命令中数字3为所设置的副本个数，-R选项可以对一个人目录下的所有目录+文件递归执行改变副本个数的操作 13.stat 1hdoop fs -stat [format] &lt; hdfs path &gt; 返回对应路径的状态信息[format]可选参数有：%b（文件大小），%o（Block大小），%n（文件名），%r（副本个数），%y（最后一次修改日期和时间）可以这样书写hadoop fs -stat %b%o%n &lt; hdfs path &gt;，不过不建议，这样每个字符输出的结果不是太容易分清楚 14.tail 1hadoop fs -tail &lt; hdfs file &gt; 在标准输出中显示文件末尾的1KB数据 15.archive 1hadoop archive -archiveName name.har -p &lt; hdfs parent dir &gt; &lt; src &gt;* &lt; hdfs dst &gt; 命令中参数name：压缩文件名，自己任意取；&lt; hdfs parent dir &gt; ：压缩文件所在的父目录；&lt; src &gt;：要压缩的文件名；&lt; hdfs dst &gt;：压缩文件存放路径*示例： 1hadoop archive -archiveName hadoop.har -p /user 1.txt 2.txt /des 示例中将hdfs中/user目录下的文件1.txt，2.txt压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下，如果1.txt，2.txt不写就是将/user目录下所有的目录和文件压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下显示har的内容可以用如下命令： 1hadoop fs -ls /des/hadoop.jar 显示har压缩的是那些文件可以用如下命令 1hadoop fs -ls -R har:///des/hadoop.har 注意：har文件不能进行二次压缩。如果想给.har加文件，只能找到原来的文件，重新创建一个。har文件中原来文件的数据并没有变化，har文件真正的作用是减少NameNode和DataNode过多的空间浪费。 16.balancer 1hdfs balancer 如果管理员发现某些DataNode保存数据过多，某些DataNode保存数据相对较少，可以使用上述命令手动启动内部的均衡过程 17.dfsadmin 1hdfs dfsadmin -help 管理员可以通过dfsadmin管理HDFS，用法可以通过上述命令查看 1hdfs dfsadmin -report 显示文件系统的基本数据 1hdfs dfsadmin -safemode &lt; enter | leave | get | wait &gt; enter：进入安全模式；leave：离开安全模式；get：获知是否开启安全模式； wait：等待离开安全模式]]></content>
      <tags>
        <tag>HDFS</tag>
        <tag>大数据开发</tag>
        <tag>命令操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop部署MapReduce+YARN]]></title>
    <url>%2F2019%2F04%2F07%2F3.Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%88MapReduce%2BYARN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1.yarn部署： 123456789101112131415161718192021222324 MapReduce: 计算的 是jar包提交的Yarn上 本身不需要部署 Yarn: 资源和作业调度 是需要部署的 因为：MapReduce on Yarn（mapreduce是运行在yarn上） &gt; Configure parameters as follows:（配置信息步骤）&gt; etc/hadoop/mapred-site.xml:&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;etc/hadoop/yarn-site.xml:&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2.sbin/start-yarn.sh 启动yarn：1234567891011121314[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ jps4001 NodeManager3254 SecondaryNameNode3910 ResourceManager3563 NameNode4317 Jps3087 DataNodeResourceManager daemon （老大 资源管理者）NodeManager daemon （小弟 节点管理者）$ sbin/start-yarn.shBrowse the web interface for the ResourceManager; by default it is available at:ResourceManager - http://localhost:8088/这时我们可以去看一下yarn的web界面：http://localhost:8088/ 3.怎样查看错误：12345678910 在生产中我们要会查看日志，在日志中查找错误 logs/目录下： hadoop-hadoop-datanode-hadoop002.log 对应的名字分别是： hadoop-用户-进程名称-机器名称 一共有三种方法可以去查看：01 vi :/搜索 ERROR02 tail -200f xxx.log（倒着查看log中的后200行日志） 另外窗口重启进程 为了再现这个错误03 rz上传到windows editplus去定位查看 备份 （一般对于生产中日志比较大的文件） 4.运行mr123456789101112131415161718192021222324252627282930 map （映射） reduce （规约） &gt; 词频统计[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ vi a.log（先编辑一个文件保存）ruozejepsonwww.ruozedata.comdashuadaifanren1abca b c ruoze jepon[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ vi b.txt（再编辑一个文件保存）a b d e f ruoze1 1 3 5[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -mkdir -p /wordcount/input （在hdfs家目录下创建一个级联文件及）[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -put a.log /wordcount/input[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -put b.txt /wordcount/input（将a.log和b.text文件上传到文件夹下）[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -ls /wordcount/input/（查看一下文件夹下的内容）Found 2 items-rw-r--r-- 1 hadoop supergroup 76 2019-02-16 21:59 /wordcount/input/a.log-rw-r--r-- 1 hadoop supergroup 24 2019-02-16 21:59 /wordcount/input/b.txt 这里我们做一个列子，mapreduce中会给出 examples，​ 我们可以通过：find ./ -name ‘example.jar’找到这个jar包：​ ./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar​ 虽然我们不知道命令时什么，但是我们可以查看命令帮助：​ hadoop回车，向下翻看，会发现有一个命令是 jar ​ 然后我们继续输入：继续输入命令：（注意：output1事先一定是不存在的）​ 查看计算结果：​ 12345678910111213141516171819202122232425262728293031323334353637383940[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hadoop jar \./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar \wordcount /wordcount/input /wordcount/output1[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -cat /wordcount/output1/part-r-0000019/02/16 22:05:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable1 33 15 1a 3adai 1b 3c 2d 1dashu 1e 1f 1fanren 1jepon 1jepson 1ruoze 3www.ruozedata.com 1[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -get /wordcount/output1/part-r-00000 ./[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ cat part-r-000001 33 15 1a 3adai 1b 3c 2d 1dashu 1e 1f 1fanren 1jepon 1jepson 1ruoze 3www.ruozedata.com 1]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop参数配置信息]]></title>
    <url>%2F2019%2F04%2F07%2FHadoop%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言： Hadoop三个参数文件，我不是很理解，我网上找了一篇学习下 ​ 配置hadoop，主要是配置core-site.xml,hdfs-site.xml,mapred-site.xml三个配置文件，默认下来，这些配置文件都是空的，所以很难知道这些配置文件有哪些配置可以生效，上网找的配置可能因为各个hadoop版本不同，导致无法生效。浏览更多的配置，有两个方法: 1.选择相应版本的hadoop,下载解压后，搜索*.xml,找到core-default.xml,hdfs-default.xml,mapred-default.xml,这些就是默认配置,可以参考这些配置的说明和key，配置hadoop集群。 core-site.xml是全局配置, hdfs-site.xml和mapred-site.xml分别是hdfs和mapred的局部配置。 2 常用的端口配置2.1 HDFS端口 参数 描述 默认 配置文件 例子值 fs.default.name namenode namenode RPC交互端口 8020 core-site.xml hdfs://master:8020/ dfs.http.address NameNode web管理端口 50070 hdfs- site.xml 0.0.0.0:50070 dfs.datanode.address datanode 控制端口 50010 hdfs -site.xml 0.0.0.0:50010 dfs.datanode.ipc.address datanode的RPC服务器地址和端口 50020 hdfs-site.xml 0.0.0.0:50020 dfs.datanode.http.address datanode的HTTP服务器和端口 50075 hdfs-site.xml 0.0.0.0:50075 2.2 MR端口 参数 描述 默认 配置文件 例子值 mapred.job.tracker job-tracker交互端口 8021 mapred-site.xml hdfs://master:8021/ job tracker的web管理端口 50030 mapred-site.xml 0.0.0.0:50030 mapred.task.tracker.http.address task-tracker的HTTP端口 50060 mapred-site.xml 0.0.0.0:50060 2.3 其它端口 参数 描述 默认 配置文件 例子值 dfs.secondary.http.address secondary NameNode web管理端口 50090 hdfs-site.xml 0.0.0.0:50090 3 三个缺省配置参考文件说明3.1 core-default.html 序号 参数名 参数值 参数说明 1 hadoop.tmp.dir /tmp/hadoop-${user.name} 临时目录设定 2 hadoop.native.lib true 使用本地hadoop库标识。 3 hadoop.http.filter.initializers http服务器过滤链设置 4 hadoop.security.group.mapping org.apache.hadoop.security.ShellBasedUnixGroupsMapping 组内用户的列表的类设定 5 hadoop.security.authorization false 服务端认证开启 6 hadoop.security.authentication simple 无认证或认证设置 7 hadoop.security.token.service.use_ip true 是否开启使用IP地址作为连接的开关 8 hadoop.logfile.size 10000000 日志文件最大为10M 9 hadoop.logfile.count 10 日志文件数量为10个 10 io.file.buffer.size 4096 流文件的缓冲区为4K 11 io.bytes.per.checksum 512 校验位数为512字节 12 io.skip.checksum.errors false 校验出错后是抛出异常还是略过标识。True则略过。 13 io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec 压缩和解压的方式设置 14 io.serializations org.apache.hadoop.io.serializer.WritableSerialization 序例化和反序列化的类设定 15 fs.default.name file:/// 缺省的文件URI标识设定。 16 fs.trash.interval 0 文件废弃标识设定，0为禁止此功能 17 fs.file.impl org.apache.hadoop.fs.LocalFileSystem 本地文件操作类设置 18 fs.hdfs.impl org.apache.hadoop.hdfs.DistributedFileSystem HDFS文件操作类设置 19 fs.s3.impl org.apache.hadoop.fs.s3.S3FileSystem S3文件操作类设置 20 fs.s3n.impl org.apache.hadoop.fs.s3native.NativeS3FileSystem S3文件本地操作类设置 21 fs.kfs.impl org.apache.hadoop.fs.kfs.KosmosFileSystem KFS文件操作类设置. 22 fs.hftp.impl org.apache.hadoop.hdfs.HftpFileSystem HTTP方式操作文件设置 23 fs.hsftp.impl org.apache.hadoop.hdfs.HsftpFileSystem HTTPS方式操作文件设置 24 fs.webhdfs.impl org.apache.hadoop.hdfs.web.WebHdfsFileSystem WEB方式操作文件类设置 25 fs.ftp.impl org.apache.hadoop.fs.ftp.FTPFileSystem FTP文件操作类设置 26 fs.ramfs.impl org.apache.hadoop.fs.InMemoryFileSystem 内存文件操作类设置 27 fs.har.impl org.apache.hadoop.fs.HarFileSystem 压缩文件操作类设置. 28 fs.har.impl.disable.cache true 是否缓存har文件的标识设定 29 fs.checkpoint.dir ${hadoop.tmp.dir}/dfs/namesecondary 备份名称节点的存放目前录设置 30 fs.checkpoint.edits.dir ${fs.checkpoint.dir} 备份名称节点日志文件的存放目前录设置 31 fs.checkpoint.period 3600 动态检查的间隔时间设置 32 fs.checkpoint.size 67108864 日志文件大小为64M 33 fs.s3.block.size 67108864 写S3文件系统的块的大小为64M 34 fs.s3.buffer.dir ${hadoop.tmp.dir}/s3 S3文件数据的本地存放目录 35 fs.s3.maxRetries 4 S3文件数据的偿试读写次数 36 fs.s3.sleepTimeSeconds 10 S3文件偿试的间隔 37 local.cache.size 10737418240 缓存大小设置为10GB 38 io.seqfile.compress.blocksize 1000000 压缩流式文件中的最小块数为100万 39 io.seqfile.lazydecompress true 块是否需要压缩标识设定 40 io.seqfile.sorter.recordlimit 1000000 内存中排序记录块类最小为100万 41 io.mapfile.bloom.size 1048576 BloomMapFiler过滤量为1M 42 io.mapfile.bloom.error.rate 0.005 43 hadoop.util.hash.type murmur 缺少hash方法为murmur 44 ipc.client.idlethreshold 4000 连接数据最小阀值为4000 45 ipc.client.kill.max 10 一个客户端连接数最大值为10 46 ipc.client.connection.maxidletime 10000 断开与服务器连接的时间最大为10秒 47 ipc.client.connect.max.retries 10 建立与服务器连接的重试次数为10次 48 ipc.server.listen.queue.size 128 接收客户连接的监听队例的长度为128 49 ipc.server.tcpnodelay false 开启或关闭服务器端TCP连接算法 50 ipc.client.tcpnodelay false 开启或关闭客户端TCP连接算法 51 webinterface.private.actions false Web交互的行为设定 52 hadoop.rpc.socket.factory.class.default org.apache.hadoop.net.StandardSocketFactory 缺省的socket工厂类设置 53 hadoop.rpc.socket.factory.class.ClientProtocol 与dfs连接时的缺省socket工厂类 54 hadoop.socks.server 服务端的工厂类缺省设置为SocksSocketFactory. 55 topology.node.switch.mapping.impl org.apache.hadoop.net.ScriptBasedMapping 56 topology.script.file.name 57 topology.script.number.args 100 参数数量最多为100 58 hadoop.security.uid.cache.secs 14400 3.2 hdfs-default.html 序号 参数名 参数值 参数说明 1 dfs.namenode.logging.level info 输出日志类型 2 dfs.secondary.http.address 0.0.0.0:50090 备份名称节点的http协议访问地址与端口 3 dfs.datanode.address 0.0.0.0:50010 数据节点的TCP管理服务地址和端口 4 dfs.datanode.http.address 0.0.0.0:50075 数据节点的HTTP协议访问地址和端口 5 dfs.datanode.ipc.address 0.0.0.0:50020 数据节点的IPC服务访问地址和端口 6 dfs.datanode.handler.count 3 数据节点的服务连接处理线程数 7 dfs.http.address 0.0.0.0:50070 名称节点的http协议访问地址与端口 8 dfs.https.enable false 支持https访问方式标识 9 dfs.https.need.client.auth false 客户端指定https访问标识 10 dfs.https.server.keystore.resource ssl-server.xml Ssl密钥服务端的配置文件 11 dfs.https.client.keystore.resource ssl-client.xml Ssl密钥客户端的配置文件 12 dfs.datanode.https.address 0.0.0.0:50475 数据节点的HTTPS协议访问地址和端口 13 dfs.https.address 0.0.0.0:50470 名称节点的HTTPS协议访问地址和端口 14 dfs.datanode.dns.interface default 数据节点采用IP地址标识 15 dfs.datanode.dns.nameserver default 指定DNS的IP地址 16 dfs.replication.considerLoad true 加载目标或不加载的标识 17 dfs.default.chunk.view.size 32768 浏览时的文件块大小设置为32K 18 dfs.datanode.du.reserved 0 每个卷预留的空闲空间数量 19 dfs.name.dir ${hadoop.tmp.dir}/dfs/name 存贮在本地的名字节点数据镜象的目录,作为名字节点的冗余备份 20 dfs.name.edits.dir ${dfs.name.dir} 存贮文件操作过程信息的存贮目录 21 dfs.web.ugi webuser,webgroup Web接口访问的用户名和组的帐户设定 22 dfs.permissions true 文件操作时的权限检查标识。 23 dfs.permissions.supergroup supergroup 超级用户的组名定义 24 dfs.block.access.token.enable false 数据节点访问令牌标识 25 dfs.block.access.key.update.interval 600 升级访问钥时的间隔时间 26 dfs.block.access.token.lifetime 600 访问令牌的有效时间 27 dfs.data.dir ${hadoop.tmp.dir}/dfs/data 数据节点的块本地存放目录 28 dfs.datanode.data.dir.perm 755 数据节点的存贮块的目录访问权限设置 29 dfs.replication 3 缺省的块复制数量 30 dfs.replication.max 512 块复制的最大数量 31 dfs.replication.min 1 块复制的最小数量 32 dfs.block.size 67108864 缺省的文件块大小为64M 33 dfs.df.interval 60000 磁盘空间统计间隔为6秒 34 dfs.client.block.write.retries 3 块写入出错时的重试次数 35 dfs.blockreport.intervalMsec 3600000 块的报告间隔时为1小时 36 dfs.blockreport.initialDelay 0 块顺序报告的间隔时间 37 dfs.heartbeat.interval 3 数据节点的心跳检测间隔时间 38 dfs.namenode.handler.count 10 名称节点的连接处理的线程数量 39 dfs.safemode.threshold.pct 0.999f 启动安全模式的阀值设定 40 dfs.safemode.extension 30000 当阀值达到量值后扩展的时限 41 dfs.balance.bandwidthPerSec 1048576 启动负载均衡的数据节点可利用带宽最大值为1M 42 dfs.hosts 可与名称节点连接的主机地址文件指定。 43 dfs.hosts.exclude 不充计与名称节点连接的主机地址文件设定 44 dfs.max.objects 0 文件数、目录数、块数的最大数量 45 dfs.namenode.decommission.interval 30 名称节点解除命令执行时的监测时间周期 46 dfs.namenode.decommission.nodes.per.interval 5 名称节点解除命令执行是否完检测次数 47 dfs.replication.interval 3 名称节点计算数据节点的复制工作的周期数. 48 dfs.access.time.precision 3600000 充许访问文件的时间精确到1小时 49 dfs.support.append false 是否充许链接文件指定 50 dfs.namenode.delegation.key.update-interval 86400000 名称节点上的代理令牌的主key的更新间隔时间为24小时 51 dfs.namenode.delegation.token.max-lifetime 604800000 代理令牌的有效时间最大值为7天 52 dfs.namenode.delegation.token.renew-interval 86400000 代理令牌的更新时间为24小时 53 dfs.datanode.failed.volumes.tolerated 0 决定停止数据节点提供服务充许卷的出错次数。0次则任何卷出错都要停止数据节点 3.3 mapred-default.html 序号 参数名 参数值 参数说明 1 hadoop.job.history.location 作业跟踪管理器的静态历史文件的存放目录。 2 hadoop.job.history.user.location 可以指定具体某个作业的跟踪管理器的历史文件存放目录 3 mapred.job.tracker.history.completed.location 已完成作业的历史文件的存放目录 4 io.sort.factor 10 排完序的文件的合并时的打开文件句柄数 5 io.sort.mb 100 排序文件的内存缓存大小为100M 6 io.sort.record.percent 0.05 排序线程阻塞的内存缓存剩余比率 7 io.sort.spill.percent 0.80 当缓冲占用量为该值时，线程需要将内容先备份到磁盘中。 8 io.map.index.skip 0 索引条目的间隔设定 9 mapred.job.tracker local 作业跟踪管理器是否和MR任务在一个进程中 10 mapred.job.tracker.http.address 0.0.0.0:50030 作业跟踪管理器的HTTP服务器访问端口和地址 11 mapred.job.tracker.handler.count 10 作业跟踪管理器的管理线程数,线程数比例是任务管理跟踪器数量的0.04 12 mapred.task.tracker.report.address 127.0.0.1:0 任务管理跟踪器的主机地址和端口地址 13 mapred.local.dir ${hadoop.tmp.dir}/mapred/local MR的中介数据文件存放目录 14 mapred.system.dir ${hadoop.tmp.dir}/mapred/system MR的控制文件存放目录 15 mapreduce.jobtracker.staging.root.dir ${hadoop.tmp.dir}/mapred/staging 每个正在运行作业文件的存放区 16 mapred.temp.dir ${hadoop.tmp.dir}/mapred/temp MR临时共享文件存放区 17 mapred.local.dir.minspacestart 0 MR本地中介文件删除时，不充许有任务执行的数量值。 18 mapred.local.dir.minspacekill 0 MR本地中介文件删除时，除非所有任务都已完成的数量值。 19 mapred.tasktracker.expiry.interval 600000 任务管理跟踪器不发送心跳的累计时间间隔超过600秒，则任务管理跟踪器失效 20 mapred.tasktracker.resourcecalculatorplugin 指定的一个用户访问资源信息的类实例 21 mapred.tasktracker.taskmemorymanager.monitoring-interval 5000 监控任务管理跟踪器任务内存使用率的时间间隔 22 mapred.tasktracker.tasks.sleeptime-before-sigkill 5000 发出进程终止后，间隔5秒后发出进程消亡信号 23 mapred.map.tasks 2 每个作业缺省的map任务数为2 24 mapred.reduce.tasks 1 每个作业缺省的reduce任务数为1 25 mapreduce.tasktracker.outofband.heartbeat false 让在任务结束后发出一个额外的心跳信号 26 mapreduce.tasktracker.outofband.heartbeat.damper 1000000 当额外心跳信号发出量太多时，则适当阻止 27 mapred.jobtracker.restart.recover false 充许任务管理器恢复时采用的方式 28 mapred.jobtracker.job.history.block.size 3145728 作业历史文件块的大小为3M 29 mapreduce.job.split.metainfo.maxsize 10000000 分隔元信息文件的最大值是10M以下 30 mapred.jobtracker.taskScheduler org.apache.hadoop.mapred.JobQueueTaskScheduler 设定任务的执行计划实现类 31 mapred.jobtracker.taskScheduler.maxRunningTasksPerJob 作业同时运行的任务数的最大值 32 mapred.map.max.attempts 4 Map任务的重试次数 33 mapred.reduce.max.attempts 4 Reduce任务的重试次数 34 mapred.reduce.parallel.copies 5 在复制阶段时reduce并行传送的值。 35 mapreduce.reduce.shuffle.maxfetchfailures 10 取map输出的最大重试次数 36 mapreduce.reduce.shuffle.connect.timeout 180000 REDUCE任务连接任务管理器获得map输出时的总耗时是3分钟 37 mapreduce.reduce.shuffle.read.timeout 180000 REDUCE任务等待map输出数据的总耗时是3分钟 38 mapred.task.timeout 600000 如果任务无读无写时的时间耗时为10分钟，将被终止 39 mapred.tasktracker.map.tasks.maximum 2 任管管理器可同时运行map任务数为2 40 mapred.tasktracker.reduce.tasks.maximum 2 任管管理器可同时运行reduce任务数为2 41 mapred.jobtracker.completeuserjobs.maximum 100 当用户的完成作业数达100个后，将其放入作业历史文件中 42 mapreduce.reduce.input.limit -1 Reduce输入量的限制。 43 mapred.job.tracker.retiredjobs.cache.size 1000 作业状态为已不在执行的保留在内存中的量为1000 44 mapred.job.tracker.jobhistory.lru.cache.size 5 作业历史文件装载到内存的数量 45 mapred.child.java.opts -Xmx200m 启动task管理的子进程时的内存设置 46 mapred.child.env 子进程的参数设置 47 mapred.child.ulimit 虚拟机所需内存的设定。 48 mapred.cluster.map.memory.mb -1 49 mapred.cluster.reduce.memory.mb -1 50 mapred.cluster.max.map.memory.mb -1 51 mapred.cluster.max.reduce.memory.mb -1 52 mapred.job.map.memory.mb -1 53 mapred.job.reduce.memory.mb -1 54 mapred.child.tmp /tmp Mr任务信息的存放目录 55 mapred.inmem.merge.threshold 1000 内存中的合并文件数设置 56 mapred.job.shuffle.merge.percent 0.66 57 mapred.job.shuffle.input.buffer.percent 0.70 58 mapred.job.reduce.input.buffer.percent 0.0 59 mapred.map.tasks.speculative.execution true Map任务的多实例并行运行标识 60 mapred.reduce.tasks.speculative.execution true Reduce任务的多实例并行运行标识 61 mapred.job.reuse.jvm.num.tasks 1 每虚拟机运行的任务数 62 mapred.min.split.size 0 Map的输入数据被分解的块数设置 63 mapred.jobtracker.maxtasks.per.job -1 一个单独作业的任务数设置 64 mapred.submit.replication 10 提交作业文件的复制级别 65 mapred.tasktracker.dns.interface default 任务管理跟踪器是否报告IP地址名的开关 66 mapred.tasktracker.dns.nameserver default 作业和任务管理跟踪器之间通讯方式采用的DNS服务的主机名或IP地址 67 tasktracker.http.threads 40 http服务器的工作线程数量 68 mapred.task.tracker.http.address 0.0.0.0:50060 任务管理跟踪器的http服务器的地址和端口 69 keep.failed.task.files false 失败任务是否保存到文件中 70 mapred.output.compress false 作业的输出是否压缩 71 mapred.output.compression.type RECORD 作业输出采用NONE, RECORD or BLOCK三种方式中一种压缩的写入到流式文件 72 mapred.output.compression.codec org.apache.hadoop.io.compress.DefaultCodec 压缩类的设置 73 mapred.compress.map.output false Map的输出是否压缩 74 mapred.map.output.compression.codec org.apache.hadoop.io.compress.DefaultCodec Map的输出压缩的实现类指定 75 map.sort.class org.apache.hadoop.util.QuickSort 排序键的排序类指定 76 mapred.userlog.limit.kb 0 每个任务的用户日志文件大小 77 mapred.userlog.retain.hours 24 作业完成后的用户日志留存时间为24小时 78 mapred.user.jobconf.limit 5242880 Jobconf的大小为5M 79 mapred.hosts 可与作业管理跟踪器连接的主机名 80 mapred.hosts.exclude 不可与作业管理跟踪器连接的主机名 81 mapred.heartbeats.in.second 100 作业管理跟踪器的每秒中到达的心跳数量为100 82 mapred.max.tracker.blacklists 4 任务管理跟踪器的黑名单列表的数量 83 mapred.jobtracker.blacklist.fault-timeout-window 180 任务管理跟踪器超时180分钟则訪任务将被重启 84 mapred.jobtracker.blacklist.fault-bucket-width 15 85 mapred.max.tracker.failures 4 任务管理跟踪器的失败任务数设定 86 jobclient.output.filter FAILED 控制任务的用户日志输出到作业端时的过滤方式 87 mapred.job.tracker.persist.jobstatus.active false 是否持久化作业管理跟踪器的信息 88 mapred.job.tracker.persist.jobstatus.hours 0 持久化作业管理跟踪器的信息的保存时间 89 mapred.job.tracker.persist.jobstatus.dir /jobtracker/jobsInfo 作业管理跟踪器的信息存放目录 90 mapreduce.job.complete.cancel.delegation.tokens true 恢复时是否变更领牌 91 mapred.task.profile false 任务分析信息是否建设标志 92 mapred.task.profile.maps 0-2 设置map任务的分析范围 93 mapred.task.profile.reduces 0-2 设置reduce任务的分析范围 94 mapred.line.input.format.linespermap 1 每次切分的行数设置 95 mapred.skip.attempts.to.start.skipping 2 在跳转模式未被设定的情况下任务的重试次数 96 mapred.skip.map.auto.incr.proc.count true MapRunner在调用map功能后的增量处理方式设置 97 mapred.skip.reduce.auto.incr.proc.count true 在调用reduce功能后的增量处理方式设置 98 mapred.skip.out.dir 跳过记录的输出目录 99 mapred.skip.map.max.skip.records 0 100 mapred.skip.reduce.max.skip.groups 0 101 job.end.retry.attempts 0 Hadoop偿试连接通知器的次数 102 job.end.retry.interval 30000 通知偿试回应的间隔操作为30秒 103 hadoop.rpc.socket.factory.class.JobSubmissionProtocol 指定与作业跟踪管理器的通讯方式，缺省是采用rpc方式 104 mapred.task.cache.levels 2 任务缓存级别设置 105 mapred.queue.names default 分隔作业队例的分隔符设定 106 mapred.acls.enabled false 指定ACL访问控制列表 107 mapred.queue.default.state RUNNING 定义队列的状态 108 mapred.job.queue.name default 已提交作业的队列设定 109 mapreduce.job.acl-modify-job 指定可修改作业的ACL列表 110 mapreduce.job.acl-view-job 指定可浏临作业的ACL列表 111 mapred.tasktracker.indexcache.mb 10 任务管理跟踪器的索引内存的最大容器 112 mapred.combine.recordsBeforeProgress 10000 在聚合处理时的记录块数 113 mapred.merge.recordsBeforeProgress 10000 在汇总处理时的记录块数 114 mapred.reduce.slowstart.completed.maps 0.05 115 mapred.task.tracker.task-controller org.apache.hadoop.mapred.DefaultTaskController 任务管理器的设定 116 mapreduce.tasktracker.group 任务管理器的组成员设定 117 mapred.healthChecker.script.path 脚本的绝对路径指定，这些脚本是心跳服务的 118 mapred.healthChecker.interval 60000 节点心跳信息的间隔 119 mapred.healthChecker.script.timeout 600000 120 mapred.healthChecker.script.args 参数列表 121 mapreduce.job.counters.limit 120 作业计数器的最小值 配置hadoop，主要是配置core-site.xml,hdfs-site.xml,mapred-site.xml三个配置文件，默认下来，这些配置文件都是空的，所以很难知道这些配置文件有哪些配置可以生效，上网找的配置可能因为各个hadoop版本不同，导致无法生效。浏览更多的配置，有两个方法: 1.选择相应版本的hadoop,下载解压后，搜索*.xml,找到core-default.xml,hdfs-default.xml,mapred-default.xml,这些就是默认配置,可以参考这些配置的说明和key，配置hadoop集群。 2.浏览apache官网,三个配置文件链接如下: http://hadoop.apache.org/common/docs/current/core-default.html http://hadoop.apache.org/common/docs/current/hdfs-default.html http://hadoop.apache.org/common/docs/current/mapred-default.html ​ 这里是浏览hadoop当前版本号的默认配置文件，其他版本号，要另外去官网找。其中第一个方法找到默认的配置是最好的，因为每个属性都有说明，可以直接使用。另外，core-site.xml是全局配置,hdfs-site.xml和mapred-site.xml分别是hdfs和mapred的局部配置。 2 常用的端口配置2.1 HDFS端口 参数 描述 默认 配置文件 例子值 fs.default.name namenode namenode RPC交互端口 8020 core-site.xml hdfs://master:8020/ dfs.http.address NameNode web管理端口 50070 hdfs- site.xml 0.0.0.0:50070 dfs.datanode.address datanode 控制端口 50010 hdfs -site.xml 0.0.0.0:50010 dfs.datanode.ipc.address datanode的RPC服务器地址和端口 50020 hdfs-site.xml 0.0.0.0:50020 dfs.datanode.http.address datanode的HTTP服务器和端口 50075 hdfs-site.xml 0.0.0.0:50075 2.2 MR端口 参数 描述 默认 配置文件 例子值 mapred.job.tracker job-tracker交互端口 8021 mapred-site.xml hdfs://master:8021/ job tracker的web管理端口 50030 mapred-site.xml 0.0.0.0:50030 mapred.task.tracker.http.address task-tracker的HTTP端口 50060 mapred-site.xml 0.0.0.0:50060 2.3 其它端口 参数 描述 默认 配置文件 例子值 dfs.secondary.http.address secondary NameNode web管理端口 50090 hdfs-site.xml 0.0.0.0:50090 3 三个缺省配置参考文件说明3.1 core-default.html 序号 参数名 参数值 参数说明 1 hadoop.tmp.dir /tmp/hadoop-${user.name} 临时目录设定 2 hadoop.native.lib true 使用本地hadoop库标识。 3 hadoop.http.filter.initializers http服务器过滤链设置 4 hadoop.security.group.mapping org.apache.hadoop.security.ShellBasedUnixGroupsMapping 组内用户的列表的类设定 5 hadoop.security.authorization false 服务端认证开启 6 hadoop.security.authentication simple 无认证或认证设置 7 hadoop.security.token.service.use_ip true 是否开启使用IP地址作为连接的开关 8 hadoop.logfile.size 10000000 日志文件最大为10M 9 hadoop.logfile.count 10 日志文件数量为10个 10 io.file.buffer.size 4096 流文件的缓冲区为4K 11 io.bytes.per.checksum 512 校验位数为512字节 12 io.skip.checksum.errors false 校验出错后是抛出异常还是略过标识。True则略过。 13 io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec 压缩和解压的方式设置 14 io.serializations org.apache.hadoop.io.serializer.WritableSerialization 序例化和反序列化的类设定 15 fs.default.name file:/// 缺省的文件URI标识设定。 16 fs.trash.interval 0 文件废弃标识设定，0为禁止此功能 17 fs.file.impl org.apache.hadoop.fs.LocalFileSystem 本地文件操作类设置 18 fs.hdfs.impl org.apache.hadoop.hdfs.DistributedFileSystem HDFS文件操作类设置 19 fs.s3.impl org.apache.hadoop.fs.s3.S3FileSystem S3文件操作类设置 20 fs.s3n.impl org.apache.hadoop.fs.s3native.NativeS3FileSystem S3文件本地操作类设置 21 fs.kfs.impl org.apache.hadoop.fs.kfs.KosmosFileSystem KFS文件操作类设置. 22 fs.hftp.impl org.apache.hadoop.hdfs.HftpFileSystem HTTP方式操作文件设置 23 fs.hsftp.impl org.apache.hadoop.hdfs.HsftpFileSystem HTTPS方式操作文件设置 24 fs.webhdfs.impl org.apache.hadoop.hdfs.web.WebHdfsFileSystem WEB方式操作文件类设置 25 fs.ftp.impl org.apache.hadoop.fs.ftp.FTPFileSystem FTP文件操作类设置 26 fs.ramfs.impl org.apache.hadoop.fs.InMemoryFileSystem 内存文件操作类设置 27 fs.har.impl org.apache.hadoop.fs.HarFileSystem 压缩文件操作类设置. 28 fs.har.impl.disable.cache true 是否缓存har文件的标识设定 29 fs.checkpoint.dir ${hadoop.tmp.dir}/dfs/namesecondary 备份名称节点的存放目前录设置 30 fs.checkpoint.edits.dir ${fs.checkpoint.dir} 备份名称节点日志文件的存放目前录设置 31 fs.checkpoint.period 3600 动态检查的间隔时间设置 32 fs.checkpoint.size 67108864 日志文件大小为64M 33 fs.s3.block.size 67108864 写S3文件系统的块的大小为64M 34 fs.s3.buffer.dir ${hadoop.tmp.dir}/s3 S3文件数据的本地存放目录 35 fs.s3.maxRetries 4 S3文件数据的偿试读写次数 36 fs.s3.sleepTimeSeconds 10 S3文件偿试的间隔 37 local.cache.size 10737418240 缓存大小设置为10GB 38 io.seqfile.compress.blocksize 1000000 压缩流式文件中的最小块数为100万 39 io.seqfile.lazydecompress true 块是否需要压缩标识设定 40 io.seqfile.sorter.recordlimit 1000000 内存中排序记录块类最小为100万 41 io.mapfile.bloom.size 1048576 BloomMapFiler过滤量为1M 42 io.mapfile.bloom.error.rate 0.005 43 hadoop.util.hash.type murmur 缺少hash方法为murmur 44 ipc.client.idlethreshold 4000 连接数据最小阀值为4000 45 ipc.client.kill.max 10 一个客户端连接数最大值为10 46 ipc.client.connection.maxidletime 10000 断开与服务器连接的时间最大为10秒 47 ipc.client.connect.max.retries 10 建立与服务器连接的重试次数为10次 48 ipc.server.listen.queue.size 128 接收客户连接的监听队例的长度为128 49 ipc.server.tcpnodelay false 开启或关闭服务器端TCP连接算法 50 ipc.client.tcpnodelay false 开启或关闭客户端TCP连接算法 51 webinterface.private.actions false Web交互的行为设定 52 hadoop.rpc.socket.factory.class.default org.apache.hadoop.net.StandardSocketFactory 缺省的socket工厂类设置 53 hadoop.rpc.socket.factory.class.ClientProtocol 与dfs连接时的缺省socket工厂类 54 hadoop.socks.server 服务端的工厂类缺省设置为SocksSocketFactory. 55 topology.node.switch.mapping.impl org.apache.hadoop.net.ScriptBasedMapping 56 topology.script.file.name 57 topology.script.number.args 100 参数数量最多为100 58 hadoop.security.uid.cache.secs 14400 3.2 hdfs-default.html 序号 参数名 参数值 参数说明 1 dfs.namenode.logging.level info 输出日志类型 2 dfs.secondary.http.address 0.0.0.0:50090 备份名称节点的http协议访问地址与端口 3 dfs.datanode.address 0.0.0.0:50010 数据节点的TCP管理服务地址和端口 4 dfs.datanode.http.address]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令进阶]]></title>
    <url>%2F2019%2F04%2F07%2FHDFS%20HA%E7%9A%84%E4%B8%80%E4%BA%9B%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[HDFS的基本结构 如上图所示，HDFS基本结构分NameNode、SecondaryNameNode、DataNode这几个。 NameNode：是Master节点，有点类似Linux里的根目录。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间； SecondaryNameNode：保存着NameNode的部分信息（不是全部信息NameNode宕掉之后恢复数据用），是NameNode的冷备份；合并fsimage和edits然后再发给namenode。（防止edits过大的一种解决方案） DataNode：负责存储client发来的数据块block；执行数据块的读写操作。是NameNode的小弟。 热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。 冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。 fsimage:元数据镜像文件（文件系统的目录树。） edits：元数据的操作日志（针对文件系统做的修改操作记录） namenode内存中存储的是=fsimage+edits。 NameNode详解作用： Namenode起一个统领的作用，用户通过namenode来实现对其他数据的访问和操作，类似于root根目录的感觉。 Namenode包含：目录与数据块之间的关系（靠fsimage和edits来实现），数据块和节点之间的关系 fsimage文件与edits文件是Namenode结点上的核心文件。 Namenode中仅仅存储目录树信息，而关于BLOCK的位置信息则是从各个Datanode上传到Namenode上的。 Namenode的目录树信息就是物理的存储在fsimage这个文件中的，当Namenode启动的时候会首先读取fsimage这个文件，将目录树信息装载到内存中。 而edits存储的是日志信息，在Namenode启动后所有对目录结构的增加，删除，修改等操作都会记录到edits文件中，并不会同步的记录在fsimage中。 而当Namenode结点关闭的时候，也不会将fsimage与edits文件进行合并，这个合并的过程实际上是发生在Namenode启动的过程中。 也就是说，当Namenode启动的时候，首先装载fsimage文件，然后在应用edits文件，最后还会将最新的目录树信息更新到新的fsimage文件中，然后启用新的edits文件。 整个流程是没有问题的，但是有个小瑕疵，就是如果Namenode在启动后发生的改变过多，会导致edits文件变得非常大，大得程度与Namenode的更新频率有关系。 那么在下一次Namenode启动的过程中，读取了fsimage文件后，会应用这个无比大的edits文件，导致启动时间变长，并且不可控，可能需要启动几个小时也说不定。 Namenode的edits文件过大的问题，也就是SecondeNamenode要解决的主要问题。 SecondNamenode会按照一定规则被唤醒，然后进行fsimage文件与edits文件的合并，防止edits文件过大，导致Namenode启动时间过长。 DataNode详解DataNode在HDFS中真正存储数据。 首先解释块（block）的概念： DataNode在存储数据的时候是按照block为单位读写数据的。block是hdfs读写数据的基本单位。 假设文件大小是100GB，从字节位置0开始，每128MB字节划分为一个block，依此类推，可以划分出很多的block。每个block就是128MB大小。 block本质上是一个 逻辑概念，意味着block里面不会真正的存储数据，只是划分文件的。 block里也会存副本，副本优点是安全，缺点是占空间 SecondaryNode 执行过程：从NameNode上 下载元数据信息（fsimage,edits），然后把二者合并，生成新的fsimage，在本地保存，并将其推送到NameNode，同时重置NameNode的edits. 工作原理（转自“大牛笔记”的博客，由于实现是清晰，受益很大，在此不做改动） 有一个文件FileA，100M大小。Client将FileA写入到HDFS上。 HDFS按默认配置。 HDFS分布在三个机架上Rack1，Rack2，Rack3。 a. Client将FileA按64M分块。分成两块，block1和Block2; b. Client向nameNode发送写数据请求，如图蓝色虚线①——&gt;。 c. NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②———&gt;。 ​ Block1: host2,host1,host3 ​ Block2: host7,host8,host4 ​ 原理： ​ NameNode具有RackAware机架感知功能，这个可以配置。 ​ 若client为DataNode节点，那存储block时，规则为：副本1，同client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。 ​ 若client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。 d. client向DataNode发送block1；发送过程是以流式写入。 ​ 流式写入过程， ​ 1&gt;将64M的block1按64k的package划分; ​ 2&gt;然后将第一个package发送给host2; ​ 3&gt;host2接收完后，将第一个package发送给host1，同时client想host2发送第二个package； ​ 4&gt;host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。 ​ 5&gt;以此类推，如图红线实线所示，直到将block1发送完毕。 ​ 6&gt;host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。 ​ 7&gt;client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线 ​ 8&gt;发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。 ​ 9&gt;发送完block2后，host7,host8,host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。 ​ 10&gt;client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。 分析，通过写过程，我们可以了解到： ​ ①写1T文件，我们需要3T的存储，3T的网络流量贷款。 ​ ②在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。 ​ ③挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。 读操作：读操作就简单一些了，如图所示，client要从datanode上，读取FileA。而FileA由block1和block2组成。 那么，读操作流程为： a. client向namenode发送读请求。 b. namenode查看Metadata信息，返回fileA的block的位置。 ​ block1:host2,host1,host3 ​ block2:host7,host8,host4 c. block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取； 上面例子中，client位于机架外，那么如果client位于机架内某个DataNode上，例如,client是host6。那么读取的时候，遵循的规律是： 优选读取本机架上的数据。 运算和存储在同一个服务器中，每一个服务器都可以是本地服务器 补充 元数据 元数据被定义为：描述数据的数据，对数据及信息资源的描述性信息。（类似于Linux中的i节点） 以 “blk_”开头的文件就是 存储数据的block。这里的命名是有规律的，除了block文件外，还有后 缀是“meta”的文件 ，这是block的源数据文件，存放一些元数据信息。 数据复制 NameNode做出关于块复制的所有决定。它周期性地从集群中的每个DataNode接收到一个心跳和一个阻塞报告。收到心跳意味着DataNode正常运行。Blockreport包含DataNode上所有块的列表。 此博客为转发博客]]></content>
      <tags>
        <tag>HDFS</tag>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce架构设计]]></title>
    <url>%2F2019%2F04%2F07%2FMapReduce%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[1.MapReduce 分布式计算框架 12345678910111213141516企业开发不用jiav代码，很复杂，很累赘，mr job基于磁盘运算，速度太慢Map：映射（元素的个数)hadoop001：x --》(x,1) key,value 键值对y --》(y,1)z --》(z,1)x --》(x,1)hadoop002：x --》(x,1)z --》(z,1)Reduce: 归约x,2y,1z,1 ** 2.MapReduce架构（重要） 当面试的时候问到，MapReduce 架构设计、Yarn架构设计、Yarn的工作流程、MapReduce job 提交到 Yarn的工作流程 （面试题为同一题），其实都是同一个问题。**123on Yarn 运行在Yarn上容器：container（Yarn的资源的抽象概念）运行在 Yarn nodemanager节点机器上，是一个虚拟的概念，将内存和cpu（vcore)封装成最小的单元，运行我们计算的任务task。 123456789101112131415vcore 虚拟core YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟CPU个数。物理corevcore: 虚拟内核 4core--》(生产1:2 默认，1:1) 处理并行度提高 3.task 运行计算任务在 container123456Yarn：job也叫 app也叫 application 也叫作业 ResourceManager：资源作业管理者它有两个属下（Applications Manager 作业管理 Resource Scheduler 资源调度）NodeManager：节点管理者 12345678910111213 1.用户向yarn提交应用程序（job），其中包括application Master程序，启动application Master命令等 2.RM为该job分配了一个容器，并于对应的NM通信，要求它在这个容器中启动job的MR application Master程序 3.启动程序之后，applocation Master首先向ApplicationsManager注册，用户就可以直接在web界面上查看job的整个运行状态和日志 4.applicationMaster向Resource Scheduler采用轮询的方式通过RPC协议去申请和领取资源列表 5.一旦applicationMaster申请到资源后，便与对应的NM节点通信，要求启动任务 6.NM为任务task设置好运行环境（环境变量，jar），将任务的启动命令写在一个脚本文件中，并通过脚本文件【启动任务】 7.各个task通过RPC协议向applicationMaster汇报自己的状态和进度，以让applicationMaster随时掌握各个任务的运行状态，从而可以在任务运行时重新启动任务，则web界面可以实时查看job当前运行状态。 8.job运行完成后，applicationMaster向RM注销自己并关闭自己一共分为两个阶段：&gt; 启动applicationMaster&gt; 由applicationMaster创建job，为它上去资源，并监控它的整个运行过程，知道运行完成 4.shuffle 洗牌123map--&gt; shufle--&gt; reducemap task默认设置为3个，reduce task默认设为1个，所以结果只有一个文件 祥细看一下博客：http://blog.itpub.net/30089851/viewspace-2095837/5.常用命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[hadoop@hadoop002 bin]$ mapred --helpUsage: mapred [--config confdir] COMMAND where COMMAND is one of: pipes run a Pipes job job manipulate MapReduce jobs queue get information regarding JobQueues classpath prints the class path needed for running mapreduce subcommands historyserver run job history servers as a standalone daemon distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive archive-logs combine aggregated logs into hadoop archives hsadmin job history server admin interfaceMost commands print help when invoked w/o parameters.[hadoop@hadoop002 bin]$ mapred job Usage: CLI &lt;command&gt; &lt;args&gt; [-submit &lt;job-file&gt;] [-status &lt;job-id&gt;] [-counter &lt;job-id&gt; &lt;group-name&gt; &lt;counter-name&gt;] [-kill &lt;job-id&gt;] [-set-priority &lt;job-id&gt; &lt;priority&gt;]. Valid values for priorities are: VERY_HIGH HIGH NORMAL LOW VERY_LOW [-events &lt;job-id&gt; &lt;from-event-#&gt; &lt;#-of-events&gt;] [-history [all] &lt;jobHistoryFile|jobId&gt; [-outfile &lt;file&gt;] [-format &lt;human|json&gt;]] [-list [all]] [-list-active-trackers] [-list-blacklisted-trackers] [-list-attempt-ids &lt;job-id&gt; &lt;task-type&gt; &lt;task-state&gt;]. Valid values for &lt;task-type&gt; are MAP REDUCE. Valid values for &lt;task-state&gt; are running, completed [-kill-task &lt;task-attempt-id&gt;] [-fail-task &lt;task-attempt-id&gt;] [-logs &lt;job-id&gt; &lt;task-attempt-id&gt;]Generic options supported are-conf &lt;configuration file&gt; specify an application configuration file-D &lt;property=value&gt; use value for given property-fs &lt;local|namenode:port&gt; specify a namenode-jt &lt;local|resourcemanager:port&gt; specify a ResourceManager-files &lt;comma separated list of files&gt; specify comma separated files to be copied to the map reduce cluster-libjars &lt;comma separated list of jars&gt; specify comma separated jar files to include in the classpath.-archives &lt;comma separated list of archives&gt; specify comma separated archives to be unarchived on the compute machines.The general command line syntax isbin/hadoop command [genericOptions] [commandOptions][hadoop@hadoop002 bin]$ mapred job -list 19/02/23 21:39:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable19/02/23 21:39:35 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032Total jobs:1 JobId State StartTime UserName Queue Priority UsedContainers RsvdContainers UsedMem RsvdMem NeededMem AM info job_1550323870337_1633 PREP 1550928758524 hadoop root.hadoop NORMAL 0 0 0M 0M 0M http://hadoop002:8088/proxy/application_1550323870337_1633/[hadoop@hadoop002 bin]$ mapred job -kill job_1550323870337_163319/02/23 21:39:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable19/02/23 21:39:48 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:803219/02/23 21:39:49 INFO impl.YarnClientImpl: Killed application application_1550323870337_1633Killed job job_1550323870337_1633[hadoop@hadoop002 bin]$ [hadoop@hadoop002 bin]$]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>架构设计</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS的读写流程及副本放置机制]]></title>
    <url>%2F2019%2F04%2F07%2FHDFS%E7%9A%84%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[1.写流程 FSDataOutputStream 1234567891011121314151617181920212223242526这个过程对于我们操作者而言是无感知的1.Client调用FileSystem.creat(filePath)方法，去与NN进行【RPC】通信，nn check该路径文件是否存在以及有没有权限创建该文件。假如OK，就创建一个新的文件，但是不关联任何的block，nn根据上传的文件大小且块大小且副本数，计算多少块，以及块存放的dn，最终将这些信息返回给客户端，则为【FSDataOutputStream】2.Client调用FSDataOutStream.write方法，将第一个块的第一个副本写到第一个DN，写完写第二个副本，写完写第三个副本，当第三个副本写完，返回ack packet给第二个副本的DN，然后第二个副本返回ack packer给第一个DN，第一个DN返回 ack packet给FSDdataOutputStream对象，标识第一个块三个副本写完了，然后依次写剩余的块。3.当文件写入数据完成后，Client掉用FSDataOutputStream.close()方法，关闭输出流，flush换成区的数据包。再次调用FileSysteam.complete(),通知NN节点写入成功。3DN3副本副本数=DN1DN1副本测试：DN挂了 能不能写入同比：3DN 3副本 1DN挂了 肯定写不成功如果10DN3副本副本书&lt;DN测试：DN挂了，是可以写入的总结：存活的DN满足我们的副本书 就能写 2.读流程 FSDataIntputStream 1234567891.Client通过FileSystem.open(filePath),去与NN进行【RPC】通信，返该文件的部分或全部的block列表，也就是返回FSDataInputStream对象。2.Client调用【FSDataInputStream】对象的read()方法，a.去与第一个块的最近的DN进行read，读取完后，会check，假如success，会关闭与当前DN通信。假如fail 会记录失败的DN +block信息，下次就不会读取，那么会去该块的第二个DN地址读取。b.然后去第二个块的最近的DN读取，会check，如果cuccess，会关闭与当前DN的通信c.假如当前block列表全部读取完成，文件还没有结束，那么FileSystem会从NN获取下一批大的block列表。3.Client调用FSDataInputStream.close()关闭输入流。 3.副本放置策略 1生产上尽量将读写的动作 选取DN节点]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hdfs</tag>
        <tag>读写流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Web界面解读]]></title>
    <url>%2F2019%2F04%2F07%2FHadoop%20Web%E7%95%8C%E9%9D%A2%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[&gt; http://hadoop001:50070 HDFS界面1.表头 2.总览 3.文件浏览器，可以在里面看到我们hdfs上的数据4.点击对应的文件可以看到文件的详细信息5.文件详细信息** &gt; http://hadoop001:8088 yarn界面**]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>web界面</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS架构设计]]></title>
    <url>%2F2019%2F04%2F07%2FHDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[1.块 block 1234567891011121314151617 &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; 块的大小为128M 副本数为3 比如一个文件260M 实际存储 规格块A: 128M 128M A0 A1 A2块B: 128M 128M B0 B1 B2块C: 4M 128M C0 C1 C2 面试题：一个文件160m,块大小128m，副本数2。请问实际几个块，实际物理存储多少？ 答：一共两个块，但是副本是2，那么2*2就是4个块，实际物理存储就是160M*2=320M &gt; 2.HDFS架构设计（重要）：进程： 123namenode 也叫nn 名称节点datanode 也叫dn 数据节点ssecondary namenode 也叫snn 第二名称节点 主从架构： 12Rack : 机架 可以放多个主机 10个， GPU主机 5个，放多少是根据机柜的电流来计算的， 总的电流量除以单个主机的电流量 然后初步估算。 nn： 1234567891011121314151617181920212223242526272829303132333435 nn是文件系统的命名空间 a.文件名称 b.文件目录结构 c.文件属性 创建时间 权限 副本数 d.文件对应哪些数据块 ---&gt;数据块对应在哪些datanode节点上 nn维护的blockmap： nn节点不会持久存储这种映射关系，dn定期发送blockreport 给nn，以此nn在【内存】 中动态维护这种映射关系！ (生产上 hdfs不适合存储小文件？为什么不合适？如果真的有小文件，该怎么办？该怎么合并) 假设 nn 内存空间8G 如果全是小文件，那么内存很容易被撑爆了 1个小文件：nn节点需要250字节，一亿：1亿*250个字节。 假如真的有小文件，那么我们就将小文件合并成一个大文件 100个小文件合并成一个大文件：nn节点可能只需要300字节，一亿/100*300字节 建议：合并为一个文件尽量小于块的大小 120M&lt;=128m 持久化数据 作用： 管理文件系统的命名空间 维护文件系统树，以两种文件永久保存在磁盘上 一种是 命名空间镜像文件： fsimage 另一种是 编辑日志： editlog[root@hadoop001 current]# pwd/tmp/hadoop-root/dfs/name/current[root@hadoop001 current]# lltotal 1040-rw-r--r--. 1 root root 1048576 Feb 17 20:23 edits_inprogress_0000000000000000001-rw-r--r--. 1 root root 321 Feb 17 19:23 fsimage_0000000000000000000-rw-r--r--. 1 root root 62 Feb 17 19:23 fsimage_0000000000000000000.md5-rw-r--r--. 1 root root 2 Feb 17 19:23 seen_txid-rw-r--r--. 1 root root 219 Feb 17 19:23 VERSION[root@hadoop001 current]# dn： 12345存储数据块 和数据块的校验和与nn通信：a.每隔3秒发送一次心跳 在datanode上可以看到上一次的通信时间，每3秒刷新一次b.每10次心跳发送一次当前节点的blockreport作用：读写文件的数据块 snn： 12345 存储：fsiamage+editlog 作用：定期合并fsimage+editlog文件为新的fsimage文件，推送给nn节点，简称为检查点checkpoint 参数：dfs.namenode.checkpoint.period 3600s 由于snn只能备份一小时之前的数据，这一小时的数据不会被备份，所以在生产上一般还会采用其他方式进行热备份]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>HDFS</tag>
        <tag>架构设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[window系统的hosts配置，主机名代替ip访问web界面]]></title>
    <url>%2F2019%2F04%2F07%2Fwindow%E7%B3%BB%E7%BB%9F%E7%9A%84hosts%E9%85%8D%E7%BD%AE%EF%BC%8C%E4%B8%BB%E6%9C%BA%E5%90%8D%E4%BB%A3%E6%9B%BFip%E8%AE%BF%E9%97%AEweb%E7%95%8C%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[1.hosts文件 123无论是Liinux还是window系统中都有hosts文件window的hosts文件存放在：C:\Windows\System32\drivers\etc 配置下在生产中，我们要用主机名来代替ip地址，解析主机名称时变解析到ip地址。 访问web页面时效果图如下：]]></content>
      <tags>
        <tag>配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据中的面试题]]></title>
    <url>%2F2019%2F04%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[第一道： 123456789id impl（展示） click（点击）1 1 11 0 11 0 01 1 02 1 0。。。。求：每个用户的展示和点击数（正常的） ， 每个用户的不正常的展示和点击数答：（不正常的是：没展示，就点击了） 第二道： 12345678910111213141516171819202122232425262728293031323334353637sql中的left join 和 join准备数据：a表+------+------+| id | name |+------+------+| 3 | ccc || 2 | bbb || 1 | aaa |+------+------+b表：+------+------+| id | name |+------+------+| 1 | aaa || 2 | bbb || 4 | ccc |+------+------+join操作：mysql&gt; select * from a join b on a.id=b.id;+------+------+------+------+| id | name | id | name |+------+------+------+------+| 1 | aaa | 1 | aaa || 2 | bbb | 2 | bbb |+------+------+------+------+left join操作：mysql&gt; select * from a left join b on a.id=b.id;+------+------+------+------+| id | name | id | name |+------+------+------+------+| 3 | ccc | NULL | NULL || 2 | bbb | 2 | bbb || 1 | aaa | 1 | aaa |+------+------+------+------+]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Linux系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount的实际操作]]></title>
    <url>%2F2019%2F04%2F07%2Fwordcount%E7%9A%84%E5%AE%9E%E9%99%85%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[** 提交 mr 作业到 yarn 上运行 wc** 1.先编辑两个文件： 12345678910111213141516171819202122232425[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ vi a.log（先编辑一个文件保存） ruozejepson www.ruozedata.com dashu adai fanren 1 a b c a bc ruoze jepon [hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ vi b.txt（再编辑一个文件保存） ab d e f ruoze 1 1 3 5 2.在 hdfs的家目录下创建一个级联目录： 1hdfs dfs -mkdir -p / wordcount/input 3.将a.log和b.text文件上传到hdfs下： 12[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -put a.log /wordcount/input[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -put b.txt /wordcount/input 4.我们可以通过：find ./ -name ‘example.jar’找到这个jar包： 1./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar 5.这个时候我们需要查看命令帮助： 1hadoop 回车 向下翻看，会发现一个 jar的命令 6.输入命令：7.继续输入命令：（注意：output1事先一定是不存在的）8.查看计算结果： 12345678910111213141516171819202122232425262728293031323334353637383940[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hadoop jar \./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar \wordcount /wordcount/input /wordcount/output1[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -cat /wordcount/output1/part-r-0000019/02/16 22:05:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable1 33 15 1a 3adai 1b 3c 2d 1dashu 1e 1f 1fanren 1jepon 1jepson 1ruoze 3www.ruozedata.com 1[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ hdfs dfs -get /wordcount/output1/part-r-00000 ./[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ cat part-r-000001 33 15 1a 3adai 1b 3c 2d 1dashu 1e 1f 1fanren 1jepon 1jepson 1ruoze 3www.ruozedata.com 1]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Linux系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于jps的深入了解以及生产中jps的一些坑]]></title>
    <url>%2F2019%2F04%2F07%2F%E5%85%B3%E4%BA%8Ejps%E7%9A%84%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%8F%8A%E7%94%9F%E4%BA%A7%E4%B8%ADjps%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91%2F</url>
    <content type="text"><![CDATA[** 1.jps命令的真相（这种情况是针对于：生产环境:） hadoop: hdfs组件 hdfs用户root用户或sudo权限的用户取获取 1.1 jps命令位置哪里的** 12[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ which jps/usr/java/jdk1.8.0_45/bin/jps 2.2 对应的进程的标识文件在哪 /tmp/hsperfdata_进程用户名称 12345678[hadoop@hadoop002 hsperfdata_hadoop]$ pwd/tmp/hsperfdata_hadoop[hadoop@hadoop002 hsperfdata_hadoop]$ lltotal 96-rw------- 1 hadoop hadoop 32768 Feb 16 20:35 1086-rw------- 1 hadoop hadoop 32768 Feb 16 20:35 1210-rw------- 1 hadoop hadoop 32768 Feb 16 20:35 1378[hadoop@hadoop002 hsperfdata_hadoop]$ 2.3 用户查看你情况 12345678910root用户看所有用户的jps结果普通用户只能看自己的当用root用户查看进程的时候，会出现这样的情况：[root@hadoop002 ~]# jps1520 Jps1378 -- process information unavailable1210 -- process information unavailable1086 -- process information unavailable不管进程还可用不可用，都会出现以上信息，进程的残留信息，让人无法判断进程是否可用。process information unavailable （会显示这样的信息，如果对于不知道jps命令的人很有可能会认为这个进程已经死掉了，但是通过普通用户查看时，进程依然再执行。） 2.4 所以怎么去判断 jps中的进程是否可用： 1真假判断: ps -ef|grep namenode 真正判断进程是否可用 2.5 为什么jps下的进程会死掉： 12一种时人为的情况另一种是 进程在Linux看来是耗内存最大的 自动给你kill 2.5如何清除掉root用户下 jps查看进程的残留： 12345[root@hadoop002 tmp]# rm -rf hsperfdata_hadoop （将这一进程标识文件删除掉就可以了）[root@hadoop002 tmp]# [root@hadoop002 tmp]# jps1906 Jps[root@hadoop002 tmp]# 2.6.pid文件 集群进程启动和停止需要的文件 1234567在家目录下/tmp下的pid文件-rw-rw-r-- 1 hadoop hadoop 5 Feb 16 20:56 hadoop-hadoop-datanode.pid-rw-rw-r-- 1 hadoop hadoop 5 Feb 16 20:56 hadoop-hadoop-namenode.pid-rw-rw-r-- 1 hadoop hadoop 5 Feb 16 20:57 hadoop-hadoop-secondarynamenode.pidLinux在tmp命令 定期删除一些文件和文件夹 30天周期有时候 Linux会将你的pid文件给删除掉，这个时候，当你需要关闭的时候，却关不了，需要开启的时候，却仍停留再上一次开启，这个时候应该怎么解决这样的问题呢。 2.7如何解决： 12345新建一个目录，mkdir /data/tmpchmod -R 777 /data/tmp将pid文件放在这个目录的下面。修改环境变量：export HADOOP_PID_DIR=/data/tmp]]></content>
      <tags>
        <tag>命令操作</tag>
        <tag>jps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产中如何通过日志查看错误]]></title>
    <url>%2F2019%2F04%2F07%2F%E7%94%9F%E4%BA%A7%E4%B8%AD%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%97%A5%E5%BF%97%E6%9F%A5%E7%9C%8B%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[解决步骤： 1234在生产中我们运行命令时肯定会出现失败的情况，如果遇到这样的情况我们该怎么办,那么这个时候我们就需要会查看日志，在日志中查找错误ps：在hadoop的场景下：logs/目录下： 123hadoop-hadoop-datanode-hadoop002.log 对应的名字分别是：elm hadoop-用户-进程名称-机器名称 一共有三种方法可以去查看： 123401 vi :/搜索 ERROR02 tail -200f xxx.log（倒着查看log中的后200行日志） 另外窗口重启进程 为了再现这个错误03 rz上传到windows editplus去定位查看 备份 （一般对于生产中日志比较大的文件）]]></content>
      <tags>
        <tag>排查错误</tag>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.hadoop伪分布式部署-hdfs进程启动的机器修改-yarn部署]]></title>
    <url>%2F2019%2F04%2F07%2F2.Hadoop%E9%87%8D%E6%96%B0%E9%83%A8%E7%BD%B2%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8Fhdfs%2F</url>
    <content type="text"><![CDATA[1.hdfs三个进程要以hadoop002启动： 123456789101112131415161718 这里的 hadoop001指的是你的主机名， 配置文件在 etc/hadoop目录下：[hadoop@hadoop002 hadoop]$ lltotal 140-rw-r--r-- 1 hadoop hadoop 884 Feb 13 22:34 core-site.xml （存放hdfs，mapredurce，yarn的公共配置信息）-rw-r--r-- 1 hadoop hadoop 4294 Feb 13 22:30 hadoop-env.sh JDK目录 hadoop家目录-rw-r--r-- 1 hadoop hadoop 867 Feb 13 22:34 hdfs-site.xml（存放hdfs独有的配置信息）-rw-r--r-- 1 hadoop hadoop 11291 Mar 24 2016 log4j.properties-rw-r--r-- 1 hadoop hadoop 758 Mar 24 2016 mapred-site.xml.template（存放mapreduce独有的配置信息）-rw-r--r-- 1 hadoop hadoop 10 Mar 24 2016 slaves（这里修改 datanode的机器名称）-rw-r--r-- 1 hadoop hadoop 690 Mar 24 2016 yarn-site.xml（存放yarn独有的配置信息）生产 学习: 不用ip部署，统一机器名称hostname部署，如果有一天公司修改了网段，我们只需要/etc/hosts 修改ip即可即可(第一行 第二行不要删除) 2.修改配置文件：123456789101112131415161718192021222324namenode进程:（namenode配置信息在core-site.xml 中 core中配置的是共有的重要的信息）[hadoop@hadoop002 hadoop]$ vi core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop002:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;datanode进程:（datanode配置信息在 slaves中）[hadoop@hadoop002 hadoop]$ vi slaves hadoop002secondarynamenode进程:（secondarynamenode的配置在 hdfs-size.xml中） vi hdfs-size.xml&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop002:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop002:50091&lt;/value&gt;&lt;/property&gt; 在修改配置文件信息的失手，我们应该学会查看官网，根据官网的信息来进行修改: 这里有是配置信息的参数名称和信息。 3.重新构建ssh信任关系：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495 之前的配置也是可以的，但是我们为了统一 重新再来一次配置 [hadoop@hadoop002 ~]$ rm -rf .ssh[hadoop@hadoop002 ~]$ ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): Created directory '/home/hadoop/.ssh'.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/hadoop/.ssh/id_rsa.Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.The key fingerprint is:ca:e4:a1:fc:9f:e2:86:e7:9c:ab:f2:19:7a:70:c5:3d hadoop@hadoop002The key's randomart image is:+--[ RSA 2048]----+| || || . . || o E || .o S. || ...= o || o+.+ || ..o=+. . || .++*B+o |+-----------------+[hadoop@hadoop002 ~]$ cd .ssh[hadoop@hadoop002 .ssh]$ lltotal 8-rw------- 1 hadoop hadoop 1675 Feb 16 20:27 id_rsa-rw-r--r-- 1 hadoop hadoop 398 Feb 16 20:27 id_rsa.pub[hadoop@hadoop002 .ssh]$ cat id_rsa.pub &gt;&gt; authorized_keys[hadoop@hadoop002 .ssh]$ lltotal 12-rw-rw-r-- 1 hadoop hadoop 398 Feb 16 20:27 authorized_keys-rw------- 1 hadoop hadoop 1675 Feb 16 20:27 id_rsa-rw-r--r-- 1 hadoop hadoop 398 Feb 16 20:27 id_rsa.pub[hadoop@hadoop002 .ssh]$ cd[hadoop@hadoop002 ~]$ cd app/hadoop-2.6.0-cdh5.7.0[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ sbin/start-dfs.sh 19/02/16 20:28:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [hadoop002]The authenticity of host 'hadoop002 (172.31.236.240)' can't be established.RSA key fingerprint is b1:94:33:ec:95:89:bf:06:3b:ef:30:2f:d7:8e:d2:4c.Are you sure you want to continue connecting (yes/no)? yeshadoop002: Warning: Permanently added 'hadoop002,172.31.236.240' (RSA) to the list of known hosts.hadoop@hadoop002's password: [2]+ Stopped sbin/start-dfs.sh[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ jps962 Jps[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ ps -ef|grep start-dfs.shhadoop 790 349 0 20:25 pts/0 00:00:00 bash sbin/start-dfs.shhadoop 887 349 0 20:28 pts/0 00:00:00 bash sbin/start-dfs.shhadoop 977 349 0 20:28 pts/0 00:00:00 grep start-dfs.sh[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ kill -9 790 887[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ cd[1]- Killed sbin/start-dfs.sh (wd: ~/app/hadoop-2.6.0-cdh5.7.0)(wd now: ~)[2]+ Killed sbin/start-dfs.sh (wd: ~/app/hadoop-2.6.0-cdh5.7.0)(wd now: ~)[hadoop@hadoop002 ~]$ cd .ssh[hadoop@hadoop002 .ssh]$ lltotal 16-rw-rw-r-- 1 hadoop hadoop 398 Feb 16 20:27 authorized_keys-rw------- 1 hadoop hadoop 1675 Feb 16 20:27 id_rsa-rw-r--r-- 1 hadoop hadoop 398 Feb 16 20:27 id_rsa.pub-rw-r--r-- 1 hadoop hadoop 406 Feb 16 20:28 known_hosts[hadoop@hadoop002 .ssh]$ chmod 600 authorized_keys[hadoop@hadoop002 .ssh]$ [hadoop@hadoop002 .ssh]$ cd -/home/hadoop[hadoop@hadoop002 ~]$ cd app/hadoop-2.6.0-cdh5.7.0启动hdfs 发现三个进程都以hadoop002启动[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ sbin/start-dfs.sh 19/02/16 20:29:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [hadoop002]hadoop002: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop002.outhadoop002: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop002.outStarting secondary namenodes [hadoop002]hadoop002: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop002.out19/02/16 20:29:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>yarn配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.hdfs为分布式部署]]></title>
    <url>%2F2019%2F04%2F07%2F1.hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2-hdfs%2F</url>
    <content type="text"><![CDATA[Hadoop： 12广义：以apache hadoop软件为主的生态圈（hive,zookeeper,spark,hbase）狭义：apache hadoop软件 查询组件的官网： 123hadoop.apachehive.apache.orgspark.apache.org hadoop软件有哪些组件： 123hdfs:存储，分布式文件系统mapreduce：计算yarn：资源（cpu，memory）和作业调度 我所用的版本为 hadoop2.6.0-cdh5.7.0cdh hadoop：http://archive.cloudera.com/cdh5/cdh/5/ 1可以通过wget上传：http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz 1.创建用户和上传hadoop软件： 123456useradd hadoopsu - hadoop 切换用户[hadoop@hadoop002 ~]$ mkdir app （需创建一个 app 文件夹 下存放hadoop软件）[hadoop@hadoop002 ~]$ cd app/[hadoop@hadoop002 ~]$ wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz或[hadoop@hadoop002 ~]$ rz 上传 2.部署jdk： 12345678910111213141516171819202122232425262728由于CDH环境的要求：jdk必须要部署在 /usr/java 目录下，否则会出现一些坑。以后需要在/usr/shara/java 下部署CDH 需要的 mysql jdbc 的jar包rz上传 jdk-8u45-linux-x64.gz解压：[root@hadoop002 java]# tar -xzvf jdk-8u45-linux-x64.gz[root@hadoop002 java]# lltotal 319156drwxr-xr-x 8 uucp 143 4096 Apr 11 2015 jdk1.8.0_45-rw-r--r-- 1 root root 173271626 Sep 19 11:49 jdk-8u45-linux-x64.gz注意：（解压之后会出现 jdk中的用户和用户组不对的情况，需要修改用户和用户组，这是查看 cat /etc/passwd，会发现有这两种形式：/bin/bash和/sbin/nologin，将用户的bash修改成nologin，用户就会无法登陆，su - zookeeper切不了：This account is currently not available.生产怎么做：/sbin/nologin --》 /bin/bash）权限修正[root@hadoop002 java]# chown -R root:root jdk1.8.0_45[root@hadoop002 java]# lltotal 319156drwxr-xr-x 8 root root 4096 Apr 11 2015 jdk1.8.0_45-rw-r--r-- 1 root root 173271626 Sep 19 11:49 jdk-8u45-linux-x64.gz配置环境变量：[root@hadoop002 java]# vi /etc/profile#envexport JAVA_HOME=/usr/java/jdk1.8.0_45export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=.:$JAVA_HOME/lib:$JER_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$JER_HOME/bin:$PATH配置完成后 source /etc/peofile 3.解压hadoop： 12345678910111213141516173.解压hadoop [hadoop@hadoop002 app]$ tar -xzvf hadoop-2.6.0-cdh5.7.0.tar.gz [hadoop@hadoop002 app]$ cd hadoop-2.6.0-cdh5.7.0 [hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ ll total 76 drwxr-xr-x 2 hadoop hadoop 4096 Mar 24 2016 bin 可执行脚本 drwxr-xr-x 2 hadoop hadoop 4096 Mar 24 2016 bin-mapreduce1 drwxr-xr-x 3 hadoop hadoop 4096 Mar 24 2016 cloudera drwxr-xr-x 6 hadoop hadoop 4096 Mar 24 2016 etc 配置目录(conf) drwxr-xr-x 5 hadoop hadoop 4096 Mar 24 2016 examples drwxr-xr-x 3 hadoop hadoop 4096 Mar 24 2016 examples-mapreduce1 drwxr-xr-x 2 hadoop hadoop 4096 Mar 24 2016 include drwxr-xr-x 3 hadoop hadoop 4096 Mar 24 2016 lib jar包目录 drwxr-xr-x 2 hadoop hadoop 4096 Mar 24 2016 libexec drwxr-xr-x 3 hadoop hadoop 4096 Mar 24 2016 sbin hadoop组件的启动 停止脚本 drwxr-xr-x 4 hadoop hadoop 4096 Mar 24 2016 share drwxr-xr-x 17 hadoop hadoop 4096 Mar 24 2016 src 4.修改配置文件信息Configuration： 12345678910111213141516171819Use the following:etc/hadoop/core-site.xml:&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;etc/hadoop/hdfs-site.xml:&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.配置ssh localhost无密码信任关系： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 &gt; 这里要注意 我用的和官方的方法不一样[hadoop@hadoop002 ~]$ ssh-keygen（密钥生成）Generating public/private rsa key pair.（三下回车）Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): Created directory &apos;/home/hadoop/.ssh&apos;.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/hadoop/.ssh/id_rsa.Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.The key fingerprint is:ba:48:3d:ff:af:4d:da:74:67:31:d6:98:ad:a0:b3:76 hadoop@hadoop002The key&apos;s randomart image is:+--[ RSA 2048]----+| || || || +.|| S . o+o|| . . . ...o|| . + o o o o|| . . + .OE. o || . . .o=++ |+-----------------+[hadoop@hadoop002 ~]$ cd .ssh[hadoop@hadoop002 .ssh]$ lltotal 8-rw------- 1 hadoop hadoop 1675 Feb 13 22:36 id_rsa 私钥-rw-r--r-- 1 hadoop hadoop 398 Feb 13 22:36 id_rsa.pub 公钥[hadoop@hadoop002 .ssh]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys（将公钥放到认证文件中，来提供访问）[hadoop@hadoop002 .ssh]$ lltotal 12-rw-rw-r-- 1 hadoop hadoop 398 Feb 13 22:37 authorized_keys-rw------- 1 hadoop hadoop 1675 Feb 13 22:36 id_rsa-rw-r--r-- 1 hadoop hadoop 398 Feb 13 22:36 id_rsa.pub-rw-r--r-- 1 hadoop hadoop 0 Feb 13 22:39 known_hostsssh localhost date 是需要输入密码，但是这个用户是没有配置密码。我们应该在没有配置密码情况下去完成无密码信任呢？将authorized_keys 权限修改成 640[hadoop@hadoop002 .ssh]$ chmod 600 authorized_keys[hadoop@hadoop002 .ssh]$ ssh localhost dateThe authenticity of host &apos;localhost (127.0.0.1)&apos; can&apos;t be established.RSA key fingerprint is b1:94:33:ec:95:89:bf:06:3b:ef:30:2f:d7:8e:d2:4c.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;localhost&apos; (RSA) to the list of known hosts.Wed Feb 13 22:41:17 CST 2019 6.格式化： 1[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ bin/hdfs namenode -format 7.启动： 123456789101112131415161718192021[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ sbin/start-dfs.sh（启动）19/02/13 22:57:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [localhost]localhost: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop002.outlocalhost: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop002.out&gt; ssh 信任关系 是配置localhostStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop002.out19/02/13 22:57:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ jps15059 Jps14948 SecondaryNameNode 第二名称节点 老二14783 DataNode 数据节点 小弟14655 NameNode 名称节点 老大 读写（必须保证这三个进程都有）[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ open http://ip:50070 8.配置环境变量： 123456789101112[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ vi ~/.bash_profile# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi# User specific environment and startup programsexport HADOOP_PREFIX=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0export PATH=$HADOOP_PREFIX/bin:$PATH[hadoop@hadoop002 hadoop-2.6.0-cdh5.7.0]$ 9.命令： 12hdfs dfs -mkdir /ruozedate(在根目录下创建一个ruozedate文件夹)hdfs dfs -ls /（查看根目录下文件或者文件夹） 10.如何查看命令帮助： 1hdfs dfs]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[普通用户如何获取root最大权限]]></title>
    <url>%2F2019%2F04%2F07%2F%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%E5%A6%82%E4%BD%95%E4%B8%B4%E6%97%B6%E8%8E%B7%E5%8F%96root%E6%9C%80%E5%A4%A7%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[首先在root用户下 编辑：vi etc/sudoers 进入配置文件找到如下内容： 在下面编辑一行： 输入 ：wq！保存并且强制退出。这样就赋予jepson用户root sudo权限了。查看root下目录时应先输入sudo：]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Linux系统</tag>
        <tag>权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux配置多台机器ssh信任关系]]></title>
    <url>%2F2019%2F04%2F07%2FLinux%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E6%9C%BA%E5%99%A8ssh%E4%BF%A1%E4%BB%BB%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[** 配置用户ssh互信关系，支持多台机器间的无密码访问：** ** 注意事项： ~/.ssh的权限为700** 1chmod -R 700 /.ssh ~/.ssh/authorized_keys的权限是600或者640 1chmod -R 600 /.ssh/authorized_keys 操作步骤：123依次在3台机器上分别执行：ssh-keygen 命令一直回车即可 进入ssh下，ll查看： 将id——rsa.pub公钥文件追加到authorized认证文件中 将hadoop002和hadoop003机器上的id_rsa.pub文件传输到hadoop001机器中： 这个时候会发现 让我们输入密码，但是我们用的hadoop用户，并没有密码，所以需要用root用户： 查看hadoop001中ssh下的文件： 将传过来的公钥文件追加到认证文件中： cat查看一下公钥文件中的信息： 这个时候我们需要配置一下hosts中的配置： 然后将配置到的公钥文件authorized传到其它两台机器的ssh下，这就完成了多台机器ssh互信配置]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于生产中tail -f和tail -F的坑]]></title>
    <url>%2F2019%2F04%2F05%2F%E5%85%B3%E4%BA%8E%E7%94%9F%E4%BA%A7%E4%B8%ADtail%20-f%E5%92%8Ctail%20-F%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[大家可能都知道，tail -f 和tail -F都是用来进行实时监控的那么这两者之间有什么区别呢？ tail -F=tail-f +retry表示：当tail -F去监控一个文件时，如果将这个文件删掉，但是之后又创建出来，那么tail -F还会继续监控，但是tail-f就不一样，删除掉这个文件后，再创建出来时，它不会再去监控这个文件。 $ tail –help-f, –follow[={name|descriptor}]​ output appended data as the file grows;​ -f, –follow, and –follow=descriptor are​ equivalent-F same as –follow=name –retry 而我们的log日志，是每达到200M，是要重新重命名的，比如加上序号1.，然后重新创建这个日志。所以在tail 一个log文件的时候, 文件滚动之后这个tail -f命令,就失效了.-F 是–follow=name –retry的缩写, –follow=name是按照文件名跟踪文件, 可以定期去重新打开文件检查文件是否被其它程序删除并重新建立. –retry这个参数, 保证文件重新建立后,可以继续被跟踪 详细可以查看网址：http://blog.itpub.net/30089851/viewspace-2134067/]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Linux系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[作业调度 crontab -e和crontab -l]]></title>
    <url>%2F2019%2F04%2F05%2F%E4%BD%9C%E4%B8%9A%E8%B0%83%E5%BA%A6%20crontab%20-e%E5%92%8Ccrontab%20-l%2F</url>
    <content type="text"><![CDATA[作业调度： 12345678910111213141516171819202122232425262728293031 crontab -e:(edit user's crontab)编辑crontab -l:(list user's crontab)查看先编写一个脚本test.sh，内容为 date；[root@hadoop001 ~]# crontab -e * * * * * /root/test.sh &gt;&gt; /root/test.log格式:* * * * * *第1个: 分第2个: 小时第3个: 日第4个: 月第5个: 周*代表 每用tail -F 来事实查看 test.log文件，发现每个1分钟就会输出日期。如果是每10秒？编写脚本:[root@hadoop001 ~]# vi test.sh( #!/bin/bashfor((i=1;i&lt;=6;i++));do date sleep 10sdoneexit )可以通过 tail -F test.log来时刻查看调度情况，会发现每隔10秒就会输出日期。如果想定时一个脚本，在凌晨2点钟运行，该怎么做呢？crontab -e 进入编辑：0 2 * * * /root/test.sh &gt;&gt; /root/test.log]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Linux系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql面试题]]></title>
    <url>%2F2019%2F04%2F05%2Fsql%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[面试题：一.哪些部门的薪水最高两位的工种有哪些？（分组取前几位） 1.每个部门每个工种的薪水和123456789select * from emp;create view sal as selectdeptno,job,sum(sal+ifnull(comm,0)) as salfrom emp group by deptno,job;select * from sal; 2.从1结果集 找出哪些部门的工种薪水和最高的前2位工种是什么？12345678910111213141516171819202122232425262728293031select a.*from sal a where (select count(*) from sal b where a.deptno=b.deptno and a.sal&gt;b.sal ) #假设取薪水和最高的每个部门的哪个工种?select * from sal;insert into emp values (1000, 'BOSS', 'BOSS', NULL, '1981-06-09', 9000, null, 10);#取top 1select a.* from sal awhere (select count(*) from sal b where a.deptno=b.deptno and a.sal&lt;b.sal) = 0order by a.deptno;select a.* from sal awhere (select count(*) from sal b where a.deptno=b.deptno and a.sal&lt;b.sal) &lt;= 1 order by a.deptno asc ,a.sal desc;]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql查询语法]]></title>
    <url>%2F2019%2F04%2F05%2Fmysql%E4%B8%AD%E7%9A%84%E6%9F%A5%E8%AF%A2%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[创建三个表并添加数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061--部门表dept部门表(deptno部门编号/dname部门名称/loc地点)create table dept ( deptno numeric(2), dname varchar(14), loc varchar(13));insert into dept values (10, &apos;ACCOUNTING&apos;, &apos;NEW YORK&apos;);insert into dept values (20, &apos;RESEARCH&apos;, &apos;DALLAS&apos;);insert into dept values (30, &apos;SALES&apos;, &apos;CHICAGO&apos;);insert into dept values (40, &apos;OPERATIONS&apos;, &apos;BOSTON&apos;);--工资等级表salgrade工资等级表(grade 等级/losal此等级的最低/hisal此等级的最高)create table salgrade ( grade numeric, losal numeric, hisal numeric);insert into salgrade values (1, 700, 1200);insert into salgrade values (2, 1201, 1400);insert into salgrade values (3, 1401, 2000);insert into salgrade values (4, 2001, 3000);insert into salgrade values (5, 3001, 9999);--员工表emp员工表(empno员工号/ename员工姓名/job工作/mgr上级编号/hiredate受雇日期/sal薪金/comm佣金/deptno部门编号)工资 ＝ 薪金 ＋ 佣金1.表自己跟自己连接create table emp ( empno numeric(4) not null, ename varchar(10), job varchar(9), mgr numeric(4), hiredate datetime, sal numeric(7, 2), comm numeric(7, 2), deptno numeric(2));insert into emp values (7369, &apos;SMITH&apos;, &apos;CLERK&apos;, 7902, &apos;1980-12-17&apos;, 800, null, 20);insert into emp values (7499, &apos;ALLEN&apos;, &apos;SALESMAN&apos;, 7698, &apos;1981-02-20&apos;, 1600, 300, 30);insert into emp values (7521, &apos;WARD&apos;, &apos;SALESMAN&apos;, 7698, &apos;1981-02-22&apos;, 1250, 500, 30);insert into emp values (7566, &apos;JONES&apos;, &apos;MANAGER&apos;, 7839, &apos;1981-04-02&apos;, 2975, null, 20);insert into emp values (7654, &apos;MARTIN&apos;, &apos;SALESMAN&apos;, 7698, &apos;1981-09-28&apos;, 1250, 1400, 30);insert into emp values (7698, &apos;BLAKE&apos;, &apos;MANAGER&apos;, 7839, &apos;1981-05-01&apos;, 2850, null, 30);insert into emp values (7782, &apos;CLARK&apos;, &apos;MANAGER&apos;, 7839, &apos;1981-06-09&apos;, 2450, null, 10);insert into emp values (7788, &apos;SCOTT&apos;, &apos;ANALYST&apos;, 7566, &apos;1982-12-09&apos;, 3000, null, 20);insert into emp values (7839, &apos;KING&apos;, &apos;PRESIDENT&apos;, null, &apos;1981-11-17&apos;, 5000, null, 10);insert into emp values (7844, &apos;TURNER&apos;, &apos;SALESMAN&apos;, 7698, &apos;1981-09-08&apos;, 1500, 0, 30);insert into emp values (7876, &apos;ADAMS&apos;, &apos;CLERK&apos;, 7788, &apos;1983-01-12&apos;, 1100, null, 20);insert into emp values (7900, &apos;JAMES&apos;, &apos;CLERK&apos;, 7698, &apos;1981-12-03&apos;, 950, null, 30);insert into emp values (7902, &apos;FORD&apos;, &apos;ANALYST&apos;, 7566, &apos;1981-12-03&apos;, 3000, null, 20);insert into emp values (7934, &apos;MILLER&apos;, &apos;CLERK&apos;, 7782, &apos;1982-01-23&apos;, 1300, null, 10); 聚合函数：1sum() max() min() count() 分组函数：1group by .... having .... 列题：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#1.求员工表 所有人的薪水和 as salsum 别名selectsum(sal) as salsumfrom emp;#2.求每个部门的所有人的薪水和selectdeptno,sum(sal) as salsumfrom empgroup by deptno;#总结: group by 后面出现多少个字段，那么select 后面也要一模一样#每/各 是为分组字段 #3.求每个部门的每个岗位的所有人的薪水和selectdeptno,job,sum(sal) as salsumfrom empgroup by deptno,job;#4.求每个部门的每个岗位的所有人的薪水和，及人数selectdeptno,job,sum(sal) as salsum,count(deptno) as pnumfrom empgroup by deptno,job;group by deptno --》3group by deptno , job #5.求薪水和大于1500的哪些部门 ==》 每个部门的薪水和 ==》 哪些薪水和大于1500的部门selectdeptno,sum(sal) as salsumfrom empgroup by deptno having sum(sal)&gt;1500;#子查询select t.* from (selectdeptno,sum(sal) as salsumfrom empgroup by deptno) as twhere t.salsum &gt; 1500; 关联 left join,right join,inner join123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172重新创建表：drop table testa;create table testa(aid int,aname varchar(100),address varchar(100));create table testb(bid int,bname varchar(100),age int);create table testsal(cid int,sal int);delete from testsal;insert into testsal values(1,100);insert into testsal values(2,300);insert into testsal values(4,700);insert into testsal values(8,1300);insert into testsal values(9,3300);select * from testsal;insert into testa values(1,&apos;xiao1&apos;,&apos;SH1&apos;);insert into testa values(2,&apos;xiao2&apos;,&apos;SH1&apos;);insert into testa values(3,&apos;xiao3&apos;,null);insert into testa values(4,&apos;xiao4&apos;,&apos;SH2&apos;);insert into testa values(5,&apos;xiao5&apos;,&apos;SH2&apos;);insert into testb values(1,&apos;xiao1&apos;,10);insert into testb values(2,&apos;xiao2&apos;,20);insert into testb values(3,&apos;xiao3&apos;,30);insert into testb values(4,&apos;xiao4&apos;,40);insert into testb values(4,&apos;xiao44&apos;,440);insert into testb values(7,&apos;xiao7&apos;,70);insert into testb values(8,&apos;xiao8&apos;,80);insert into testb values(9,&apos;xiao9&apos;,90);## left join： select a.*,b.*from testa a left join testb b on a.aid=b.bid a &lt;-- b #以左表为主，左表数据最全，右表来匹配的，匹配多少算多少 aid aname bid bname age 1 xiao1 1 xiao1 10 2 xiao2 2 xiao2 20 3 xiao3 3 xiao3 30 4 xiao4 4 xiao4 40 5 xiao5 null null null ## right join： select a.*,b.*from testa a right join testb b on a.aid=b.bid #以右表为主，右表数据最全，左表来匹配的，匹配多少算多少 a --&gt; b 1 xiao1 1 xiao1 10 2 xiao2 2 xiao2 20 3 xiao3 3 xiao3 30 4 xiao4 4 xiao4 40 7 xiao7 70 8 xiao8 80 9 xiao9 90## inner join select a.*,b.*from testa a inner join testb b on a.aid=b.bid #等值连接 两边都要存在 1 xiao1 1 xiao1 10 2 xiao2 2 xiao2 20 3 xiao3 3 xiao3 30 4 xiao4 4 xiao4 40## 笛卡尔集 select a.*,b.*from testa a, testb b;## 等值连接 select a.*,b.*from testa a, testb b where a.aid = b.bid; jion相关联的题：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#哪些人 有薪水的有年龄的有地址select s.*,b.*,a.*from testsal sleft join testb b on s.cid =b.bidleft join testa a on s.cid =a.aidwhere s.sal is not null and b.age is not null and a.address is not null;select s.*,b.*,a.*from testsal sleft join testb b on s.cid =b.bidleft join testa a on b.bid =a.aidwhere s.sal is not null and b.age is not null and a.address is not null;select s.*,b.*,a.*from testsal sleft join testb b on s.cid =b.bidright join testa a on b.bid =a.aid;#哪些人 有薪水的 有年龄的有地址select s.*,t.*from testsal sleft join (select b.*,a.*from testb b right join testa a on b.bid =a.aid) t on s.cid = t.aidwhere s.sal is not null and t.age is not null and t.address is not null;select s.*,t.*from testsal sinner join (select b.*,a.*from testb b inner join testa a on b.bid =a.aid) t on s.cid = t.aid;]]></content>
      <categories>
        <category>mysql数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql的基础语句]]></title>
    <url>%2F2019%2F04%2F05%2Fmysql%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[mysql的基础语句： 1.如何创建一个新的DB和用户1234567create database DB名字(创建一个新的DB)grant all privileges on DB名字.* to 用户名字@&apos;%&apos; identified by &apos;密码&apos;(创建一个新用户并赋予它对这个DB的全部权限， %:ip地址，表示该用户允许任意ip地址来访问当前数据库 %也可以换成任意ip，比如：127.0.0.1：表示当前机器 也可以是网段，比如：192.168.137.1~255 ) 操作完成后记刷新，养成良好的习惯：flush privileges; 视野开阔：1java函数启动也可以不需要 main启动类，详情查看：https://github.com/dbeaver/dbeaver/issues/2671 如何查看mysql的链接数 适当kill：123show processlist;kill id找到哪个process 卡，导致夯住 锁死 --&gt;和开发 和其他小伙伴确认清楚 再kill 字段类型：12345678910111213数值类型： int 整数 float 单精度 double 双精度 decimal 小数值：一般用于金额字段 字符串类型 char 定长字符串 0-255字节 varchar 变长字符串 0-65535字节 日期和时间类型 date 日期 YYYY-MM-DD time 时间 HH：MM：SS datetime timestamp 数据库类型分为：123DDL：create dropDML：inisert update delete selectDCL：grant 创建表：123create table 表名每个表中必须只能有一个主键，但是可以有1个或者两个字段组成联合主键也可以是由两个字段组成自增长 id自增长是非业务系统的唯一标识 约束：1234primary key = unique+not null唯一约束 unique非空约束 not null默认约束 创建表：12345678910111213#表 table （创建一张名字rzdata的表）create table rzdata(id int AUTO_INCREMENT primary key, （id自增长 主键）stu_num int,stu_name varchar(100),stu_age int,createtime timestamp default current_timestamp,createuser varchar(100),updatetime timestamp default current_timestamp on update current_timestamp,updateuser varchar(100)); 向表中插入数据：12345678insert into 数据表怕(列名)values(数据) 语法列如：(一下两种方式都可以)insert into ruozedata.rzdata(stu_num,stu_name,stu_age,createuser,updateuser) values(110,&apos;jepson&apos;,18,&apos;j&apos;,&apos;r&apos;);insert into ruozedata.rzdatavalues(10,110,&apos;jepson&apos;,18,&apos;2019-01-27 12:00:00&apos;,&apos;j&apos;,&apos;2019-01-27 12:00:00&apos;,&apos;r&apos;); 更改表中数据：1234update 数据表 set 数据 where 条件列如：update ruozedata.rzdata set stu_age=22 ;（如果不加条件表示表中 age全部变成22）update ruozedata.rzdata set stu_age=26 where id=1;（表示 id为1的 年龄改成26） 删除表中数据：12delete from ruozedata.rzdata; （表示删除表中所有数据）delete from ruozedata.rzdata where stu_num=110;（删除stu_num 为110的这一条数据） 查询表中数据：12345select * from ruozedata.rzdata;（表示查询表中所有数据）select id,stu_name from ruozedata.rzdata;（表示查看表中id，和stu_name数据）select id,stu_name from ruozedata.rzdata where stu_age=18;（表示查看stu_age=18这一条数据中的id和 stu_name的数据）select * from ruozedata.rzdata where stu_age=18;#假设你只想取 stu_name，stu_age两列 ，但是你的SQL是* 所有字段 这样会有额外的开销 生产中建字段注意事项：1234#createtime cretime ctime cre_time create_time#orderno order_no ordernum orderid#两张表 order_no=orderid （这就不符合规范了，建字段前要先查看别人是怎么创建字段的，符合规矩）#3.字段名称不允许中文 中文的汉语拼音 建字段 条件查询：123where and or列如：select * from ruozedata.rzdata where stu_age=18 and stu_name=&apos;jepson&apos;; select * from ruozedata.rzdata where stu_age=24 or stu_name=&apos;jepson&apos;; 模糊查询：12345like列如：select * from ruozedata.rzdata where stu_name like &apos;%n%&apos;; select * from ruozedata.rzdata where stu_name like &apos;j%&apos;; select * from ruozedata.rzdata where stu_name like &apos;%u&apos;; select * from ruozedata.rzdata where stu_name like &apos;__s%&apos;; #占位符_ 第三个字母s 排序查询：12345order byselect * from ruozedata.rzdata order by stu_age asc; (asc表示升序）select * from ruozedata.rzdata order by stu_age desc;（desc表示降序）按年龄升序 学号降序select * from ruozedata.rzdata order by stu_age asc,stu_num desc; 限制行数查询：1select * from ruozedata.rzdata limit 2; （限制显示两行）]]></content>
  </entry>
  <entry>
    <title><![CDATA[mysql二进制部署]]></title>
    <url>%2F2019%2F04%2F05%2Fmysql%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[mysql环境搭建 1.Download and Check MD5[root@sht-sgmhadoopnn-01 ~]# cd /usr/local mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz选择rz上传 [root@sht-sgmhadoopnn-01 local]# cat mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz.md561affe944eff55fcf51b31e67f25dc10 mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz [root@sht-sgmhadoopnn-01 local]# md5sum mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz61affe944eff55fcf51b31e67f25dc10 mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz 2.Check isnot install[root@sht-sgmhadoopnn-01 local]# ps -ef|grep mysqldroot 2493 2423 0 19:48 pts/3 00:00:00 grep mysqld[root@sht-sgmhadoopnn-01 local]# rpm -qa |grep -i mysql 3.tar and mv[root@sht-sgmhadoopnn-01 local]# tar xzvf mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz[root@sht-sgmhadoopnn-01 local]# mv mysql-5.6.23-linux-glibc2.5-x86_64 mysql 4.Create group and user[root@sht-sgmhadoopnn-01 local]# groupadd -g 101 dba[root@sht-sgmhadoopnn-01 local]# useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin useradd: warning: the home directory already exists.（之前创建过mysqladmin用户，将其删除掉就好）Not copying any file from skel directory into it.（需要将/etc/skel中的 .bash~ 文件复制到/usr/local/mysql ，目录下）[root@hadoop001 local]# [root@sht-sgmhadoopnn-01 local]# id mysqladminuid=514(mysqladmin) gid=101(dba) groups=101(dba),0(root)[root@sht-sgmhadoopnn-01 local]# passwd mysqladminChanging password for user mysqladmin.New UNIX password:BAD PASSWORD: it is too simplistic/systematicRetype new UNIX password:passwd: all authentication tokens updated successfully.[root@sht-sgmhadoopnn-01 local]# if user mysqladmin is existing,please execute the following command of usermod. ##[root@sht-sgmhadoopnn-01 local]# usermod -u 514 -g dba -G root -d /usr/local/mysql mysqladmin # copy 环境变量配置文件至mysqladmin用户的home目录中,为了以下步骤配置个人环境变量 [root@sht-sgmhadoopnn-01 local]# cp /etc/skel/.* /usr/local/mysqlcp: omitting directory /etc/skel/.&#39; cp: omitting directory/etc/skel/..’cp: omitting directory `/etc/skel/.mozilla’ 对于一个系统想要运行 肯定需要配置文件 my.cnf 5.Create /etc/my.cnf (640权限)#defualt start: /etc/my.cnf-&gt;/etc/mysql/my.cnf-&gt;SYSCONFDIR/my.cnf-&gt;$MYSQL_HOME/my.cnf-&gt; –defaults-extra-file-&gt;~/my.cnf [root@sht-sgmhadoopnn-01 mysql]# cd /etc/[root@sht-sgmhadoopnn-01 etc]# touch my.cnf[root@sht-sgmhadoopnn-01 etc]# vi my.cnf [client]port = 3306socket = /usr/local/mysql/data/mysql.sock [mysqld]port = 3306socket = /usr/local/mysql/data/mysql.sock skip-external-lockingkey_buffer_size = 256Msort_buffer_size = 2Mread_buffer_size = 2Mread_rnd_buffer_size = 4Mquery_cache_size= 32Mmax_allowed_packet = 16Mmyisam_sort_buffer_size=128Mtmp_table_size=32M table_open_cache = 512thread_cache_size = 8wait_timeout = 86400interactive_timeout = 86400max_connections = 600 Try number of CPU’s*2 for thread_concurrency thread_concurrency = 32 #isolation level and default enginedefault-storage-engine = INNODBtransaction-isolation = READ-COMMITTED server-id = 1basedir = /usr/local/mysqldatadir = /usr/local/mysql/datapid-file = /usr/local/mysql/data/hostname.pid #open performance schemalog-warningssysdate-is-now binlog_format = MIXEDlog_bin_trust_function_creators=1log-error = /usr/local/mysql/data/hostname.errlog-bin=/usr/local/mysql/arch/mysql-bin #other logs #general_log =1 #general_log_file = /usr/local/mysql/data/general_log.err #slow_query_log=1 #slow_query_log_file=/usr/local/mysql/data/slow_log.err #for replication slave #log-slave-updates #sync_binlog = 1 #for innodb optionsinnodb_data_home_dir = /usr/local/mysql/data/innodb_data_file_path = ibdata1:500M:autoextendinnodb_log_group_home_dir = /usr/local/mysql/archinnodb_log_files_in_group = 2innodb_log_file_size = 200M #生产上 机械硬盘 sata盘 5000r 7200 10000 15000 ==&gt; ssd 生产 innodb_buffer_pool_size 调大 8G innodb_buffer_pool_size = 1024Minnodb_additional_mem_pool_size = 50Minnodb_log_buffer_size = 16M innodb_lock_wait_timeout = 100 #innodb_thread_concurrency = 0innodb_flush_log_at_trx_commit = 1innodb_locks_unsafe_for_binlog=1 #innodb io features: add for mysql5.5.8performance_schemainnodb_read_io_threads=4innodb-write-io-threads=4innodb-io-capacity=200 #purge threads change default(0) to 1 for purgeinnodb_purge_threads=1innodb_use_native_aio=on #case-sensitive file names and separate tablespaceinnodb_file_per_table = 1lower_case_table_names=1 [mysqldump]quickmax_allowed_packet = 16M [mysql]no-auto-rehash [mysqlhotcopy]interactive-timeout [myisamchk]key_buffer_size = 256Msort_buffer_size = 256Mread_buffer = 2Mwrite_buffer = 2M 6.chown and chmod privileges and try first install[root@sht-sgmhadoopnn-01 local]# chown mysqladmin:dba /etc/my.cnf[root@sht-sgmhadoopnn-01 local]# chmod 640 /etc/my.cnf[root@sht-sgmhadoopnn-01 etc]# ll my.cnf-rw-r—– 1 mysqladmin dba 2201 Aug 25 23:09 my.cnf [root@sht-sgmhadoopnn-01 local]# chown -R mysqladmin:dba /usr/local/mysql[root@sht-sgmhadoopnn-01 local]# chmod -R 755 /usr/local/mysql[root@sht-sgmhadoopnn-01 local]# su - mysqladmin[mysqladmin@sht-sgmhadoopnn-01 ~]$ pwd/usr/local/mysql 创建arch目录 存储binlog 归档日志 生产中:mysql–maxwell–kafka–ss–hbase 实时 mcp[mysqladmin@sht-sgmhadoopnn-01 ~]$ mkdir arch [mysqladmin@sht-sgmhadoopnn-01 ~]$ scripts/mysql_install_dbInstalling MySQL system tables…./bin/mysqld:error while loading shared libraries: libaio.so.1: cannot open shared object file:No such file or directory #缺少libaio.so 包 ###see version[root@sht-sgmhadoopnn-01 local]# cat /proc/versionLinux version 2.6.18-164.11.1.el5 (mockbuild@builder10.centos.org) (gcc version 4.1.2 20080704 (Red Hat 4.1.2-46)) #1 SMP Wed Jan 20 07:32:21 EST 2010[root@sht-sgmhadoopnn-01 local]# rpm -qa |grep gcclibgcc-4.1.2-46.el5_4.2libgcc-4.1.2-46.el5_4.2[root@sht-sgmhadoopnn-01 local]# yum -y install libaio 7.Again install[mysqladmin@sht-sgmhadoopnn-01 ~]$ scripts/mysql_install_db \–user=mysqladmin \–basedir=/usr/local/mysql \–datadir=/usr/local/mysql/data 查看data/hostname.err文件 仔细错误 8.Configure mysql service and boot auto start[root@sht-sgmhadoopnn-01 ~]# cd /usr/local/mysql #将服务文件拷贝到init.d下，并重命名为mysql[root@sht-sgmhadoopnn-01 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql #赋予可执行权限[root@sht-sgmhadoopnn-01 mysql]# chmod +x /etc/rc.d/init.d/mysql #删除服务[root@sht-sgmhadoopnn-01 mysql]# chkconfig –del mysql #添加服务[root@sht-sgmhadoopnn-01 mysql]# chkconfig –add mysql[root@sht-sgmhadoopnn-01 mysql]# chkconfig –level 345 mysql on[root@sht-sgmhadoopnn-01 mysql]# vi /etc/rc.local #!/bin/sh# This script will be executed after all the other init scripts. You can put your own initialization stuff in here if you don’t want to do the full Sys V style init stuff. touch /var/lock/subsys/local su - mysqladmin -c “/etc/init.d/mysql start –federated” 9.Start mysql and to view process and listening[root@sht-sgmhadoopnn-01 mysql]# su - mysqladmin[mysqladmin@sht-sgmhadoopnn-01 ~]$ pwd/usr/local/mysql[mysqladmin@sht-sgmhadoopnn-01 ~]$ rm -rf my.cnf [mysqladmin@hadoop001 ~]$ service mysql startStarting MySQL. [ OK ][mysqladmin@hadoop001 ~]$[mysqladmin@hadoop001 ~]$ service mysql statusMySQL running (3625) [ OK ][mysqladmin@hadoop001 ~]$ [mysqladmin@sht-sgmhadoopnn-01 ~]$ mysqld_safe &amp;[1] 11802[mysqladmin@sht-sgmhadoopnn-01 ~]$ 150825 22:53:38 mysqld_safe Logging to ‘/usr/local/mysql/data/hostname.err’.150825 22:53:38 mysqld_safe Starting mysqld daemon with databases from /usr/local/mysql/data/150825 22:53:39 mysqld_safe mysqld from pid file /usr/local/mysql/data/hostname.pid ended[mysqladmin@sht-sgmhadoopnn-01 ~]$ [mysqladmin@sht-sgmhadoopnn-01 ~]$ ps -ef|grep mysqld514 6247 6219 0 17:30 pts/1 00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe514 6902 6247 2 17:30 pts/1 00:00:01 /usr/local/mysql/bin/mysqld –basedir=/usr/local/mysql –datadir=/usr/local/mysql/data –plugin-dir=/usr/local/mysql/lib/plugin –log-error=/usr/local/mysql/data/hostname.err –pid-file=/usr/local/mysql/data/hostname.pid –socket=/usr/local/mysql/data/mysql.sock –port=3306514 6927 6219 0 17:31 pts/1 00:00:00 grep mysqld [mysqladmin@sht-sgmhadoopnn-01 ~]$ netstat -tulnp | grep mysql(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)tcp 0 0 :::3306 :::* LISTEN 11541/mysqld [root@sht-sgmhadoopnn-01 local]# service mysql statusMySQL running (21507) [ OK ] 部署完成了创建一个root用户 密码为空​ 一个空用户 密码为空 生产上密码肯定不为空 比如123456bin/mysql -uroot -p -h127.0.0.1 一回城就让你输入密码 这是安全的 不会被history记录 bin/mysql -uroot -p123456 -h127.0.0.1 一回城就进数据库 这是不安全的 会被history记录bin/mysql -u root -p 123456 -h 127.0.0.1 -P3306 10.Login mysql[mysqladmin@sht-sgmhadoopnn-01 ~]$ mysqlWelcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 1Server version: 5.6.23-log MySQL Community Server (GPL) Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners. Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. mysql&gt; show databases;+——————–+| Database |+——————–+| information_schema || mysql || performance_schema || test |+——————–+4 rows in set (0.00 sec) 11.Update password and Purge usermysql&gt; use mysqlDatabase changed mysql&gt; update user set password=password(‘123456’) where user=’root’; Query OK, 4 rows affected (0.00 sec)Rows matched: 4 Changed: 4 Warnings: 0 mysql&gt; select host,user,password from user;+—————-+——+——————————————-+| host | user | password |+—————-+——+——————————————-+| localhost | root | 6340BE3C15D246B0D74BAF3F135915ED19E0069F || sht-sgmhadoopnn-01 | root | 6340BE3C15D246B0D74BAF3F135915ED19E0069F || 127.0.0.1 | root | 6340BE3C15D246B0D74BAF3F135915ED19E0069F || ::1 | root | 6340BE3C15D246B0D74BAF3F135915ED19E0069F || localhost | | || sht-sgmhadoopnn-01 | | |+—————-+——+——————————————-+6 rows in set (0.00 sec) mysql&gt; delete from user where user=’’;mysql&gt; select host,user,password from user;+—————-+——+——————————————-+| host | user | password |+—————-+——+——————————————-+| localhost | root | 6340BE3C15D246B0D74BAF3F135915ED19E0069F || sht-sgmhadoopnn-01 | root | 6340BE3C15D246B0D74BAF3F135915ED19E0069F || 127.0.0.1 | root | 6340BE3C15D246B0D74BAF3F135915ED19E0069F || ::1 | root | 6340BE3C15D246B0D74BAF3F135915ED19E0069F |+—————-+——+——————————————-+4 rows in set (0.00 sec) #针对用户 权限的操作语句 养成习惯 都最后一步执行刷新权限mysql&gt; flush privileges; 12.Configure .bash_profile[mysqladmin@sht-sgmhadoopnn-01 ~]$ cat .bash_profile .bash_profile Get the aliases and functions if [ -f ~/.bashrc ]; then​ . ~/.bashrcfi User specific environment and startup programs MYSQL_HOME=/usr/local/mysqlPATH=${MYSQL_HOME}/bin:$PATH PS1=uname -n“:”‘$USER’”:”‘$PWD’”:&gt;”; export PS1 这行是设置显示目录，将自己的家目录显示出来。 配置环境变量时，最前面加上 export，将最新的要部署的变量“放在前面” PATH=$PATH:$HOME/binexport PATH $PATH=/usr/bin:/usr/local/bin:/usr/local/mysql/binmysql5.5 mysql 假设 /usr/local/bin/mysqlmysql5.6 mysql Error1: File ‘/usr/local/mysql/arch/mysql-bin.index’ not found (Errcode: 13)test2.localdomain:mysqladmin:/usr/local/mysql/arch:&gt;chmod 755 test2.localdomain:mysqladmin:/usr/local/mysql/arch:&gt;chown –R mysqladmin:dba tidb如果安装失败，rm -rf data/​ rm -rf arch/]]></content>
  </entry>
  <entry>
    <title><![CDATA[jdk1.8安装]]></title>
    <url>%2F2019%2F04%2F05%2Fjdk1.8%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[JDK的安装&amp;&amp;全局环境变量 123456789101112131415jdk 文件必须放到: /usr/ java目录下，否则后期安装上CDH之后会出现一些坑。解压jdk的 tar包: tar -xzvf jdk-8u45-linux-x64.gz注意: 解压之后，修改jdk文件夹的权限 chown -R root:root jdk1.8.0_45 文件夹和文件夹里面的文件夹和文件 chown -R root:root jdk1.8.0_45/*配置环境变量： vi /etc/profile #add JAVA_HOME export JAVA_HOME=/usr/java/jdk1.8.0_45 export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 生效： source /etc/profile查看是否成功： java -version]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux命令进阶]]></title>
    <url>%2F2019%2F04%2F04%2FLinux%E5%91%BD%E4%BB%A4%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[Linux进阶命令： 查看用户命令： 12ll /usr/sbin/user*列如:(usermod,userdel,useradd) 查看用户组命令： 12ll /usr/sbin/group*列如:(groupmod,groupdel,groupadd) 添加和删除用户： 123456789useradd ruoze (创建一个ruoze用户，并且自动创建一个ruoze用户组)通过 id ruoze 命令查看信息：uid=501(ruoze) gid=501(ruoze) groups=501(ruoze) uid:(用户) gid:(主组) groups:(所有组)查看用户创建记录：cat /etc/passwd查看组创建记录： cat /etc/groupuserdel ruoze (删除ruoze用户)如果删除用户之后，/etc/passwd中没有记录 /etc/group中没有记录(因为ruoze组中只有一个ruoze用户，当删掉ruoze用户，该组经过校验发现只有自己，就会删除该组，但是家目录中用户还在，删除后再创建会提示用户存在，不过用户一样会创建出来) 将用户添加到用户组： 12usermod -a -G 用户组 用户usermod -g 用户组 用户 (将用户组改为主组) 设置密码： 1passwd 用户 怎样切换用户： 123su - 用户su 用户- : 代表切用户以后，进入该用户的家目录且执行 环境变量文件 退出当前用户： 1exit 普通用户如何临时获取root最大权限： 1234vi /etc/sudoers修改配置文件：(列如：root ALL=(ALL) ALL 用户 ALL=(root) NOPASSWD:ALL) ：wq! 强制退出后, 输入sudo ls -l /root 命令，查看信息 管道符和过滤： 123| (管道符)grep (过滤)用法：(列如：cat /home | grep jepson 在查看home下所有用户之后自动将jepson筛选出来) 怎样查看进程： 12345ps -ef | grep tail显示出的信息：root 2425 1779 0 20:39 pts/0 00:00:00 tail -f install.log.syslog2425：进程pid1779：父idps -ef | grep &apos;log&apos; (模糊查询) 杀死进程： 12345kill -9 进程的pidkill -9 pid pid(并行杀)如果我只想杀死包含log字符的所有进程:(kill -9 $(pgrep -f log))(提醒: kill前确认清楚该进程是否该杀 不要杀错 )(linux操作: 遇到rm kill高危命令 需确认再确认) 查看端口号： 1netstat -nlp | grep pid或者进程名字 题目： 123456老板说去A服务器 login,打开xxx软件的web界面？ip+portps - ef | grep xxx --&gt; pid --&gt; netstat -nlp | grep pid等价于netstat -nlp | grep xxx(0.0.0.0:22 或者 ::: 80都表示的是本机ip，代表当前ip对外提供访问 如果：127.0.0.1:80 或者 localhost:80 只针对内部当前机器访问 ) 如果出现链接拒绝错误 Connection refused应该怎样做： 123window : 控制面板 卸载程序 打开或关闭windows功能 选中Telnet客户端 部署 通过 cmd ping一下ip，然后 telnet ip port，如果不行，很有可能是ip只针对内部当前机器访问 linux : yum install telnet.x86_64 搜索命令： 123456列如搜索xxx软件 history | grep xxx (通过查看历史命令记录来查找xxx软件) ps - ef | grep xxx (通过查看进程搜索xxx软件) find / -name &apos;*log*&apos; (全局 根目录模糊查询) find ./ -name &apos;*log*&apos; (当前目录开始 模糊查询) * (代表模糊查询) Linux安装和卸载软件 rpm 123456yum search 软件名字: (搜索软件)yum install -y 软件名字: (下载软件)yun remove 软件名字: (卸载软件)rpm -qa | grep 软件名字: (搜索软件)rpm -e 软件名字: (卸载软件，卸载过程中可能会出现失败的情况，由于该软件包依赖于其他rpm) rpm -e --nodeps 软件名字: (强制卸载软件) 查找命令： 1which xxx (命令都是从$PATH中查找) mv和cp哪个速度更快： 1当在同一系统中时mv的速度会快于cp的速度 文本编辑命令： 12vi xxx.log: (编辑xxx.log文件)在生产中使用vi修改配置文件，别人的服务文件时应先要复制一份，再进行修改 命令行模式常见快捷键： 123456dd: (删除当前光标所在行)dG: (删除当前光标所在行下的所有行包括光标所在行)ndd: (删除光标所在行下的n行包括光标所在行)gg: (光标跳转到第一行第一个字母)G: (光标跳转到最后一行的第一个字)shift+$:(行尾) 生产场景下如果一个文件内容比较多，想清空该如何做： 123gg --&gt; dG: (第一种方式)echo &quot;&quot; &gt;文件: (为清空，文件中还存留一个字节，要注意)cat /dev/null &gt;文件: (真正的清空) 当一个文件内容非常多，比如系统文件，如何快速定位到你想搜索的关键词： 123在尾行模式下输入 /关键词set nu: (表格化，开头有行号)set nonu: (取消表格化) 权限： 12345678910111213141516171819202122232425drwxr-xr-x. 3 root root 4096 Jan 19 20:42 1drwxr-xr-x. 2 root root 4096 Jan 19 20:44 4-rw-r--r--. 1 root root 10033 Jan 19 20:57 install.log.syslog-rw-r--r--. 1 root root 49566 Jan 23 21:37 jepson.log-rw-r--r--. 1 root root 1 Jan 23 21:30 ruoze.log.1d rwxr-xr-x第一个: d:(代表文件夹) -:(文件) l:(连接)r: 读 4w: 写 2x: 执行shell脚本 1列如：rwxr-xr-x 755 rwxr--r-- 744 r--r--r-- 444 r-xrwxrw- 576rwxr-xr-x :分为三组 第一组:rwx 7 代表文件和文件夹所属用户的权限: 读写执行 第二组:r-x 5 代表文件和文件夹所属的用户组的权限: 读执行 第三组:r-x 5 代表其他组的所有用户对这个文件或者文件夹的权限: 读执行root root第三列:所属用户第四列:所属用户组第五列字节(文件大小)ll -h:(只能查看文件大小)du -sh:(即可以查看文件大小，也可以查看文件夹大小)第六列:文件时间 修改用户权限命令： 1234chmod -R 777 目录:(修改用户权限)chown -R 用户:所属用户组 目录:(修改所属用户和用户组，来达到修改权限的作用)-R表示递归，文件夹下所有的都会被修改整套Linux命令中，只有这两个命令用到的是大R，其它都是小r 软连接： 12ln -s 原始路径 目标路径注意:(删除目标路径，原路径不会被删除) 上传和下载： 123yum install -y lrzsz (下载rpm)rz: window --&gt; linuxrs: linux --&gt; window 系统命令： 12topload average:0.00,0.00,0.00( 生产服务器 不要超过10经验值) 查看内存： 1free -m 查看硬盘： 1df -h 压缩解压： 123456zip:(压缩) unzip:(解压)列如: zip -r 文件夹.zip 文件夹/*(压缩文件夹以及文件夹下所有内容) unzip 文件夹.ziptar -czvf(压缩) tar -xzvf(解压)列如:tar -czvf 文件夹.tar 文件夹/* tar -xzvf 文件夹.tar 作业调度： 1234567891011121314151617181920212223crontab -e:(edit user's crontab)编辑crontab -l:(list user's crontab)查看[root@hadoop001 ~]# crontab -e * * * * * /root/test.sh &gt;&gt; /root/test.log格式:* * * * * *第1个: 分第2个: 小时第3个: 日第4个: 月第5个: 周*代表 每每10秒？编写脚本:[root@hadoop001 ~]# vi test.sh( #!/bin/bashfor((i=1;i&lt;=6;i++));do date sleep 10sdoneexit ) 后台执行命令 不交互： 12345 ./test.sh &amp; nohup ./test.sh &amp;生产: ==&gt; nohup ./test.sh &amp; nohup ./test.sh &gt; /root/test.log 2&gt;&amp;1 &amp; # 详细解释 https://blog.csdn.net/ggxiaobai/article/details/53507530]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Linux系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux一些常用命令]]></title>
    <url>%2F2019%2F04%2F04%2FLinux%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux基本命令： 查看IP: 12ifconfigipconfig(windows) 关闭防护墙： 12service ipconfig stopchkconfig off 查看当前光标所在路径： 1pwd Linux系统中根目录是指： 1/ 有关于cd相关命令： 1234cd 或者 cd~ (切换到根目录)cd - (切换到上次目录)cd ../ (切换到上层目录)cd ../ ../ (切换到上两层目录) 目录路径： 12绝对路径 (例如： /home/etc /为开头)相对路径 (当前目录开始，没有/为开头) 清空屏幕： 1clear 有关于ll ls 相关知识点： 1234567ls (查看当前目录下所有文件和文件夹)ll (查看当前目录下所有文件和文件夹详细信息)ls -l == ll (别名)ls -l -a == ls -la (查看所有隐藏文件和文件夹)-a 表示查看隐藏的文件和文件夹 (以.开头的文件或者文件夹)ll -h (查看文件大小)ll -rt (文件或者文件夹按时间顺序排序) 创建目录： 123mkdir (列如：mkdir ruozedate/)mkdir -p 4/5/6 (创建级联目录)mkdir 4 5 6 (创建并行目录) 移动命令： 12345mv 原路径文件夹/文件 目标端的路径文件夹/文件列如 (mv instail.log ruozedate/)也可以在移动的过程中重命名列如 (mv instail.log ruozedate/instail.log2019)注意 (移动只有一份) 复制命令： 1234cp 原路径文件夹/文件 目标路径文件夹/文件 列如 (cp instail.log ruozedate/instail.log2019&quot;复制的过程中可以重新命名&quot;)如果复制的是文件夹的话 需要加 -r 列如 (cp -r d66 ruozedate/d6&quot;复制的过程可以重命名&quot;) 查看帮助命令： 1列如: ls --help 查看文件内容： 123cat (文件内容一下子全部显示)more (文件内容分页显示，按空格往下查看，不能往回查看，按q退出)less（文件内容分页显示，按下箭头往下查看，上箭头往回查看，按q退出） 实时查看： 12tail -f xxx.logtail -F xxx.log F=f+retry 别名： 12345ls -l==ll (ll就是别名)alias rzcd=&apos;cd /home/jepson/&apos; (在当前窗口输入 rzcd命令 就可以进入jespon下，注意：当前 session生效)永久生效该怎么配置： (1)全局环境变量文件： vi /etc/profile (在profile中输入 alias rzcd=&apos;cd /home/jepson/&apos; )，wq退出，输入 source /etc/profile，这样就可以永久生效了。 (2)个人环境变量文件： vi .bash_profile (在profile中输入 alias rzcd=&apos;cd /home/jepson/&apos; ),退出，输入source .bash_profile或者 . .bash_profile,这样也可以永久生效。 创建空文件： 1touch (列如：touch instail.log) 删除命令： 1234rm xxx.log (删除文件)rm -r xxx (删除文件夹)rm -f xxx.log (强制删除)注意：(-r 只代表文件夹 -f 表示强制) 设置变量: 12设置 key=value (前后不能有空格) 使用 $&#123;key&#125;列如：设置path=&quot;123&quot; 之后echo $&#123;path&#125;,就会输出&quot;123&quot;]]></content>
  </entry>
</search>
